# VIF Critic Training Configuration
# =================================
# Configuration for training the Value Identity Function critic model.
# Supports ablation studies by swapping encoder and hyperparameters.

# Text Encoder Configuration
# --------------------------
# The encoder converts journal entry text to dense embeddings.
# Swap model_name for ablation studies comparing encoder quality.
encoder:
  type: sbert
  model_name: nomic-ai/nomic-embed-text-v1.5  # 768 native, Matryoshka to 256d
  trust_remote_code: true
  truncate_dim: 256          # Matryoshka truncation (MTEB 61.04 @ 256d)
  text_prefix: "classification: "  # Required task prefix for nomic models
  # Alternatives for ablation:
  # model_name: all-MiniLM-L6-v2     # 384 dim, fast, good quality
  # model_name: all-mpnet-base-v2     # 768 dim, higher quality, slower

# State Encoder Configuration
# ---------------------------
state_encoder:
  window_size: 1       # Current entry only (param budget too tight for window)
  ema_alpha: 0.3       # Smoothing factor for history EMA (higher = more recent bias)
  # Previous: window_size: 3 (state_dim=790, ratio=432x â€” severe overfitting)

# Model Architecture
# ------------------
model:
  hidden_dim: 32       # Reduced from 256 to control overfitting (~10k params)
  dropout: 0.3         # Increased from 0.2 for stronger regularization
  output_dim: 10       # Number of Schwartz dimensions (fixed)
  heteroscedastic: false  # When true, adds variance head for learned aleatoric uncertainty

# Training Configuration
# ----------------------
training:
  epochs: 100
  batch_size: 16
  learning_rate: 0.001
  weight_decay: 0.01      # L2 regularization

  # Learning rate scheduler
  scheduler:
    type: reduce_on_plateau
    factor: 0.5           # Reduce LR by this factor
    patience: 10          # Wait this many epochs before reducing
    min_lr: 0.00001

  # Early stopping
  early_stopping:
    patience: 20          # Stop if no improvement for this many epochs
    min_delta: 0.001      # Minimum change to count as improvement

# Data Configuration
# ------------------
data:
  labels_path: logs/judge_labels/judge_labels.parquet
  wrangled_dir: logs/wrangled
  train_ratio: 0.70
  val_ratio: 0.15
  seed: 42

# MC Dropout / BNN Configuration
# ------------------------------
mc_dropout:
  n_samples: 50           # Number of forward passes for uncertainty estimation

# BNN prior parameters (for CriticBNN, train_bnn.py)
bnn:
  prior_mean: 0.0
  prior_variance: 1.0
  posterior_rho_init: -3.0

# Output Configuration
# --------------------
output:
  checkpoint_dir: models/vif
  log_dir: logs/vif_training

# Ablation Study Presets
# ----------------------
# To run ablation studies, override specific sections:
#
# High-quality encoder ablation:
#   encoder:
#     model_name: all-mpnet-base-v2
#
# Smaller model ablation:
#   model:
#     hidden_dim: 128
#
# Deeper history ablation:
#   state_encoder:
#     window_size: 5
#     ema_alpha: 0.2
