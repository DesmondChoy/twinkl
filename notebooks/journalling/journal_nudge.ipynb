{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Journal Generation with Nudging\n",
    "\n",
    "This notebook extends the synthetic journal generation with a two-way conversational nudging system.\n",
    "When an entry is vague or potentially rich with unexplored tension, the system responds with a brief nudge that invites elaboration.\n",
    "\n",
    "**Design goal**: Nudges should feel like natural curiosity from a thoughtful companion, not interrogation or therapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import polars as pl\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Add project root to path for prompts module\n",
    "PROJECT_ROOT = (\n",
    "    Path(__file__).parent.parent if \"__file__\" in dir() else Path.cwd().parent\n",
    ")\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check for API Key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"WARNING: OPENAI_API_KEY not found in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs loaded successfully.\n",
      "Available Persona Attributes: ['age_ranges', 'cultures', 'professions', 'schwartz_values']\n",
      "Schwartz Values with elaborations: ['Self-Direction', 'Stimulation', 'Hedonism', 'Achievement', 'Power', 'Security', 'Conformity', 'Tradition', 'Benevolence', 'Universalism']\n",
      "Nudge config loaded: ['response_probability', 'response_modes', 'min_words', 'max_words']\n"
     ]
    }
   ],
   "source": [
    "# Configuration Loading\n",
    "CONFIG_PATH = Path(\"config/synthetic_data.yaml\")\n",
    "if not CONFIG_PATH.exists():\n",
    "    CONFIG_PATH = Path(\"../config/synthetic_data.yaml\")\n",
    "\n",
    "SCHWARTZ_VALUES_PATH = Path(\"config/schwartz_values.yaml\")\n",
    "if not SCHWARTZ_VALUES_PATH.exists():\n",
    "    SCHWARTZ_VALUES_PATH = Path(\"../config/schwartz_values.yaml\")\n",
    "\n",
    "\n",
    "def load_config(path: str | Path) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "config = load_config(CONFIG_PATH)\n",
    "schwartz_config = load_config(SCHWARTZ_VALUES_PATH)\n",
    "\n",
    "print(\"Configs loaded successfully.\")\n",
    "print(f\"Available Persona Attributes: {list(config['personas'].keys())}\")\n",
    "print(f\"Schwartz Values with elaborations: {list(schwartz_config['values'].keys())}\")\n",
    "print(f\"Nudge config loaded: {list(config['nudge'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models\n",
    "\n",
    "Extended models for conversational journaling with nudges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Base models (from journal_gen.ipynb)\nclass Persona(BaseModel):\n    name: str = Field(description=\"Full name of the persona\")\n    age: str\n    profession: str\n    culture: str\n    core_values: list[str] = Field(description=\"Top 3 Schwartz values\")\n    bio: str = Field(\n        description=\"A short paragraph describing their background, stressors, and goals\"\n    )\n\n\nclass JournalEntry(BaseModel):\n    \"\"\"LLM-generated journal entry. Metadata (tone, verbosity, etc.) tracked separately.\"\"\"\n\n    date: str\n    content: str\n\n\n# Nudge models and schemas imported from src/\nfrom src.models.nudge import NudgeCategory, NudgeResult, JournalTurn\nfrom src.nudge.schemas import (\n    NUDGE_DECISION_RESPONSE_FORMAT,\n    NUDGE_RESPONSE_FORMAT,\n    NUDGE_RESPONSE_RESPONSE_FORMAT,\n)\n\n\nclass ConversationalEntry(BaseModel):\n    \"\"\"Complete conversational exchange for one journaling session.\"\"\"\n\n    initial_entry: JournalEntry\n    nudge: NudgeResult | None = None\n    response: JournalTurn | None = None  # User's response to the nudge\n    # Metadata\n    tone: str\n    verbosity: str\n    reflection_mode: str\n    response_mode: str | None = None  # How the persona responded to the nudge\n\n\n# JSON schemas for OpenAI structured output (non-nudge only)\nPERSONA_SCHEMA = {\n    \"type\": \"object\",\n    \"additionalProperties\": False,\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"age\": {\"type\": \"string\"},\n        \"profession\": {\"type\": \"string\"},\n        \"culture\": {\"type\": \"string\"},\n        \"core_values\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n        \"bio\": {\"type\": \"string\"},\n    },\n    \"required\": [\"name\", \"age\", \"profession\", \"culture\", \"core_values\", \"bio\"],\n}\n\nJOURNAL_ENTRY_SCHEMA = {\n    \"type\": \"object\",\n    \"additionalProperties\": False,\n    \"properties\": {\n        \"date\": {\"type\": \"string\"},\n        \"content\": {\"type\": \"string\"},\n    },\n    \"required\": [\"date\", \"content\"],\n}\n\nPERSONA_RESPONSE_FORMAT = {\n    \"type\": \"json_schema\",\n    \"name\": \"Persona\",\n    \"schema\": PERSONA_SCHEMA,\n    \"strict\": True,\n}\n\nJOURNAL_ENTRY_RESPONSE_FORMAT = {\n    \"type\": \"json_schema\",\n    \"name\": \"JournalEntry\",\n    \"schema\": JOURNAL_ENTRY_SCHEMA,\n    \"strict\": True,\n}"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_value_context(values: list[str], schwartz_config: dict) -> str:\n",
    "    \"\"\"Build rich context about Schwartz values for persona generation.\n",
    "\n",
    "    Args:\n",
    "        values: List of Schwartz value names (e.g., [\"Achievement\", \"Benevolence\"])\n",
    "        schwartz_config: The loaded schwartz_values.yaml config\n",
    "\n",
    "    Returns:\n",
    "        Formatted string with value elaborations for prompt injection\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "\n",
    "    for value_name in values:\n",
    "        if value_name not in schwartz_config[\"values\"]:\n",
    "            continue\n",
    "\n",
    "        v = schwartz_config[\"values\"][value_name]\n",
    "\n",
    "        # Build a focused context block for this value\n",
    "        context_parts.append(f\"\"\"\n",
    "### {value_name}\n",
    "**Core Motivation:** {v[\"core_motivation\"].strip()}\n",
    "\n",
    "**How this manifests in behavior:**\n",
    "{chr(10).join(f\"- {b}\" for b in v[\"behavioral_manifestations\"][:5])}\n",
    "\n",
    "**Life domain expressions:**\n",
    "- Work: {v[\"life_domain_expressions\"][\"work\"].strip()}\n",
    "- Relationships: {v[\"life_domain_expressions\"][\"relationships\"].strip()}\n",
    "\n",
    "**Typical stressors for this person:**\n",
    "{chr(10).join(f\"- {s}\" for s in v[\"typical_stressors\"][:4])}\n",
    "\n",
    "**Typical goals:**\n",
    "{chr(10).join(f\"- {g}\" for g in v[\"typical_goals\"][:3])}\n",
    "\n",
    "**Internal conflicts they may experience:**\n",
    "{v[\"internal_conflicts\"].strip()}\n",
    "\n",
    "**Narrative guidance:**\n",
    "{v[\"persona_narrative_guidance\"].strip()}\n",
    "\"\"\")\n",
    "\n",
    "    return \"\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates are stored in prompts/ folder as YAML files\n",
    "# See prompts/__init__.py for the loader utility\n",
    "from prompts import (\n",
    "    persona_generation_prompt,\n",
    "    journal_entry_prompt,\n",
    "    nudge_decision_prompt,\n",
    "    nudge_generation_prompt,\n",
    "    nudge_response_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Client Setup\n",
    "\n",
    "Using `gpt-5-mini`. \n",
    "\n",
    "**Note:** GPT-5 models do not support `temperature` or `top_p` parameters. Instead, use the `reasoning` parameter to control how much the model \"thinks\" before responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI()\n",
    "MODEL_NAME = \"gpt-5-mini-2025-08-07\"\n",
    "# MODEL_NAME = \"gpt-5-nano-2025-08-07\"\n",
    "\n",
    "# Type alias for reasoning effort levels\n",
    "ReasoningEffort = Literal[\"minimal\", \"low\", \"medium\", \"high\"]\n",
    "\n",
    "# Default reasoning effort - change this to affect all generations\n",
    "DEFAULT_REASONING_EFFORT: ReasoningEffort = \"high\"\n",
    "\n",
    "\n",
    "async def generate_completion(\n",
    "    prompt: str,\n",
    "    response_format: dict | None = None,\n",
    ") -> str | None:\n",
    "    \"\"\"Generate a completion using the OpenAI Responses API (async).\n",
    "\n",
    "    Uses DEFAULT_REASONING_EFFORT to control how much the model \"thinks\".\n",
    "    Valid reasoning effort values: \"minimal\", \"low\", \"medium\", \"high\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        kwargs = {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"input\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"reasoning\": {\"effort\": DEFAULT_REASONING_EFFORT},\n",
    "        }\n",
    "\n",
    "        if response_format:\n",
    "            kwargs[\"text\"] = {\"format\": response_format}\n",
    "\n",
    "        response = await client.responses.create(**kwargs)\n",
    "        return response.output_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating completion: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _verbosity_targets(verbosity: str) -> tuple[int, int, int]:\n",
    "    \"\"\"Returns (min_words, max_words, max_paragraphs) as guidance for the LLM.\"\"\"\n",
    "    normalized = verbosity.strip().lower()\n",
    "    if normalized.startswith(\"short\"):\n",
    "        return 25, 80, 1\n",
    "    if normalized.startswith(\"medium\"):\n",
    "        return 90, 180, 2\n",
    "    return 160, 260, 3\n",
    "\n",
    "\n",
    "def _build_banned_pattern(banned_terms: list[str]) -> re.Pattern:\n",
    "    \"\"\"Build regex pattern to detect banned Schwartz value terms.\"\"\"\n",
    "    escaped = [re.escape(term) for term in banned_terms if term.strip()]\n",
    "    if not escaped:\n",
    "        return re.compile(r\"$^\")\n",
    "    return re.compile(r\"(?i)\\b(\" + \"|\".join(escaped) + r\")\\b\")\n",
    "\n",
    "\n",
    "def generate_date_sequence(\n",
    "    start_date: str, num_entries: int, min_days: int = 2, max_days: int = 10\n",
    ") -> list[str]:\n",
    "    \"\"\"Generate a sequence of dates with random intervals.\n",
    "\n",
    "    Args:\n",
    "        start_date: Starting date in YYYY-MM-DD format\n",
    "        num_entries: Number of dates to generate\n",
    "        min_days: Minimum days between entries\n",
    "        max_days: Maximum days between entries\n",
    "\n",
    "    Returns:\n",
    "        List of date strings in YYYY-MM-DD format\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    current = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "\n",
    "    for i in range(num_entries):\n",
    "        dates.append(current.strftime(\"%Y-%m-%d\"))\n",
    "        if i < num_entries - 1:\n",
    "            days_gap = random.randint(min_days, max_days)\n",
    "            current += timedelta(days=days_gap)\n",
    "\n",
    "    return dates\n",
    "\n",
    "\n",
    "# Banned terms include Schwartz value labels AND derivative adjectives\n",
    "SCHWARTZ_BANNED_TERMS = [\n",
    "    # Value labels\n",
    "    \"Self-Direction\",\n",
    "    \"Stimulation\",\n",
    "    \"Hedonism\",\n",
    "    \"Achievement\",\n",
    "    \"Power\",\n",
    "    \"Security\",\n",
    "    \"Conformity\",\n",
    "    \"Tradition\",\n",
    "    \"Benevolence\",\n",
    "    \"Universalism\",\n",
    "    # Derivative adjectives and related terms\n",
    "    \"self-directed\",\n",
    "    \"autonomous\",\n",
    "    \"stimulating\",\n",
    "    \"excited\",\n",
    "    \"hedonistic\",\n",
    "    \"hedonist\",\n",
    "    \"pleasure-seeking\",\n",
    "    \"achievement-oriented\",\n",
    "    \"ambitious\",\n",
    "    \"powerful\",\n",
    "    \"authoritative\",\n",
    "    \"secure\",\n",
    "    \"conformist\",\n",
    "    \"conforming\",\n",
    "    \"traditional\",\n",
    "    \"traditionalist\",\n",
    "    \"benevolent\",\n",
    "    \"kind-hearted\",\n",
    "    \"universalistic\",\n",
    "    \"altruistic\",\n",
    "    # Meta terms\n",
    "    \"Schwartz\",\n",
    "    \"values\",\n",
    "    \"core values\",\n",
    "]\n",
    "\n",
    "BANNED_PATTERN = _build_banned_pattern(SCHWARTZ_BANNED_TERMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nudge Decision Logic\n",
    "\n",
    "LLM-based classification to decide whether to nudge and which category.\n",
    "\n",
    "The LLM analyzes entry content semantically to detect:\n",
    "- **clarification** — Entry too vague to understand\n",
    "- **elaboration** — Solid entry with unexplored depth  \n",
    "- **tension_surfacing** — Hints at unresolved conflict\n",
    "- **no_nudge** — Entry is complete and grounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.nudge import should_suppress_nudge, decide_nudge, format_previous_entries\n\n\n# Test the LLM-based decision logic using the extracted module\nasync def test_nudge_decision():\n    test_entry = JournalEntry(date=\"2024-01-15\", content=\"Feeling off today.\")\n    should, category, reason = await decide_nudge(\n        entry_content=test_entry.content,\n        entry_date=test_entry.date,\n        previous_entries=None,\n        llm_complete=generate_completion,\n    )\n    print(\n        f\"Test vague entry: should_nudge={should}, category={category}, reason={reason}\"\n    )\n\n    test_entry2 = JournalEntry(\n        date=\"2024-01-15\",\n        content=\"Had a meeting with the team about the project deadline. It was fine, I guess. We sorted out the schedule.\",\n    )\n    should2, category2, reason2 = await decide_nudge(\n        entry_content=test_entry2.content,\n        entry_date=test_entry2.date,\n        previous_entries=None,\n        llm_complete=generate_completion,\n    )\n    print(\n        f\"Test hedging entry: should_nudge={should2}, category={category2}, reason={reason2}\"\n    )\n\n\nawait test_nudge_decision()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nudge Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.nudge import generate_nudge_text"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nudge Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.nudge import select_response_mode, generate_nudge_response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass ConversationalPipelineResult:\n    \"\"\"Complete results from one persona's conversational generation pipeline.\"\"\"\n\n    persona_id: int\n    persona: Persona | None\n    entries: list[ConversationalEntry]\n    persona_prompt: str\n    entry_prompts: list[str]\n    nudge_prompts: list[str] = field(default_factory=list)\n    response_prompts: list[str] = field(default_factory=list)\n    error: str | None = None\n\n\nasync def create_random_persona(\n    config: dict, schwartz_config: dict, max_attempts: int = 2\n) -> tuple[Persona | None, str]:\n    \"\"\"Generate a random persona with Schwartz values shown through life circumstances.\"\"\"\n    age = random.choice(config[\"personas\"][\"age_ranges\"])\n    prof = random.choice(config[\"personas\"][\"professions\"])\n    cult = random.choice(config[\"personas\"][\"cultures\"])\n    num_values = random.choice([1, 2])\n    vals = random.sample(config[\"personas\"][\"schwartz_values\"], num_values)\n\n    # Build rich value context from the Schwartz elaborations\n    value_context = build_value_context(vals, schwartz_config)\n\n    prompt = persona_generation_prompt.render(\n        age=age,\n        profession=prof,\n        culture=cult,\n        values=vals,\n        value_context=value_context,\n        banned_terms=SCHWARTZ_BANNED_TERMS,\n    )\n\n    first_person_pattern = re.compile(r\"(?i)\\b(i|my|me)\\b\")\n    last_persona: Persona | None = None\n\n    for _ in range(max_attempts):\n        raw_json = await generate_completion(\n            prompt, response_format=PERSONA_RESPONSE_FORMAT\n        )\n        if not raw_json:\n            continue\n\n        data = json.loads(raw_json)\n        data[\"core_values\"] = vals  # Ensure correct values\n        persona = Persona(**data)\n        last_persona = persona\n\n        # Only validate banned terms and first-person usage\n        if BANNED_PATTERN.search(persona.bio) or first_person_pattern.search(\n            persona.bio\n        ):\n            continue\n        return persona, prompt\n\n    return last_persona, prompt\n\n\nasync def generate_journal_entry(\n    persona: Persona,\n    config: dict,\n    date_str: str,\n    previous_entries: list[JournalEntry] | None = None,\n    max_attempts: int = 2,\n) -> tuple[tuple[JournalEntry, str, str, str] | None, str]:\n    \"\"\"Generate a journal entry for a persona on a given date.\n\n    Returns:\n        Tuple of ((entry, tone, verbosity, reflection_mode) or None, prompt used)\n    \"\"\"\n    tone = random.choice(config[\"journal_entries\"][\"tones\"])\n    verbosity = random.choice(config[\"journal_entries\"][\"verbosity\"])\n    reflection_mode = random.choice(config[\"journal_entries\"][\"reflection_mode\"])\n    min_words, max_words, max_paragraphs = _verbosity_targets(verbosity)\n\n    # Format previous entries for the prompt\n    prev_entries_data = None\n    if previous_entries:\n        prev_entries_data = [\n            {\"date\": e.date, \"content\": e.content} for e in previous_entries\n        ]\n\n    prompt = journal_entry_prompt.render(\n        name=persona.name,\n        age=persona.age,\n        profession=persona.profession,\n        culture=persona.culture,\n        bio=persona.bio,\n        date=date_str,\n        tone=tone,\n        verbosity=verbosity,\n        min_words=min_words,\n        max_words=max_words,\n        max_paragraphs=max_paragraphs,\n        reflection_mode=reflection_mode,\n        previous_entries=prev_entries_data,\n    )\n\n    last_entry: JournalEntry | None = None\n\n    for _ in range(max_attempts):\n        raw_json = await generate_completion(\n            prompt, response_format=JOURNAL_ENTRY_RESPONSE_FORMAT\n        )\n        if not raw_json:\n            continue\n\n        entry = JournalEntry(**json.loads(raw_json))\n        last_entry = entry\n\n        # Only validate banned terms (prevent label leakage)\n        if not BANNED_PATTERN.search(entry.content):\n            return (entry, tone, verbosity, reflection_mode), prompt\n\n    if last_entry:\n        return (last_entry, tone, verbosity, reflection_mode), prompt\n    return None, prompt\n\n\nasync def generate_conversational_entry(\n    persona: Persona,\n    config: dict,\n    date_str: str,\n    previous_entries: list[ConversationalEntry] | None = None,\n) -> tuple[ConversationalEntry | None, str, str | None, str | None]:\n    \"\"\"Generate entry, decide on nudge, optionally generate response.\n\n    Returns:\n        Tuple of (ConversationalEntry or None, entry_prompt, nudge_prompt, response_prompt)\n    \"\"\"\n    # Step 1: Generate initial entry\n    prev_journal_entries = [e.initial_entry for e in (previous_entries or [])]\n    entry_result, entry_prompt = await generate_journal_entry(\n        persona, config, date_str, previous_entries=prev_journal_entries\n    )\n\n    if not entry_result:\n        return None, entry_prompt, None, None\n\n    entry, tone, verbosity, reflection_mode = entry_result\n\n    # Step 2: Anti-annoyance check (pure, no LLM call needed)\n    if previous_entries and should_suppress_nudge(\n        [e.nudge is not None for e in previous_entries]\n    ):\n        return (\n            ConversationalEntry(\n                initial_entry=entry,\n                nudge=None,\n                response=None,\n                tone=tone,\n                verbosity=verbosity,\n                reflection_mode=reflection_mode,\n                response_mode=None,\n            ),\n            entry_prompt,\n            None,\n            None,\n        )\n\n    # Step 3: Decide whether to nudge (LLM-based semantic classification)\n    # format_previous_entries strips metadata — only date + content pass through\n    prev_data = format_previous_entries(\n        [\n            {\"date\": e.initial_entry.date, \"content\": e.initial_entry.content}\n            for e in (previous_entries or [])\n        ]\n    )\n\n    do_nudge, nudge_category, trigger_reason = await decide_nudge(\n        entry_content=entry.content,\n        entry_date=entry.date,\n        previous_entries=prev_data,\n        llm_complete=generate_completion,\n    )\n\n    nudge_result = None\n    response = None\n    nudge_prompt = None\n    response_prompt = None\n    response_mode = None\n\n    if do_nudge and nudge_category:\n        # Step 4: Generate nudge text\n        nudge_text, nudge_prompt = await generate_nudge_text(\n            entry_content=entry.content,\n            entry_date=entry.date,\n            category=nudge_category,\n            previous_entries=prev_data,\n            config=config,\n            llm_complete=generate_completion,\n        )\n\n        if nudge_text:\n            nudge_result = NudgeResult(\n                nudge_text=nudge_text,\n                nudge_category=nudge_category,\n                trigger_reason=trigger_reason or \"\",\n            )\n\n            # Step 5: Decide if persona responds (probabilistic)\n            if random.random() < config[\"nudge\"][\"response_probability\"]:\n                (\n                    response,\n                    response_prompt,\n                    response_mode,\n                ) = await generate_nudge_response(\n                    persona_name=persona.name,\n                    persona_age=persona.age,\n                    persona_profession=persona.profession,\n                    persona_culture=persona.culture,\n                    persona_bio=persona.bio,\n                    entry_content=entry.content,\n                    entry_date=entry.date,\n                    nudge_text=nudge_text,\n                    config=config,\n                    llm_complete=generate_completion,\n                )\n                if response:\n                    nudge_result.was_responded_to = True\n\n    return (\n        ConversationalEntry(\n            initial_entry=entry,\n            nudge=nudge_result,\n            response=response,\n            tone=tone,\n            verbosity=verbosity,\n            reflection_mode=reflection_mode,\n            response_mode=response_mode,\n        ),\n        entry_prompt,\n        nudge_prompt,\n        response_prompt,\n    )\n\n\nasync def generate_conversational_pipeline(\n    persona_id: int,\n    config: dict,\n    schwartz_config: dict,\n    num_entries: int = 3,\n    start_date: str = \"2023-10-27\",\n) -> ConversationalPipelineResult:\n    \"\"\"Generate one persona and all their conversational journal entries.\"\"\"\n    entry_prompts: list[str] = []\n    nudge_prompts: list[str] = []\n    response_prompts: list[str] = []\n    entries: list[ConversationalEntry] = []\n\n    # 1. Generate persona\n    persona, persona_prompt = await create_random_persona(config, schwartz_config)\n\n    if not persona:\n        return ConversationalPipelineResult(\n            persona_id=persona_id,\n            persona=None,\n            entries=[],\n            persona_prompt=persona_prompt,\n            entry_prompts=[],\n            error=\"Failed to generate persona\",\n        )\n\n    # 2. Generate conversational entries sequentially\n    dates = generate_date_sequence(start_date, num_entries)\n\n    for date_str in dates:\n        (\n            conv_entry,\n            entry_prompt,\n            nudge_prompt,\n            response_prompt,\n        ) = await generate_conversational_entry(\n            persona, config, date_str, previous_entries=entries\n        )\n        entry_prompts.append(entry_prompt)\n        if nudge_prompt:\n            nudge_prompts.append(nudge_prompt)\n        if response_prompt:\n            response_prompts.append(response_prompt)\n\n        if conv_entry:\n            entries.append(conv_entry)\n\n    return ConversationalPipelineResult(\n        persona_id=persona_id,\n        persona=persona,\n        entries=entries,\n        persona_prompt=persona_prompt,\n        entry_prompts=entry_prompts,\n        nudge_prompts=nudge_prompts,\n        response_prompts=response_prompts,\n        error=None,\n    )\n\n\nasync def run_parallel_conversational_personas(\n    num_personas: int,\n    config: dict,\n    schwartz_config: dict,\n    min_entries: int = 3,\n    max_entries: int = 10,\n    start_date: str = \"2023-10-27\",\n) -> list[ConversationalPipelineResult | Exception]:\n    \"\"\"Run multiple conversational persona pipelines in parallel.\n\n    Args:\n        num_personas: Number of personas to generate in parallel\n        config: Main configuration dict\n        schwartz_config: Schwartz values elaboration config\n        min_entries: Minimum journal entries per persona\n        max_entries: Maximum journal entries per persona\n        start_date: Starting date for journal entries\n\n    Returns:\n        List of ConversationalPipelineResult or Exception, in persona order\n    \"\"\"\n    # Each persona gets a random number of entries for training diversity\n    tasks = [\n        generate_conversational_pipeline(\n            i + 1,\n            config,\n            schwartz_config,\n            num_entries=random.randint(min_entries, max_entries),\n            start_date=start_date,\n        )\n        for i in range(num_personas)\n    ]\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return list(results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Logging System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_log_dir() -> Path:\n    \"\"\"Create and return a timestamped log directory.\"\"\"\n    base_dir = Path(\"logs/synthetic_data\")\n    if not base_dir.exists():\n        base_dir = Path(\"../logs/synthetic_data\")\n    base_dir.mkdir(parents=True, exist_ok=True)\n\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    log_dir = base_dir / timestamp\n    log_dir.mkdir(exist_ok=True)\n    return log_dir\n\n\ndef write_config_log(\n    log_dir: Path, config: dict, num_personas: int, min_entries: int, max_entries: int\n) -> None:\n    \"\"\"Write config.md with run parameters.\"\"\"\n    content = f\"\"\"# Run Configuration\n\n**Timestamp**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n**Notebook**: journal_nudge.ipynb\n\n## Persona Generation\n- Num personas: {num_personas}\n- Entries per persona: {min_entries}-{max_entries} (variable)\n\n## Nudge Settings\n- Decision method: LLM-based classification (prompts/nudge_decision.yaml)\n- Response probability: {config[\"nudge\"][\"response_probability\"]}\n\n## Model Settings\n- Model: {MODEL_NAME}\n- Reasoning effort: {DEFAULT_REASONING_EFFORT}\n\"\"\"\n    (log_dir / \"config.md\").write_text(content)\n\n\ndef write_persona_log(log_dir: Path, result: ConversationalPipelineResult) -> None:\n    \"\"\"Write persona_XXX.md with all entries, nudges, and summary statistics.\"\"\"\n    if not result.persona:\n        return\n\n    p = result.persona\n    lines = [\n        f\"# Persona {result.persona_id:03d}: {p.name}\",\n        \"\",\n        \"## Profile\",\n        f\"- Age: {p.age}\",\n        f\"- Profession: {p.profession}\",\n        f\"- Culture: {p.culture}\",\n        f\"- Core Values: {', '.join(p.core_values)}\",\n        f\"- Bio: {p.bio}\",\n        \"\",\n        \"---\",\n    ]\n\n    for i, entry in enumerate(result.entries, 1):\n        lines.extend(\n            [\n                \"\",\n                f\"## Entry {i} - {entry.initial_entry.date}\",\n                \"\",\n                \"### Initial Entry\",\n                f\"**Tone**: {entry.tone} | **Verbosity**: {entry.verbosity} | **Reflection Mode**: {entry.reflection_mode}\",\n                \"\",\n                entry.initial_entry.content,\n            ]\n        )\n\n        if entry.nudge:\n            lines.extend(\n                [\n                    \"\",\n                    f\"### Nudge ({entry.nudge.nudge_category.replace('_', ' ').title()})\",\n                    f\"**Trigger**: {entry.nudge.trigger_reason}\",\n                    \"\",\n                    f'\"{entry.nudge.nudge_text}\"',\n                ]\n            )\n\n            if entry.response:\n                lines.extend(\n                    [\n                        \"\",\n                        \"### Response\",\n                        f\"**Mode**: {entry.response_mode or 'Unknown'}\",\n                        \"\",\n                        entry.response.content,\n                    ]\n                )\n            else:\n                lines.append(\"\\n*(No response - persona did not reply to nudge)*\")\n        else:\n            lines.append(\"\\n*(No nudge for this entry)*\")\n\n        lines.extend([\"\", \"---\"])\n\n    # Summary Statistics\n    nudge_count = sum(1 for e in result.entries if e.nudge)\n    response_count = sum(1 for e in result.entries if e.nudge and e.response)\n\n    # Count nudge categories\n    category_counts = {\"clarification\": 0, \"elaboration\": 0, \"tension_surfacing\": 0}\n    for e in result.entries:\n        if e.nudge:\n            category_counts[e.nudge.nudge_category] += 1\n\n    # Count response modes\n    mode_counts = {\n        \"Answering directly\": 0,\n        \"Deflecting/redirecting\": 0,\n        \"Revealing deeper thought\": 0,\n    }\n    for e in result.entries:\n        if e.response and e.response_mode:\n            if e.response_mode in mode_counts:\n                mode_counts[e.response_mode] += 1\n\n    lines.extend(\n        [\n            \"\",\n            \"## Summary Statistics\",\n            \"\",\n            \"| Metric | Value |\",\n            \"|--------|-------|\",\n            f\"| Total Entries | {len(result.entries)} |\",\n            f\"| Nudges Generated | {nudge_count} |\",\n            f\"| Responses Given | {response_count} |\",\n            f\"| Nudge Categories | clarification ({category_counts['clarification']}), elaboration ({category_counts['elaboration']}), tension_surfacing ({category_counts['tension_surfacing']}) |\",\n            f\"| Response Modes | Answering directly ({mode_counts['Answering directly']}), Deflecting ({mode_counts['Deflecting/redirecting']}), Revealing deeper ({mode_counts['Revealing deeper thought']}) |\",\n        ]\n    )\n\n    (log_dir / f\"persona_{result.persona_id:03d}.md\").write_text(\"\\n\".join(lines))\n\n\ndef write_prompts_log(\n    log_dir: Path, results: list[ConversationalPipelineResult]\n) -> None:\n    \"\"\"Write prompts.md with all LLM prompts.\"\"\"\n    lines = [\"# Prompts Log\", \"\"]\n\n    for result in results:\n        if isinstance(result, Exception) or not result.persona:\n            continue\n\n        lines.extend(\n            [\n                f\"## Persona {result.persona_id:03d}: {result.persona.name}\",\n                \"\",\n                \"### Persona Generation Prompt\",\n                \"```\",\n                result.persona_prompt,\n                \"```\",\n                \"\",\n            ]\n        )\n\n        for i, prompt in enumerate(result.entry_prompts, 1):\n            lines.extend(\n                [\n                    f\"### Entry {i} - Initial Entry Prompt\",\n                    \"```\",\n                    prompt,\n                    \"```\",\n                    \"\",\n                ]\n            )\n\n        if result.nudge_prompts:\n            for i, prompt in enumerate(result.nudge_prompts, 1):\n                lines.extend(\n                    [\n                        f\"### Nudge Prompt {i}\",\n                        \"```\",\n                        prompt,\n                        \"```\",\n                        \"\",\n                    ]\n                )\n\n        if result.response_prompts:\n            for i, prompt in enumerate(result.response_prompts, 1):\n                lines.extend(\n                    [\n                        f\"### Response Prompt {i}\",\n                        \"```\",\n                        prompt,\n                        \"```\",\n                        \"\",\n                    ]\n                )\n\n        lines.append(\"---\\n\")\n\n    (log_dir / \"prompts.md\").write_text(\"\\n\".join(lines))\n\n\ndef save_run_logs(\n    results: list[ConversationalPipelineResult | Exception],\n    config: dict,\n    num_personas: int,\n    min_entries: int,\n    max_entries: int,\n) -> Path:\n    \"\"\"Save all logs for a run.\n\n    Returns:\n        Path to the log directory\n    \"\"\"\n    log_dir = get_log_dir()\n\n    # Filter successful results\n    successful = [\n        r for r in results if isinstance(r, ConversationalPipelineResult) and r.persona\n    ]\n\n    write_config_log(log_dir, config, num_personas, min_entries, max_entries)\n\n    for result in successful:\n        write_persona_log(log_dir, result)\n\n    write_prompts_log(log_dir, successful)\n\n    print(f\"Logs saved to: {log_dir}\")\n    return log_dir"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_conversational_results(\n",
    "    result: ConversationalPipelineResult | Exception,\n",
    ") -> None:\n",
    "    \"\"\"Display all outputs for one persona.\"\"\"\n",
    "    if isinstance(result, Exception):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"PERSONA FAILED WITH EXCEPTION:\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        print(f\"{type(result).__name__}: {result}\")\n",
    "        print(f\"{'=' * 80}\\n\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PERSONA {result.persona_id}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    if result.error:\n",
    "        print(f\"\\nError: {result.error}\")\n",
    "        return\n",
    "\n",
    "    # Persona details\n",
    "    p = result.persona\n",
    "    print(f\"\\n## Generated Persona: {p.name}\")\n",
    "    print(f\"Age: {p.age} | Profession: {p.profession} | Culture: {p.culture}\")\n",
    "    print(f\"Values: {', '.join(p.core_values)}\")\n",
    "    print(f\"Bio: {p.bio}\")\n",
    "\n",
    "    # Entries with nudges\n",
    "    for i, entry in enumerate(result.entries, 1):\n",
    "        print(f\"\\n{'─' * 40}\")\n",
    "        print(f\"### Entry {i}: {entry.initial_entry.date}\")\n",
    "        print(\n",
    "            f\"Tone: {entry.tone} | Verbosity: {entry.verbosity} | Mode: {entry.reflection_mode}\"\n",
    "        )\n",
    "        print(f\"\\n**Initial Entry:**\")\n",
    "        print(entry.initial_entry.content)\n",
    "\n",
    "        if entry.nudge:\n",
    "            print(f\"\\n**Nudge ({entry.nudge.nudge_category}):**\")\n",
    "            print(f\"Trigger: {entry.nudge.trigger_reason}\")\n",
    "            print(f'\"{entry.nudge.nudge_text}\"')\n",
    "\n",
    "            if entry.response:\n",
    "                print(f\"\\n**Response:**\")\n",
    "                print(entry.response.content)\n",
    "            else:\n",
    "                print(\"\\n*(No response)*\")\n",
    "        else:\n",
    "            print(\"\\n*(No nudge)*\")\n",
    "\n",
    "    # Summary stats\n",
    "    nudge_count = sum(1 for e in result.entries if e.nudge)\n",
    "    response_count = sum(1 for e in result.entries if e.nudge and e.response)\n",
    "    print(f\"\\n{'─' * 40}\")\n",
    "    print(f\"### Summary for {p.name}\")\n",
    "    print(f\"Total entries: {len(result.entries)}\")\n",
    "    print(f\"Nudges given: {nudge_count}\")\n",
    "    print(f\"Responses received: {response_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nNUM_PERSONAS = 3\nMIN_ENTRIES = 3\nMAX_ENTRIES = 10\nSTART_DATE = \"2025-10-25\"\n\nprint(f\"Generating {NUM_PERSONAS} personas with conversational journaling...\")\nprint(f\"Each persona will have {MIN_ENTRIES}-{MAX_ENTRIES} entries with potential nudges.\")\nprint(f\"Model: {MODEL_NAME} | Reasoning: {DEFAULT_REASONING_EFFORT}\")\nprint(f\"Start date: {START_DATE}\")\nprint(f\"Nudge decision: LLM-based classification (prompts/nudge_decision.yaml)\")\nprint(f\"Response probability: {config['nudge']['response_probability']}\\n\")\n\n# Run all personas in parallel\nresults = await run_parallel_conversational_personas(\n    num_personas=NUM_PERSONAS,\n    config=config,\n    schwartz_config=schwartz_config,\n    min_entries=MIN_ENTRIES,\n    max_entries=MAX_ENTRIES,\n    start_date=START_DATE,\n)\n\n# Display results\nfor result in results:\n    display_conversational_results(result)\n\n# Save logs\nsuccessful_results = [\n    r for r in results if isinstance(r, ConversationalPipelineResult) and r.persona\n]\nlog_dir = save_run_logs(results, config, NUM_PERSONAS, MIN_ENTRIES, MAX_ENTRIES)\n\n# Final summary\nprint(f\"\\n{'=' * 80}\")\nprint(f\"FINAL SUMMARY\")\nprint(f\"{'=' * 80}\")\nprint(f\"Successfully generated: {len(successful_results)}/{NUM_PERSONAS} personas\")\n\ntotal_entries = sum(len(r.entries) for r in successful_results)\nentry_counts = [len(r.entries) for r in successful_results]\ntotal_nudges = sum(sum(1 for e in r.entries if e.nudge) for r in successful_results)\ntotal_responses = sum(\n    sum(1 for e in r.entries if e.nudge and e.response) for r in successful_results\n)\n\nprint(f\"Total entries: {total_entries}\")\nif entry_counts:\n    print(f\"Entries per persona: min={min(entry_counts)}, max={max(entry_counts)}, avg={sum(entry_counts)/len(entry_counts):.1f}\")\nprint(f\"Total nudges given: {total_nudges}\")\nprint(f\"Total responses: {total_responses}\")\nif total_nudges > 0:\n    print(f\"Response rate: {total_responses / total_nudges:.1%}\")\nprint(f\"\\nLogs saved to: {log_dir}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twinkl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
