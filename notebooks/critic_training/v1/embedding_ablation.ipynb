{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Embedding Model Ablation Study\n",
    "\n",
    "This notebook systematically evaluates whether the current embedding model (`all-MiniLM-L6-v2`) is leaving prediction performance on the table, and quantifies the impact of alternative encoders on the VIF Critic.\n",
    "\n",
    "**Motivation:**  \n",
    "The embedding model is the first link in the VIF inference chain:\n",
    "\n",
    "```\n",
    "Journal Entry → [Embedding Model] → State Vector → [Critic MLP] → Alignment Scores\n",
    "```\n",
    "\n",
    "Research shows `all-MiniLM-L6-v2` has known emotional blindness (e.g. \"I love reading\" vs \"I hate reading\" → 0.59 cosine similarity). Since Twinkl detects value alignment through emotional nuance in journal entries, this blindness may directly constrain downstream performance.\n",
    "\n",
    "**Experiments:**\n",
    "1. **Embedding Quality Probe** — Do the encoders differentiate emotionally opposite value statements?\n",
    "2. **End-to-End Ablation** — Train identical CriticMLP models with different encoders, compare on the same test set\n",
    "3. **Matryoshka Dimension Search** — For models supporting it, find the optimal embedding dimension\n",
    "4. **Latency Benchmark** — Measure encoding speed for practical deployment\n",
    "\n",
    "**Models Under Test:**\n",
    "\n",
    "| Model | Dims | Max Tokens | MTEB | Notes |\n",
    "|-------|------|------------|------|-------|\n",
    "| `all-MiniLM-L6-v2` | 384 | 256 | ~56.3 | Current baseline |\n",
    "| `all-mpnet-base-v2` | 768 | 512 | ~60.0 | Easy upgrade, same library |\n",
    "| `nomic-ai/nomic-embed-text-v1.5` | 768 | 8192 | ~62.3 | Long-context, Matryoshka support |\n",
    "\n",
    "**Contents:**\n",
    "1. Setup & Imports\n",
    "2. Embedding Quality Probe (Emotional Discrimination)\n",
    "3. Data Loading (Shared Splits)\n",
    "4. End-to-End Encoder Ablation\n",
    "5. Side-by-Side Results\n",
    "6. Per-Dimension Deep Dive\n",
    "7. Matryoshka Dimension Search\n",
    "8. Latency Benchmark\n",
    "9. Conclusions & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Walk up to find project root (contains src/ and pyproject.toml)\n",
    "_dir = Path.cwd()\n",
    "while _dir != _dir.parent:\n",
    "    if (_dir / \"src\").is_dir() and (_dir / \"pyproject.toml\").is_file():\n",
    "        os.chdir(_dir)\n",
    "        break\n",
    "    _dir = _dir.parent\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.spatial.distance import cosine as cosine_dist\n",
    "from scipy import stats\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "probe-header",
   "metadata": {},
   "source": [
    "## 1. Embedding Quality Probe\n",
    "\n",
    "Before training any models, test whether each encoder can distinguish emotionally opposite statements about the same topic — the core signal Twinkl needs for value detection.\n",
    "\n",
    "**Test Design:**\n",
    "- **Pairs of value-laden statements** where the topic is identical but the value alignment flips\n",
    "- A good encoder should give **low** cosine similarity to opposite pairs and **high** similarity to semantically equivalent pairs\n",
    "- Covers each Schwartz value dimension with realistic journal-style text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probe-pairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value-opposite pairs: same topic, flipped alignment\n",
    "# Format: (positive_alignment, negative_alignment, value_dimension)\n",
    "PROBE_PAIRS = [\n",
    "    # Known failing case from literature\n",
    "    (\"I love reading books\", \"I hate reading books\", \"sentiment_control\"),\n",
    "    # Benevolence vs Power\n",
    "    (\"I helped my colleague succeed at their presentation\",\n",
    "     \"I ensured my colleague knew their place during the presentation\",\n",
    "     \"benevolence_vs_power\"),\n",
    "    # Conformity vs Self-Direction\n",
    "    (\"I followed the team's decision even though I disagreed\",\n",
    "     \"I pushed back on the team's decision and went my own way\",\n",
    "     \"conformity_vs_self_direction\"),\n",
    "    # Security vs Stimulation\n",
    "    (\"I stayed with the familiar routine because it felt safe\",\n",
    "     \"I tried something completely new and unpredictable today\",\n",
    "     \"security_vs_stimulation\"),\n",
    "    # Tradition vs Self-Direction\n",
    "    (\"I honored the customs my family has always followed\",\n",
    "     \"I broke with tradition and chose my own path forward\",\n",
    "     \"tradition_vs_self_direction\"),\n",
    "    # Achievement (positive vs negative)\n",
    "    (\"I pushed myself hard and achieved my ambitious goal today\",\n",
    "     \"I let the opportunity pass because the effort wasn't worth it\",\n",
    "     \"achievement\"),\n",
    "    # Universalism (positive vs negative)\n",
    "    (\"I volunteered for the environmental cleanup because every ecosystem matters\",\n",
    "     \"I skipped the cleanup because it's not my problem to solve\",\n",
    "     \"universalism\"),\n",
    "    # Hedonism (positive vs negative)\n",
    "    (\"I indulged in a long luxurious evening doing exactly what I enjoy\",\n",
    "     \"I forced myself through another joyless evening of obligations\",\n",
    "     \"hedonism\"),\n",
    "    # Nuanced benevolence\n",
    "    (\"Listening to my friend's struggles reminded me how much I care about their wellbeing\",\n",
    "     \"Listening to my friend's struggles reminded me how draining other people's problems are\",\n",
    "     \"benevolence_nuance\"),\n",
    "    # Power (positive vs negative)\n",
    "    (\"Leading the meeting felt natural — I thrive when I'm in control\",\n",
    "     \"Leading the meeting felt exhausting — I'd rather someone else take charge\",\n",
    "     \"power_nuance\"),\n",
    "]\n",
    "\n",
    "# Semantic equivalence pairs (should have HIGH similarity)\n",
    "EQUIVALENT_PAIRS = [\n",
    "    (\"I helped my colleague with their work\",\n",
    "     \"I assisted my coworker on their project\",\n",
    "     \"benevolence_equivalent\"),\n",
    "    (\"I took a risk and tried something new\",\n",
    "     \"I stepped outside my comfort zone today\",\n",
    "     \"stimulation_equivalent\"),\n",
    "    (\"I followed the rules even when it was hard\",\n",
    "     \"I stayed compliant with the guidelines despite wanting to deviate\",\n",
    "     \"conformity_equivalent\"),\n",
    "]\n",
    "\n",
    "print(f\"Opposite pairs: {len(PROBE_PAIRS)}\")\n",
    "print(f\"Equivalent pairs: {len(EQUIVALENT_PAIRS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-encoders",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define encoders to test\n",
    "ENCODER_CONFIGS = {\n",
    "    \"MiniLM-L6 (baseline)\": {\n",
    "        \"model_name\": \"all-MiniLM-L6-v2\",\n",
    "        \"prefix\": \"\",  # No prefix needed\n",
    "    },\n",
    "    \"MPNet-base\": {\n",
    "        \"model_name\": \"all-mpnet-base-v2\",\n",
    "        \"prefix\": \"\",\n",
    "    },\n",
    "    \"Nomic-v1.5\": {\n",
    "        \"model_name\": \"nomic-ai/nomic-embed-text-v1.5\",\n",
    "        \"prefix\": \"search_document: \",  # Nomic requires task prefix\n",
    "    },\n",
    "}\n",
    "\n",
    "# Load all encoder models\n",
    "encoder_models = {}\n",
    "for name, cfg in ENCODER_CONFIGS.items():\n",
    "    print(f\"Loading {name} ({cfg['model_name']})...\")\n",
    "    model = SentenceTransformer(cfg[\"model_name\"], trust_remote_code=True)\n",
    "    encoder_models[name] = model\n",
    "    dim = model.get_sentence_embedding_dimension()\n",
    "    print(f\"  Embedding dim: {dim}\")\n",
    "\n",
    "print(\"\\nAll encoders loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(emb1, emb2):\n",
    "    \"\"\"Cosine similarity between two embeddings.\"\"\"\n",
    "    return 1 - cosine_dist(emb1, emb2)\n",
    "\n",
    "\n",
    "def run_probe(encoder_name, model, pairs, prefix=\"\"):\n",
    "    \"\"\"Run probe pairs through an encoder and return similarities.\"\"\"\n",
    "    results = []\n",
    "    for text_a, text_b, label in pairs:\n",
    "        emb_a = model.encode(prefix + text_a)\n",
    "        emb_b = model.encode(prefix + text_b)\n",
    "        sim = compute_cosine_similarity(emb_a, emb_b)\n",
    "        results.append({\"label\": label, \"similarity\": sim})\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run probes for all encoders\n",
    "probe_results = {}  # encoder_name -> {\"opposite\": [...], \"equivalent\": [...]}\n",
    "\n",
    "for name, cfg in ENCODER_CONFIGS.items():\n",
    "    model = encoder_models[name]\n",
    "    prefix = cfg[\"prefix\"]\n",
    "\n",
    "    opposite = run_probe(name, model, PROBE_PAIRS, prefix)\n",
    "    equivalent = run_probe(name, model, EQUIVALENT_PAIRS, prefix)\n",
    "\n",
    "    probe_results[name] = {\"opposite\": opposite, \"equivalent\": equivalent}\n",
    "\n",
    "# Display results table\n",
    "print(\"=\" * 90)\n",
    "print(\"EMBEDDING QUALITY PROBE: Emotionally Opposite Pairs\")\n",
    "print(\"(Lower similarity = better discrimination)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Pair':<30}\", end=\"\")\n",
    "for name in ENCODER_CONFIGS:\n",
    "    print(f\"{name:>20}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i, (text_a, text_b, label) in enumerate(PROBE_PAIRS):\n",
    "    print(f\"{label:<30}\", end=\"\")\n",
    "    for name in ENCODER_CONFIGS:\n",
    "        sim = probe_results[name][\"opposite\"][i][\"similarity\"]\n",
    "        print(f\"{sim:>20.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"-\" * 90)\n",
    "# Mean opposite similarity\n",
    "print(f\"{'MEAN (opposite):':<30}\", end=\"\")\n",
    "for name in ENCODER_CONFIGS:\n",
    "    mean_sim = np.mean([r[\"similarity\"] for r in probe_results[name][\"opposite\"]])\n",
    "    print(f\"{mean_sim:>20.3f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"SEMANTIC EQUIVALENCE PAIRS (Higher = better)\")\n",
    "print(\"-\" * 90)\n",
    "for i, (text_a, text_b, label) in enumerate(EQUIVALENT_PAIRS):\n",
    "    print(f\"{label:<30}\", end=\"\")\n",
    "    for name in ENCODER_CONFIGS:\n",
    "        sim = probe_results[name][\"equivalent\"][i][\"similarity\"]\n",
    "        print(f\"{sim:>20.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'MEAN (equivalent):':<30}\", end=\"\")\n",
    "for name in ENCODER_CONFIGS:\n",
    "    mean_sim = np.mean([r[\"similarity\"] for r in probe_results[name][\"equivalent\"]])\n",
    "    print(f\"{mean_sim:>20.3f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "# Discrimination gap\n",
    "print()\n",
    "print(f\"{'DISCRIMINATION GAP:':<30}\", end=\"\")\n",
    "for name in ENCODER_CONFIGS:\n",
    "    mean_opp = np.mean([r[\"similarity\"] for r in probe_results[name][\"opposite\"]])\n",
    "    mean_eq = np.mean([r[\"similarity\"] for r in probe_results[name][\"equivalent\"]])\n",
    "    gap = mean_eq - mean_opp\n",
    "    print(f\"{gap:>20.3f}\", end=\"\")\n",
    "print()\n",
    "print(\"(Higher gap = better at distinguishing opposite vs equivalent meaning)\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probe-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probe results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "encoder_names = list(ENCODER_CONFIGS.keys())\n",
    "x = np.arange(len(PROBE_PAIRS))\n",
    "width = 0.25\n",
    "\n",
    "# Opposite pairs\n",
    "for i, name in enumerate(encoder_names):\n",
    "    sims = [r[\"similarity\"] for r in probe_results[name][\"opposite\"]]\n",
    "    axes[0].bar(x + i * width, sims, width, label=name, alpha=0.85)\n",
    "\n",
    "axes[0].set_ylabel(\"Cosine Similarity\")\n",
    "axes[0].set_title(\"Opposite Pairs (Lower = Better Discrimination)\")\n",
    "axes[0].set_xticks(x + width)\n",
    "pair_labels = [p[2] for p in PROBE_PAIRS]\n",
    "axes[0].set_xticklabels(pair_labels, rotation=45, ha=\"right\", fontsize=7)\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.3, label=\"chance\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "# Summary: mean opposite vs mean equivalent\n",
    "bar_width = 0.3\n",
    "x_summary = np.arange(len(encoder_names))\n",
    "\n",
    "mean_opp = [np.mean([r[\"similarity\"] for r in probe_results[n][\"opposite\"]]) for n in encoder_names]\n",
    "mean_eq = [np.mean([r[\"similarity\"] for r in probe_results[n][\"equivalent\"]]) for n in encoder_names]\n",
    "\n",
    "bars1 = axes[1].bar(x_summary - bar_width/2, mean_opp, bar_width, label=\"Opposite pairs\", color=\"#e74c3c\", alpha=0.85)\n",
    "bars2 = axes[1].bar(x_summary + bar_width/2, mean_eq, bar_width, label=\"Equivalent pairs\", color=\"#2ecc71\", alpha=0.85)\n",
    "\n",
    "axes[1].set_ylabel(\"Mean Cosine Similarity\")\n",
    "axes[1].set_title(\"Discrimination Summary\\n(Bigger gap between bars = better)\")\n",
    "axes[1].set_xticks(x_summary)\n",
    "axes[1].set_xticklabels(encoder_names, fontsize=9)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "# Add gap annotations\n",
    "for i, (opp, eq) in enumerate(zip(mean_opp, mean_eq)):\n",
    "    gap = eq - opp\n",
    "    mid = (opp + eq) / 2\n",
    "    axes[1].annotate(f\"gap={gap:.3f}\", xy=(i, mid), fontsize=8, ha=\"center\",\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load the dataset and create identical train/val/test splits. The splits are by persona (same seed = same split for all encoders) to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vif.dataset import load_all_data, split_by_persona\n",
    "from src.models.judge import SCHWARTZ_VALUE_ORDER\n",
    "\n",
    "labels_df, entries_df = load_all_data()\n",
    "train_df, val_df, test_df = split_by_persona(labels_df, entries_df, seed=SEED)\n",
    "\n",
    "print(f\"Labels: {labels_df.shape[0]} entries, {labels_df.select('persona_id').n_unique()} personas\")\n",
    "print(f\"Entries: {entries_df.shape[0]} entries\")\n",
    "print()\n",
    "print(f\"Train: {len(train_df)} entries ({train_df.select('persona_id').n_unique()} personas)\")\n",
    "print(f\"Val:   {len(val_df)} entries ({val_df.select('persona_id').n_unique()} personas)\")\n",
    "print(f\"Test:  {len(test_df)} entries ({test_df.select('persona_id').n_unique()} personas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ablation-header",
   "metadata": {},
   "source": [
    "## 3. End-to-End Encoder Ablation\n",
    "\n",
    "For each encoder:\n",
    "1. Create a new `SBERTEncoder` → `StateEncoder` (embedding dim varies, so state dim varies)\n",
    "2. Create new `VIFDataset` instances (re-caches embeddings for this encoder)\n",
    "3. Train a fresh `CriticMLP` with identical hyperparameters\n",
    "4. Evaluate on the test set with MC Dropout uncertainty\n",
    "\n",
    "**Controlled variables:** Data splits, hidden_dim, dropout, learning rate, scheduler, early stopping, batch size, random seed.  \n",
    "**Independent variable:** Encoder model (and therefore embedding dimension and state dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ablation-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vif.encoders import SBERTEncoder\n",
    "from src.vif.state_encoder import StateEncoder\n",
    "from src.vif.critic import CriticMLP\n",
    "from src.vif.dataset import VIFDataset\n",
    "from src.vif.eval import evaluate_with_uncertainty, format_results_table\n",
    "\n",
    "# Shared hyperparameters\n",
    "WINDOW_SIZE = 3\n",
    "EMA_ALPHA = 0.3\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.2\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 100\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.01\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "N_MC_SAMPLES = 50\n",
    "\n",
    "print(\"Ablation hyperparameters:\")\n",
    "print(f\"  Window size:   {WINDOW_SIZE}\")\n",
    "print(f\"  EMA alpha:     {EMA_ALPHA}\")\n",
    "print(f\"  Hidden dim:    {HIDDEN_DIM}\")\n",
    "print(f\"  Dropout:       {DROPOUT}\")\n",
    "print(f\"  Batch size:    {BATCH_SIZE}\")\n",
    "print(f\"  Max epochs:    {N_EPOCHS}\")\n",
    "print(f\"  Learning rate: {LR}\")\n",
    "print(f\"  Weight decay:  {WEIGHT_DECAY}\")\n",
    "print(f\"  Early stop:    {EARLY_STOP_PATIENCE} epochs\")\n",
    "print(f\"  MC samples:    {N_MC_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_encoder(encoder_name, encoder_config, train_df, val_df, test_df):\n",
    "    \"\"\"Train a CriticMLP with a specific encoder and evaluate.\n",
    "\n",
    "    Returns dict with training history, test results, and metadata.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"ENCODER: {encoder_name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Reset seeds for each encoder\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # Create encoder and state encoder\n",
    "    model_name = encoder_config[\"model_name\"]\n",
    "    print(f\"Loading encoder: {model_name}\")\n",
    "    text_encoder = SBERTEncoder(model_name)\n",
    "    state_encoder = StateEncoder(\n",
    "        text_encoder, window_size=WINDOW_SIZE, ema_alpha=EMA_ALPHA\n",
    "    )\n",
    "    emb_dim = text_encoder.embedding_dim\n",
    "    state_dim = state_encoder.state_dim\n",
    "    print(f\"  Embedding dim: {emb_dim}\")\n",
    "    print(f\"  State dim:     {state_dim}\")\n",
    "\n",
    "    # Create datasets (caches embeddings)\n",
    "    print(\"Creating datasets (caching embeddings)...\")\n",
    "    t0 = time.time()\n",
    "    train_dataset = VIFDataset(train_df, state_encoder, cache_embeddings=True)\n",
    "    val_dataset = VIFDataset(val_df, state_encoder, cache_embeddings=True)\n",
    "    test_dataset = VIFDataset(test_df, state_encoder, cache_embeddings=True)\n",
    "    cache_time = time.time() - t0\n",
    "    print(f\"  Embedding cache time: {cache_time:.1f}s\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Create model\n",
    "    model = CriticMLP(\n",
    "        input_dim=state_dim,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        dropout=DROPOUT,\n",
    "    )\n",
    "    model.to(device)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Model params:  {n_params:,}\")\n",
    "\n",
    "    # Training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=10, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"lr\": []}\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    print(\"\\nTraining...\")\n",
    "    t0 = time.time()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                pred = model(batch_x)\n",
    "                val_loss += criterion(pred, batch_y).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"lr\"].append(current_lr)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss - 0.001:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = {\n",
    "                k: v.cpu().clone() for k, v in model.state_dict().items()\n",
    "            }\n",
    "            patience_counter = 0\n",
    "            print(\n",
    "                f\"  Epoch {epoch + 1:3d}: train={train_loss:.4f}, \"\n",
    "                f\"val={val_loss:.4f}, lr={current_lr:.6f} [BEST]\"\n",
    "            )\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if epoch % 10 == 0:\n",
    "                print(\n",
    "                    f\"  Epoch {epoch + 1:3d}: train={train_loss:.4f}, \"\n",
    "                    f\"val={val_loss:.4f}, lr={current_lr:.6f}\"\n",
    "                )\n",
    "\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"  Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    train_time = time.time() - t0\n",
    "    print(f\"  Training time: {train_time:.1f}s\")\n",
    "\n",
    "    # Load best model and evaluate\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"\\nEvaluating with MC Dropout...\")\n",
    "    test_results = evaluate_with_uncertainty(\n",
    "        model, test_loader, n_mc_samples=N_MC_SAMPLES, device=device\n",
    "    )\n",
    "    print(format_results_table(test_results))\n",
    "\n",
    "    return {\n",
    "        \"encoder_name\": encoder_name,\n",
    "        \"model_name\": model_name,\n",
    "        \"embedding_dim\": emb_dim,\n",
    "        \"state_dim\": state_dim,\n",
    "        \"n_params\": n_params,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"history\": history,\n",
    "        \"test_results\": test_results,\n",
    "        \"cache_time\": cache_time,\n",
    "        \"train_time\": train_time,\n",
    "        \"epochs_trained\": epoch + 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-ablation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ablation for each encoder\n",
    "ablation_results = {}\n",
    "\n",
    "for name, cfg in ENCODER_CONFIGS.items():\n",
    "    result = train_with_encoder(name, cfg, train_df, val_df, test_df)\n",
    "    ablation_results[name] = result\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL ENCODER ABLATIONS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 4. Side-by-Side Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison table\n",
    "encoder_names = list(ablation_results.keys())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ENCODER ABLATION: SIDE-BY-SIDE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "header = f\"{'Metric':<30}\"\n",
    "for name in encoder_names:\n",
    "    header += f\"{name:>22}\"\n",
    "header += f\"{'Winner':>12}\"\n",
    "print(header)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "\n",
    "def find_winner(values_and_names, higher_is_better=True):\n",
    "    valid = [(v, n) for v, n in values_and_names if v is not None]\n",
    "    if not valid:\n",
    "        return \"N/A\"\n",
    "    if higher_is_better:\n",
    "        return max(valid, key=lambda x: x[0])[1]\n",
    "    return min(valid, key=lambda x: x[0])[1]\n",
    "\n",
    "\n",
    "rows = [\n",
    "    (\"Embedding Dim\", [str(ablation_results[n][\"embedding_dim\"]) for n in encoder_names], None),\n",
    "    (\"State Dim\", [str(ablation_results[n][\"state_dim\"]) for n in encoder_names], None),\n",
    "    (\"Model Params\", [f\"{ablation_results[n]['n_params']:,}\" for n in encoder_names], None),\n",
    "    (\"Best Val Loss\",\n",
    "     [f\"{ablation_results[n]['best_val_loss']:.4f}\" for n in encoder_names],\n",
    "     find_winner([(ablation_results[n][\"best_val_loss\"], n) for n in encoder_names], higher_is_better=False)),\n",
    "    (\"Test MSE\",\n",
    "     [f\"{ablation_results[n]['test_results']['mse_mean']:.4f}\" for n in encoder_names],\n",
    "     find_winner([(ablation_results[n][\"test_results\"][\"mse_mean\"], n) for n in encoder_names], higher_is_better=False)),\n",
    "    (\"Test Spearman\",\n",
    "     [f\"{ablation_results[n]['test_results']['spearman_mean']:.4f}\" for n in encoder_names],\n",
    "     find_winner([(ablation_results[n][\"test_results\"][\"spearman_mean\"], n) for n in encoder_names], higher_is_better=True)),\n",
    "    (\"Test Accuracy\",\n",
    "     [f\"{ablation_results[n]['test_results']['accuracy_mean']:.2%}\" for n in encoder_names],\n",
    "     find_winner([(ablation_results[n][\"test_results\"][\"accuracy_mean\"], n) for n in encoder_names], higher_is_better=True)),\n",
    "    (\"Error-Uncertainty Corr\",\n",
    "     [f\"{ablation_results[n]['test_results']['calibration']['error_uncertainty_correlation']:.3f}\" for n in encoder_names],\n",
    "     find_winner([(ablation_results[n][\"test_results\"][\"calibration\"][\"error_uncertainty_correlation\"], n) for n in encoder_names], higher_is_better=True)),\n",
    "    (\"Embedding Cache Time (s)\",\n",
    "     [f\"{ablation_results[n]['cache_time']:.1f}\" for n in encoder_names],\n",
    "     find_winner([(ablation_results[n][\"cache_time\"], n) for n in encoder_names], higher_is_better=False)),\n",
    "    (\"Training Time (s)\",\n",
    "     [f\"{ablation_results[n]['train_time']:.1f}\" for n in encoder_names],\n",
    "     find_winner([(ablation_results[n][\"train_time\"], n) for n in encoder_names], higher_is_better=False)),\n",
    "    (\"Epochs Trained\",\n",
    "     [str(ablation_results[n][\"epochs_trained\"]) for n in encoder_names],\n",
    "     None),\n",
    "]\n",
    "\n",
    "for metric, values, winner in rows:\n",
    "    line = f\"{metric:<30}\"\n",
    "    for v in values:\n",
    "        line += f\"{v:>22}\"\n",
    "    if winner:\n",
    "        line += f\"{winner:>12}\"\n",
    "    print(line)\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves comparison\n",
    "fig, axes = plt.subplots(1, len(encoder_names), figsize=(6 * len(encoder_names), 4))\n",
    "\n",
    "if len(encoder_names) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, name in zip(axes, encoder_names):\n",
    "    h = ablation_results[name][\"history\"]\n",
    "    ax.plot(h[\"train_loss\"], label=\"Train\", alpha=0.7)\n",
    "    ax.plot(h[\"val_loss\"], label=\"Val\", alpha=0.7)\n",
    "    ax.set_title(f\"{name}\\n(val={ablation_results[name]['best_val_loss']:.4f})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"MSE Loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Training Curves by Encoder\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "per-dim-header",
   "metadata": {},
   "source": [
    "## 5. Per-Dimension Deep Dive\n",
    "\n",
    "Compare how each encoder performs on individual Schwartz value dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-dim-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-dimension comparison tables\n",
    "for metric_key, metric_label, higher_better in [\n",
    "    (\"mse_per_dim\", \"MSE (lower is better)\", False),\n",
    "    (\"spearman_per_dim\", \"Spearman Correlation (higher is better)\", True),\n",
    "    (\"accuracy_per_dim\", \"Accuracy (higher is better)\", True),\n",
    "]:\n",
    "    print(f\"\\n{'=' * 90}\")\n",
    "    print(f\"{metric_label}\")\n",
    "    print(f\"{'=' * 90}\")\n",
    "\n",
    "    header = f\"{'Dimension':<20}\"\n",
    "    for name in encoder_names:\n",
    "        header += f\"{name:>22}\"\n",
    "    header += f\"{'Winner':>12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for dim_name in SCHWARTZ_VALUE_ORDER:\n",
    "        line = f\"{dim_name:<20}\"\n",
    "        vals = []\n",
    "        for name in encoder_names:\n",
    "            v = ablation_results[name][\"test_results\"][metric_key][dim_name]\n",
    "            vals.append((v, name))\n",
    "            if metric_key == \"accuracy_per_dim\":\n",
    "                line += f\"{v:>21.2%} \"\n",
    "            else:\n",
    "                v_str = f\"{v:.4f}\" if not np.isnan(v) else \"N/A\"\n",
    "                line += f\"{v_str:>22}\"\n",
    "\n",
    "        valid_vals = [(v, n) for v, n in vals if not np.isnan(v)]\n",
    "        if valid_vals:\n",
    "            winner = find_winner(valid_vals, higher_is_better=higher_better)\n",
    "            line += f\"{winner:>12}\"\n",
    "        print(line)\n",
    "\n",
    "    # Mean row\n",
    "    mean_key = metric_key.replace(\"_per_dim\", \"_mean\")\n",
    "    line = f\"{'MEAN':<20}\"\n",
    "    vals = []\n",
    "    for name in encoder_names:\n",
    "        v = ablation_results[name][\"test_results\"][mean_key]\n",
    "        vals.append((v, name))\n",
    "        if metric_key == \"accuracy_per_dim\":\n",
    "            line += f\"{v:>21.2%} \"\n",
    "        else:\n",
    "            line += f\"{v:>22.4f}\"\n",
    "    winner = find_winner(vals, higher_is_better=higher_better)\n",
    "    line += f\"{winner:>12}\"\n",
    "    print(line)\n",
    "    print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-dim-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-dimension visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "dim_names = SCHWARTZ_VALUE_ORDER\n",
    "x = np.arange(len(dim_names))\n",
    "width = 0.25\n",
    "colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\"]\n",
    "\n",
    "# MSE per dimension\n",
    "for i, name in enumerate(encoder_names):\n",
    "    vals = [ablation_results[name][\"test_results\"][\"mse_per_dim\"][d] for d in dim_names]\n",
    "    axes[0].barh(x + i * width, vals, width, label=name, color=colors[i], alpha=0.85)\n",
    "axes[0].set_xlabel(\"MSE\")\n",
    "axes[0].set_title(\"MSE per Dimension (lower = better)\")\n",
    "axes[0].set_yticks(x + width)\n",
    "axes[0].set_yticklabels(dim_names, fontsize=8)\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.2, axis=\"x\")\n",
    "\n",
    "# Spearman per dimension\n",
    "for i, name in enumerate(encoder_names):\n",
    "    vals = [ablation_results[name][\"test_results\"][\"spearman_per_dim\"][d] for d in dim_names]\n",
    "    axes[1].barh(x + i * width, vals, width, label=name, color=colors[i], alpha=0.85)\n",
    "axes[1].set_xlabel(\"Spearman Correlation\")\n",
    "axes[1].set_title(\"Spearman per Dimension (higher = better)\")\n",
    "axes[1].set_yticks(x + width)\n",
    "axes[1].set_yticklabels(dim_names, fontsize=8)\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.2, axis=\"x\")\n",
    "\n",
    "# Accuracy per dimension\n",
    "for i, name in enumerate(encoder_names):\n",
    "    vals = [ablation_results[name][\"test_results\"][\"accuracy_per_dim\"][d] for d in dim_names]\n",
    "    axes[2].barh(x + i * width, vals, width, label=name, color=colors[i], alpha=0.85)\n",
    "axes[2].set_xlabel(\"Accuracy\")\n",
    "axes[2].set_title(\"Accuracy per Dimension (higher = better)\")\n",
    "axes[2].set_yticks(x + width)\n",
    "axes[2].set_yticklabels(dim_names, fontsize=8)\n",
    "axes[2].legend(fontsize=8)\n",
    "axes[2].grid(True, alpha=0.2, axis=\"x\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calibration-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration comparison: uncertainty vs error for each encoder\n",
    "fig, axes = plt.subplots(1, len(encoder_names), figsize=(6 * len(encoder_names), 5))\n",
    "\n",
    "if len(encoder_names) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, name in zip(axes, encoder_names):\n",
    "    res = ablation_results[name][\"test_results\"]\n",
    "    errors = np.abs(res[\"predictions\"] - res[\"targets\"]).flatten()\n",
    "    uncerts = res[\"uncertainties\"].flatten()\n",
    "\n",
    "    ax.scatter(uncerts, errors, alpha=0.3, s=10)\n",
    "    ax.set_xlabel(\"Predicted Uncertainty (std)\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "\n",
    "    corr = res[\"calibration\"][\"error_uncertainty_correlation\"]\n",
    "    ax.set_title(f\"{name}\\ncorr={corr:.3f}\")\n",
    "\n",
    "    # Trend line\n",
    "    z = np.polyfit(uncerts, errors, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(uncerts.min(), uncerts.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), \"r-\", linewidth=2, label=\"trend\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.suptitle(\"Uncertainty Calibration by Encoder\\n(Positive slope = well calibrated)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-dim-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-dimension calibration for each encoder\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"PER-DIMENSION CALIBRATION (Error-Uncertainty Spearman Correlation)\")\n",
    "print(\"(Positive = uncertainty rises with error = well calibrated)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "header = f\"{'Dimension':<20}\"\n",
    "for name in encoder_names:\n",
    "    header += f\"{name:>22}\"\n",
    "print(header)\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for dim_idx, dim_name in enumerate(SCHWARTZ_VALUE_ORDER):\n",
    "    line = f\"{dim_name:<20}\"\n",
    "    for name in encoder_names:\n",
    "        res = ablation_results[name][\"test_results\"]\n",
    "        dim_errors = np.abs(res[\"predictions\"][:, dim_idx] - res[\"targets\"][:, dim_idx])\n",
    "        dim_uncert = res[\"uncertainties\"][:, dim_idx]\n",
    "\n",
    "        if np.std(dim_uncert) < 1e-8 or np.std(dim_errors) < 1e-8:\n",
    "            line += f\"{'N/A':>22}\"\n",
    "        else:\n",
    "            corr, _ = stats.spearmanr(dim_uncert, dim_errors)\n",
    "            line += f\"{corr:>22.3f}\"\n",
    "    print(line)\n",
    "\n",
    "print(\"-\" * 90)\n",
    "# Overall\n",
    "line = f\"{'OVERALL':<20}\"\n",
    "for name in encoder_names:\n",
    "    corr = ablation_results[name][\"test_results\"][\"calibration\"][\"error_uncertainty_correlation\"]\n",
    "    line += f\"{corr:>22.3f}\"\n",
    "print(line)\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matryoshka-header",
   "metadata": {},
   "source": [
    "## 6. Matryoshka Dimension Search\n",
    "\n",
    "`nomic-embed-text-v1.5` supports [Matryoshka Representation Learning](https://huggingface.co/blog/matryoshka), meaning its embeddings can be truncated to lower dimensions with graceful quality degradation.\n",
    "\n",
    "This is useful because:\n",
    "- Smaller embeddings → smaller state vectors → smaller MLP → faster training/inference\n",
    "- May reduce overfitting with the current small dataset\n",
    "- Helps find the optimal quality/size tradeoff\n",
    "\n",
    "We test dimensions: 64, 128, 256, 384, 512, 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matryoshka-ablation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vif.encoders import TextEncoder\n",
    "\n",
    "\n",
    "class TruncatedSBERTEncoder:\n",
    "    \"\"\"Wraps an SBERTEncoder to truncate embeddings to a target dimension.\n",
    "\n",
    "    For Matryoshka models, the first N dimensions are trained to be\n",
    "    independently useful, so truncation is valid without retraining.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_encoder, target_dim: int):\n",
    "        self._base = base_encoder\n",
    "        self._target_dim = target_dim\n",
    "        self._model_name = f\"{base_encoder.model_name}[:{target_dim}]\"\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self) -> int:\n",
    "        return self._target_dim\n",
    "\n",
    "    @property\n",
    "    def model_name(self) -> str:\n",
    "        return self._model_name\n",
    "\n",
    "    def encode(self, texts: list[str]) -> np.ndarray:\n",
    "        full = self._base.encode(texts)\n",
    "        return full[:, :self._target_dim]\n",
    "\n",
    "    def encode_batch(self, texts: list[str], batch_size: int = 32) -> np.ndarray:\n",
    "        full = self._base.encode_batch(texts, batch_size=batch_size)\n",
    "        return full[:, :self._target_dim]\n",
    "\n",
    "\n",
    "# Only run Matryoshka search if nomic is in the ablation\n",
    "MATRYOSHKA_DIMS = [64, 128, 256, 384, 512, 768]\n",
    "\n",
    "# Load the nomic encoder once (reuse from probe if available)\n",
    "nomic_base = SBERTEncoder(\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "print(f\"Base nomic dim: {nomic_base.embedding_dim}\")\n",
    "\n",
    "matryoshka_results = {}\n",
    "\n",
    "for dim in MATRYOSHKA_DIMS:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"MATRYOSHKA DIM: {dim}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Reset seeds\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # Truncated encoder\n",
    "    trunc_encoder = TruncatedSBERTEncoder(nomic_base, dim)\n",
    "    state_enc = StateEncoder(trunc_encoder, window_size=WINDOW_SIZE, ema_alpha=EMA_ALPHA)\n",
    "    state_dim = state_enc.state_dim\n",
    "    print(f\"  State dim: {state_dim}\")\n",
    "\n",
    "    # Create datasets\n",
    "    print(\"  Caching embeddings...\")\n",
    "    tr_ds = VIFDataset(train_df, state_enc, cache_embeddings=True)\n",
    "    va_ds = VIFDataset(val_df, state_enc, cache_embeddings=True)\n",
    "    te_ds = VIFDataset(test_df, state_enc, cache_embeddings=True)\n",
    "\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_dl = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    te_dl = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = CriticMLP(input_dim=state_dim, hidden_dim=HIDDEN_DIM, dropout=DROPOUT)\n",
    "    model.to(device)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Model params: {n_params:,}\")\n",
    "\n",
    "    # Train\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=10, min_lr=1e-5)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "\n",
    "    print(\"  Training...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        t_loss = 0.0\n",
    "        for bx, by in tr_dl:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(bx), by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t_loss += loss.item()\n",
    "        t_loss /= len(tr_dl)\n",
    "\n",
    "        model.eval()\n",
    "        v_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for bx, by in va_dl:\n",
    "                bx, by = bx.to(device), by.to(device)\n",
    "                v_loss += criterion(model(bx), by).item()\n",
    "        v_loss /= len(va_dl)\n",
    "\n",
    "        scheduler.step(v_loss)\n",
    "\n",
    "        if v_loss < best_val_loss - 0.001:\n",
    "            best_val_loss = v_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if patience >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"  Early stop at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Evaluate\n",
    "    model.load_state_dict(best_state)\n",
    "    model.to(device)\n",
    "    test_res = evaluate_with_uncertainty(model, te_dl, n_mc_samples=N_MC_SAMPLES, device=device)\n",
    "\n",
    "    matryoshka_results[dim] = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"n_params\": n_params,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"test_results\": test_res,\n",
    "        \"epochs\": epoch + 1,\n",
    "    }\n",
    "\n",
    "    print(f\"  Val loss: {best_val_loss:.4f} | Test MSE: {test_res['mse_mean']:.4f} | \"\n",
    "          f\"Spearman: {test_res['spearman_mean']:.4f} | Acc: {test_res['accuracy_mean']:.2%}\")\n",
    "\n",
    "print(\"\\nMatryoshka search complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matryoshka-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matryoshka results table\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"MATRYOSHKA DIMENSION SEARCH (nomic-embed-text-v1.5)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Emb Dim':>8} {'State Dim':>10} {'Params':>12} {'Val Loss':>10} \"\n",
    "      f\"{'Test MSE':>10} {'Spearman':>10} {'Accuracy':>10} {'Epochs':>8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for dim in MATRYOSHKA_DIMS:\n",
    "    r = matryoshka_results[dim]\n",
    "    tr = r[\"test_results\"]\n",
    "    print(f\"{dim:>8} {r['state_dim']:>10} {r['n_params']:>12,} {r['best_val_loss']:>10.4f} \"\n",
    "          f\"{tr['mse_mean']:>10.4f} {tr['spearman_mean']:>10.4f} {tr['accuracy_mean']:>10.2%} \"\n",
    "          f\"{r['epochs']:>8}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matryoshka-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matryoshka dimension vs. performance visualization\n",
    "dims = MATRYOSHKA_DIMS\n",
    "mse_vals = [matryoshka_results[d][\"test_results\"][\"mse_mean\"] for d in dims]\n",
    "spearman_vals = [matryoshka_results[d][\"test_results\"][\"spearman_mean\"] for d in dims]\n",
    "acc_vals = [matryoshka_results[d][\"test_results\"][\"accuracy_mean\"] for d in dims]\n",
    "param_vals = [matryoshka_results[d][\"n_params\"] for d in dims]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# MSE vs dim\n",
    "axes[0, 0].plot(dims, mse_vals, \"o-\", color=\"#e74c3c\", linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel(\"Embedding Dimension\")\n",
    "axes[0, 0].set_ylabel(\"Test MSE\")\n",
    "axes[0, 0].set_title(\"MSE vs Embedding Dimension\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "# Add baseline reference\n",
    "if \"MiniLM-L6 (baseline)\" in ablation_results:\n",
    "    baseline_mse = ablation_results[\"MiniLM-L6 (baseline)\"][\"test_results\"][\"mse_mean\"]\n",
    "    axes[0, 0].axhline(y=baseline_mse, color=\"blue\", linestyle=\"--\", alpha=0.5,\n",
    "                        label=f\"MiniLM baseline ({baseline_mse:.4f})\")\n",
    "    axes[0, 0].legend(fontsize=8)\n",
    "\n",
    "# Spearman vs dim\n",
    "axes[0, 1].plot(dims, spearman_vals, \"o-\", color=\"#2ecc71\", linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel(\"Embedding Dimension\")\n",
    "axes[0, 1].set_ylabel(\"Test Spearman\")\n",
    "axes[0, 1].set_title(\"Spearman vs Embedding Dimension\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "if \"MiniLM-L6 (baseline)\" in ablation_results:\n",
    "    baseline_sp = ablation_results[\"MiniLM-L6 (baseline)\"][\"test_results\"][\"spearman_mean\"]\n",
    "    axes[0, 1].axhline(y=baseline_sp, color=\"blue\", linestyle=\"--\", alpha=0.5,\n",
    "                        label=f\"MiniLM baseline ({baseline_sp:.4f})\")\n",
    "    axes[0, 1].legend(fontsize=8)\n",
    "\n",
    "# Accuracy vs dim\n",
    "axes[1, 0].plot(dims, acc_vals, \"o-\", color=\"#f39c12\", linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel(\"Embedding Dimension\")\n",
    "axes[1, 0].set_ylabel(\"Test Accuracy\")\n",
    "axes[1, 0].set_title(\"Accuracy vs Embedding Dimension\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "if \"MiniLM-L6 (baseline)\" in ablation_results:\n",
    "    baseline_acc = ablation_results[\"MiniLM-L6 (baseline)\"][\"test_results\"][\"accuracy_mean\"]\n",
    "    axes[1, 0].axhline(y=baseline_acc, color=\"blue\", linestyle=\"--\", alpha=0.5,\n",
    "                        label=f\"MiniLM baseline ({baseline_acc:.2%})\")\n",
    "    axes[1, 0].legend(fontsize=8)\n",
    "\n",
    "# Params vs dim\n",
    "axes[1, 1].plot(dims, [p / 1000 for p in param_vals], \"o-\", color=\"#9b59b6\", linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel(\"Embedding Dimension\")\n",
    "axes[1, 1].set_ylabel(\"Model Parameters (K)\")\n",
    "axes[1, 1].set_title(\"Model Size vs Embedding Dimension\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Matryoshka Dimension Search (nomic-embed-text-v1.5)\", y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latency-header",
   "metadata": {},
   "source": [
    "## 7. Latency Benchmark\n",
    "\n",
    "Measure encoding speed for each model. In the VIF pipeline, encoding happens:\n",
    "- Once per journal entry (during data caching or real-time inference)\n",
    "- 50x during MC Dropout (but this is on the MLP, not the encoder)\n",
    "\n",
    "So encoder latency mainly matters for real-time inference responsiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latency-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark encoding latency\n",
    "# Use realistic journal entry lengths\n",
    "SAMPLE_TEXTS = [\n",
    "    # Short entry (~50 tokens)\n",
    "    \"Today I helped my colleague with their presentation. It felt good to contribute.\",\n",
    "    # Medium entry (~100 tokens)\n",
    "    (\"I spent the morning reflecting on my career goals. I realized that I've been prioritizing \"\n",
    "     \"achievement over my relationships. My partner mentioned feeling neglected, and it struck \"\n",
    "     \"me that maybe success isn't worth it if I'm alone at the top. I need to find a better balance.\"),\n",
    "    # Long entry with nudge (~250 tokens)\n",
    "    (\"Today was a difficult day at work. The team disagreed about the project direction, and I \"\n",
    "     \"found myself torn between following the consensus and speaking up about what I believed was \"\n",
    "     \"right. In the end, I chose to voice my concerns, even though it made some colleagues \"\n",
    "     \"uncomfortable. I value harmony, but I also can't ignore my principles.\\n\\n\"\n",
    "     'Nudge: \"You mentioned feeling torn between harmony and principles. What would it look '\n",
    "     'like to honor both?\"\\n\\n'\n",
    "     \"Response: I think I could have been more diplomatic in how I raised my concerns. Instead of \"\n",
    "     \"directly challenging the team lead's proposal, I could have framed it as building on their \"\n",
    "     \"idea. That way I stay true to my values while respecting the group dynamic.\"),\n",
    "]\n",
    "\n",
    "N_WARMUP = 5\n",
    "N_RUNS = 50\n",
    "\n",
    "latency_results = {}\n",
    "\n",
    "for name, cfg in ENCODER_CONFIGS.items():\n",
    "    model = encoder_models[name]\n",
    "    prefix = cfg[\"prefix\"]\n",
    "\n",
    "    # Prepare texts with prefix\n",
    "    texts = [prefix + t for t in SAMPLE_TEXTS]\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(N_WARMUP):\n",
    "        model.encode(texts)\n",
    "\n",
    "    # Benchmark single encoding\n",
    "    single_times = []\n",
    "    for _ in range(N_RUNS):\n",
    "        t0 = time.perf_counter()\n",
    "        model.encode([texts[1]])  # Medium entry\n",
    "        single_times.append(time.perf_counter() - t0)\n",
    "\n",
    "    # Benchmark batch encoding (3 entries = one window)\n",
    "    batch_times = []\n",
    "    for _ in range(N_RUNS):\n",
    "        t0 = time.perf_counter()\n",
    "        model.encode(texts)\n",
    "        batch_times.append(time.perf_counter() - t0)\n",
    "\n",
    "    latency_results[name] = {\n",
    "        \"single_mean_ms\": np.mean(single_times) * 1000,\n",
    "        \"single_std_ms\": np.std(single_times) * 1000,\n",
    "        \"batch_mean_ms\": np.mean(batch_times) * 1000,\n",
    "        \"batch_std_ms\": np.std(batch_times) * 1000,\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 80)\n",
    "print(\"ENCODING LATENCY BENCHMARK\")\n",
    "print(f\"(Device: CPU, {N_RUNS} runs after {N_WARMUP} warmup)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Encoder':<25} {'Single (ms)':>15} {'Batch/3 (ms)':>15} {'Speedup':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "baseline_single = latency_results[list(ENCODER_CONFIGS.keys())[0]][\"single_mean_ms\"]\n",
    "for name in ENCODER_CONFIGS:\n",
    "    r = latency_results[name]\n",
    "    speedup = baseline_single / r[\"single_mean_ms\"]\n",
    "    print(f\"{name:<25} {r['single_mean_ms']:>10.1f} ± {r['single_std_ms']:.1f} \"\n",
    "          f\"{r['batch_mean_ms']:>10.1f} ± {r['batch_std_ms']:.1f} \"\n",
    "          f\"{speedup:>9.2f}x\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 8. Conclusions & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EMBEDDING ABLATION STUDY — SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Probe results\n",
    "print(\"\\n1. EMBEDDING QUALITY PROBE\")\n",
    "print(\"-\" * 40)\n",
    "for name in encoder_names:\n",
    "    mean_opp = np.mean([r[\"similarity\"] for r in probe_results[name][\"opposite\"]])\n",
    "    mean_eq = np.mean([r[\"similarity\"] for r in probe_results[name][\"equivalent\"]])\n",
    "    gap = mean_eq - mean_opp\n",
    "    print(f\"  {name:<25} opp={mean_opp:.3f}  eq={mean_eq:.3f}  gap={gap:.3f}\")\n",
    "\n",
    "# End-to-end results\n",
    "print(\"\\n2. END-TO-END CRITIC PERFORMANCE\")\n",
    "print(\"-\" * 40)\n",
    "for name in encoder_names:\n",
    "    r = ablation_results[name]\n",
    "    tr = r[\"test_results\"]\n",
    "    print(f\"  {name:<25} MSE={tr['mse_mean']:.4f}  Spearman={tr['spearman_mean']:.4f}  \"\n",
    "          f\"Acc={tr['accuracy_mean']:.2%}  Calib={tr['calibration']['error_uncertainty_correlation']:.3f}\")\n",
    "\n",
    "# Matryoshka best\n",
    "print(\"\\n3. MATRYOSHKA DIMENSION SEARCH (nomic-embed-text-v1.5)\")\n",
    "print(\"-\" * 40)\n",
    "best_dim = min(matryoshka_results.keys(),\n",
    "               key=lambda d: matryoshka_results[d][\"test_results\"][\"mse_mean\"])\n",
    "best_r = matryoshka_results[best_dim]\n",
    "print(f\"  Best dimension: {best_dim}\")\n",
    "print(f\"  MSE: {best_r['test_results']['mse_mean']:.4f}\")\n",
    "print(f\"  Spearman: {best_r['test_results']['spearman_mean']:.4f}\")\n",
    "print(f\"  Params: {best_r['n_params']:,}\")\n",
    "\n",
    "# Latency\n",
    "print(\"\\n4. LATENCY\")\n",
    "print(\"-\" * 40)\n",
    "for name in encoder_names:\n",
    "    r = latency_results[name]\n",
    "    print(f\"  {name:<25} {r['single_mean_ms']:.1f}ms/entry\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Interpret the results above to determine:\n",
    "\n",
    "1. Does the embedding quality probe predict end-to-end performance?\n",
    "   (i.e., does better emotional discrimination → better VIF scores?)\n",
    "\n",
    "2. Is the improvement from a larger encoder significant enough to\n",
    "   justify the latency and parameter cost increase?\n",
    "\n",
    "3. What Matryoshka dimension gives the best quality/size tradeoff?\n",
    "\n",
    "4. Is the performance gap large enough to warrant further investment\n",
    "   (e.g., domain fine-tuning or emotion-aware embeddings)?\n",
    "\"\"\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
