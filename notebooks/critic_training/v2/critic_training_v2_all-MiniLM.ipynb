{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fbf03c",
   "metadata": {},
   "source": "# VIF Critic Training v2 — Diagnostic-First Notebook\n\nThis notebook is a **diagnostic-first** training notebook for the VIF Critic model,\nextended with **multi-model comparison** across ordinal loss functions.\nEvery section maps to specific GitHub issues and prints results explicitly.\n\n**Issues Surfaced:**\n- **#9** — Uncertainty calibration quality\n- **#10** — Class imbalance and neutral-dominated predictions\n- **#12** — Overfitting risk with high parameter-to-sample ratio\n- **#14** — Embedding quality and truncation risk\n\n**Models Compared:**\n- **CORAL** — Consistent Rank Logits (cumulative binary CE)\n- **CORN** — Conditional Ordinal Regression Network\n- **EMD** — Earth Mover Distance (squared L2 between CDFs)\n- **SoftOrdinal** — KL divergence with smoothed ordinal targets\n\n**Scope:** MLP-based multi-model comparison. Diagnostic embedding probe. Metrics + guidance scorecard."
  },
  {
   "cell_type": "markdown",
   "id": "be4a5387",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15973950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:24.158276Z",
     "iopub.status.busy": "2026-02-14T09:27:24.158132Z",
     "iopub.status.idle": "2026-02-14T09:27:24.163163Z",
     "shell.execute_reply": "2026-02-14T09:27:24.162845Z"
    }
   },
   "outputs": [],
   "source": "CONFIG = {\n    # Encoder\n    \"encoder_model\": \"all-MiniLM-L6-v2\",\n    # Models to compare\n    \"models_to_train\": [\"CORAL\", \"CORN\", \"EMD\", \"SoftOrdinal\"],\n    # Model\n    \"hidden_dim\": 256,\n    \"dropout\": 0.2,\n    # Optimizer\n    \"learning_rate\": 0.001,\n    \"weight_decay\": 0.01,\n    # Training\n    \"batch_size\": 16,\n    \"epochs\": 100,\n    # Early stopping\n    \"early_stopping_patience\": 20,\n    \"early_stopping_min_delta\": 0.001,\n    # LR scheduler\n    \"scheduler_factor\": 0.5,\n    \"scheduler_patience\": 10,\n    \"scheduler_min_lr\": 1e-5,\n    # State encoder\n    \"window_size\": 3,\n    # MC Dropout\n    \"mc_dropout_samples\": 50,\n    # Data splits\n    \"train_ratio\": 0.70,\n    \"val_ratio\": 0.15,\n    \"seed\": 2025,\n}\n\nprint(\"=\" * 50)\nprint(\"CONFIGURATION\")\nprint(\"=\" * 50)\nfor key, value in CONFIG.items():\n    print(f\"  {key:<30s} {str(value):>15s}\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "id": "0c6b52e7",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8d709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:24.164396Z",
     "iopub.status.busy": "2026-02-14T09:27:24.164328Z",
     "iopub.status.idle": "2026-02-14T09:27:25.578376Z",
     "shell.execute_reply": "2026-02-14T09:27:25.577825Z"
    }
   },
   "outputs": [],
   "source": "import os\nimport sys\nfrom pathlib import Path\n\n# Walk up to find project root (contains src/ and pyproject.toml)\n_dir = Path.cwd()\nwhile _dir != _dir.parent:\n    if (_dir / \"src\").is_dir() and (_dir / \"pyproject.toml\").is_file():\n        os.chdir(_dir)\n        break\n    _dir = _dir.parent\nsys.path.insert(0, \".\")\n\nimport numpy as np\nimport polars as pl\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom src.vif.dataset import (\n    load_all_data,\n    split_by_persona,\n    merge_labels_and_entries,\n    VIFDataset,\n)\nfrom src.vif.encoders import SBERTEncoder\nfrom src.vif.state_encoder import StateEncoder\nfrom src.vif.critic_ordinal import (\n    CriticMLPCORAL,\n    coral_loss_multi,\n    CriticMLPCORN,\n    corn_loss_multi,\n    CriticMLPEMD,\n    emd_loss_multi,\n    CriticMLPSoftOrdinal,\n    soft_ordinal_loss_multi,\n)\nfrom src.vif.eval import (\n    evaluate_with_uncertainty,\n    format_results_table,\n    discretize_predictions,\n)\nfrom src.models.judge import SCHWARTZ_VALUE_ORDER\n\n# Reproducibility\nnp.random.seed(CONFIG[\"seed\"])\ntorch.manual_seed(CONFIG[\"seed\"])\n\n# Device selection\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"Device: {device}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Schwartz dimensions: {len(SCHWARTZ_VALUE_ORDER)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "f23ed05b",
   "metadata": {},
   "source": [
    "# Section 1: Data Quality Audit (Issue #10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6146dad",
   "metadata": {},
   "source": [
    "**Issue #10 — Class Imbalance:**: Most Schwartz dimensions are neutral-dominated.\n",
    "The model may learn to predict `0` for everything and still achieve high accuracy.\n",
    "This section quantifies the imbalance and flags dimensions that may be unlearnable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ceb44",
   "metadata": {},
   "source": [
    "* `labels_df` - judge-assigned alignment scores\n",
    "* `entries_df` - journal text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7fca9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:25.579798Z",
     "iopub.status.busy": "2026-02-14T09:27:25.579640Z",
     "iopub.status.idle": "2026-02-14T09:27:25.605155Z",
     "shell.execute_reply": "2026-02-14T09:27:25.603986Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_df, entries_df = load_all_data()\n",
    "merged_df = merge_labels_and_entries(labels_df, entries_df)\n",
    "\n",
    "print(f\"Labels shape:  {labels_df.shape}\")\n",
    "print(f\"Entries shape: {entries_df.shape}\")\n",
    "print(f\"Merged shape:  {merged_df.shape}\")\n",
    "print(f\"Unique personas: {merged_df.select('persona_id').n_unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52926c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.head().glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff36c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_df.head().glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_label_rows = labels_df.height - merged_df.height\n",
    "if dropped_label_rows > 0:\n",
    "    print(f\"Dropped label rows after merge: {dropped_label_rows}\")\n",
    "    key_options = [\n",
    "        [\"persona_id\", \"t_index\"],\n",
    "        [\"entry_id\"],\n",
    "    ]\n",
    "    for keys in key_options:\n",
    "        if all(k in labels_df.columns for k in keys) and all(\n",
    "            k in merged_df.columns for k in keys\n",
    "        ):\n",
    "            labels_keys = labels_df.select(keys).unique()\n",
    "            merged_keys = merged_df.select(keys).unique()\n",
    "            dropped_keys = labels_keys.join(merged_keys, on=keys, how=\"anti\")\n",
    "            print(f\"  Missing key rows by {keys}: {dropped_keys.height}\")\n",
    "            if dropped_keys.height > 0:\n",
    "                print(dropped_keys.head(5))\n",
    "            break\n",
    "\n",
    "print(\n",
    "    f\"\\nLabel columns: {[c for c in labels_df.columns if c.startswith('alignment_')]}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nEntry text columns: {[c for c in entries_df.columns if c not in labels_df.columns]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd19b7",
   "metadata": {},
   "source": [
    "## Class Distribution per Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd1905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:25.606728Z",
     "iopub.status.busy": "2026-02-14T09:27:25.606631Z",
     "iopub.status.idle": "2026-02-14T09:27:25.688014Z",
     "shell.execute_reply": "2026-02-14T09:27:25.687623Z"
    }
   },
   "outputs": [],
   "source": [
    "n_dims = len(SCHWARTZ_VALUE_ORDER)\n",
    "distribution = np.zeros((n_dims, 3), dtype=int)  # rows=dims, cols=[-1, 0, +1]\n",
    "class_labels = [-1, 0, 1]\n",
    "\n",
    "for i, dim in enumerate(SCHWARTZ_VALUE_ORDER):\n",
    "    col = f\"alignment_{dim}\"\n",
    "    series = merged_df.select(col).to_series()\n",
    "    for j, val in enumerate(class_labels):\n",
    "        distribution[i, j] = int((series == val).sum())\n",
    "\n",
    "# Compute imbalance ratios\n",
    "imbalance_ratios = []\n",
    "for i in range(n_dims):\n",
    "    row = distribution[i]\n",
    "    ratio = float(\"inf\") if (row == 0).any() else row.max() / row.min()\n",
    "    imbalance_ratios.append(ratio)\n",
    "\n",
    "# Print table\n",
    "print(f\"{'Dimension':<20s} {'-1':>6s} {'0':>6s} {'+1':>6s} {'Ratio':>8s} {'Flag':>8s}\")\n",
    "print(\"-\" * 55)\n",
    "for i, dim in enumerate(SCHWARTZ_VALUE_ORDER):\n",
    "    flag = \">10x\" if imbalance_ratios[i] > 10 else \"\"\n",
    "    print(\n",
    "        f\"{dim:<20s} {distribution[i, 0]:>6d} {distribution[i, 1]:>6d} {distribution[i, 2]:>6d} {imbalance_ratios[i]:>8.1f} {flag:>8s}\"\n",
    "    )\n",
    "\n",
    "neutral_pct = distribution[:, 1].sum() / distribution.sum() * 100\n",
    "print(f\"\\nOverall neutral %: {neutral_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41fe31",
   "metadata": {},
   "source": [
    "Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bcfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 7))\n",
    "im = ax.imshow(distribution, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "ax.set_xticks([0, 1, 2])\n",
    "ax.set_xticklabels([\"-1\", \"0\", \"+1\"])\n",
    "ax.set_yticks(range(n_dims))\n",
    "ax.set_yticklabels(SCHWARTZ_VALUE_ORDER)\n",
    "ax.set_title(\"Class Distribution per Dimension\")\n",
    "for i in range(n_dims):\n",
    "    for j in range(3):\n",
    "        color = \"white\" if distribution[i, j] > distribution.max() * 0.6 else \"black\"\n",
    "        ax.text(\n",
    "            j,\n",
    "            i,\n",
    "            str(distribution[i, j]),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=9,\n",
    "            color=color,\n",
    "        )\n",
    "plt.colorbar(im, ax=ax, label=\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80bb339",
   "metadata": {},
   "source": [
    "Sparsity flags: Checking all dimensions where any class < 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0b88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:25.689120Z",
     "iopub.status.busy": "2026-02-14T09:27:25.689047Z",
     "iopub.status.idle": "2026-02-14T09:27:25.691357Z",
     "shell.execute_reply": "2026-02-14T09:27:25.691019Z"
    }
   },
   "outputs": [],
   "source": [
    "MIN_SAMPLES = 10\n",
    "\n",
    "print(f\"{'Dimension':<20s} {'Min class':>10s} {'Min count':>10s} {'Status':>12s}\")\n",
    "print(\"-\" * 55)\n",
    "unlearnable = []\n",
    "for i, dim in enumerate(SCHWARTZ_VALUE_ORDER):\n",
    "    min_idx = distribution[i].argmin()\n",
    "    min_count = distribution[i, min_idx]\n",
    "    min_class = class_labels[min_idx]\n",
    "    status = \"UNLEARNABLE\" if min_count < MIN_SAMPLES else \"OK\"\n",
    "    if min_count < MIN_SAMPLES:\n",
    "        unlearnable.append(dim)\n",
    "    print(f\"{dim:<20s} {min_class:>10d} {min_count:>10d} {status:>12s}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nUnlearnable dims (any class < {MIN_SAMPLES} samples): {unlearnable if unlearnable else 'None'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf3286",
   "metadata": {},
   "source": [
    "# Section 2: Embedding Quality Probe (Issue #14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7145d24",
   "metadata": {},
   "source": [
    "**Issue #14 — Embedding Quality:** Are SBERT embeddings discriminative enough for value alignment? Does truncation lose information from long entries?\n",
    "\n",
    "This section probes the encoder without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8f890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:25.692352Z",
     "iopub.status.busy": "2026-02-14T09:27:25.692276Z",
     "iopub.status.idle": "2026-02-14T09:27:30.501002Z",
     "shell.execute_reply": "2026-02-14T09:27:30.500451Z"
    }
   },
   "outputs": [],
   "source": "text_encoder = SBERTEncoder(CONFIG[\"encoder_model\"])\nstate_encoder = StateEncoder(\n    text_encoder,\n    window_size=CONFIG[\"window_size\"],\n)\n\nprint(f\"Encoder: {text_encoder.model_name}\")\nprint(f\"Embedding dim: {text_encoder.embedding_dim}\")\nprint(f\"State dim: {state_encoder.state_dim}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one random entry\n",
    "rng = np.random.default_rng(CONFIG[\"seed\"])\n",
    "sample_idx = rng.integers(0, len(merged_df))\n",
    "sample = merged_df.row(sample_idx, named=True)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"RANDOM ENTRY VERIFICATION (row {sample_idx})\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"persona_id:    {sample['persona_id']}\")\n",
    "print(f\"persona_name:  {sample.get('persona_name', 'N/A')}\")\n",
    "print(f\"core_values:   {sample.get('core_values', 'N/A')}\")\n",
    "print(f\"date:          {sample['date']}\")\n",
    "print(f\"t_index:       {sample['t_index']}\")\n",
    "\n",
    "print(f\"\\n--- initial_entry (first 200 chars) ---\")\n",
    "print(f\"{(sample['initial_entry'] or '')[:200]}\")\n",
    "print(f\"\\n--- nudge_text (first 200 chars) ---\")\n",
    "print(f\"{(sample['nudge_text'] or '')[:200]}\")\n",
    "print(f\"\\n--- response_text (first 200 chars) ---\")\n",
    "print(f\"{(sample['response_text'] or '')[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eaef17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:30.637001Z",
     "iopub.status.busy": "2026-02-14T09:27:30.636940Z",
     "iopub.status.idle": "2026-02-14T09:27:30.860782Z",
     "shell.execute_reply": "2026-02-14T09:27:30.860433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2c — Entry length / truncation risk\n",
    "tokenizer = text_encoder._model.tokenizer\n",
    "max_seq_length = text_encoder._model.max_seq_length\n",
    "\n",
    "all_texts = []\n",
    "for row in merged_df.iter_rows(named=True):\n",
    "    text = state_encoder.concatenate_entry_text(\n",
    "        row[\"initial_entry\"], row[\"nudge_text\"], row[\"response_text\"]\n",
    "    )\n",
    "    all_texts.append(text)\n",
    "\n",
    "token_counts = []\n",
    "for text in all_texts:\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    token_counts.append(len(tokens))\n",
    "\n",
    "token_counts = np.array(token_counts)\n",
    "n_truncated = (token_counts > max_seq_length).sum()\n",
    "pct_truncated = n_truncated / len(token_counts) * 100\n",
    "\n",
    "print(f\"Max context length: {max_seq_length} tokens\")\n",
    "print(f\"\\nToken count statistics:\")\n",
    "print(f\"  Min:    {token_counts.min()}\")\n",
    "print(f\"  Mean:   {token_counts.mean():.1f}\")\n",
    "print(f\"  Median: {np.median(token_counts):.1f}\")\n",
    "print(f\"  P90:    {np.percentile(token_counts, 90):.0f}\")\n",
    "print(f\"  P95:    {np.percentile(token_counts, 95):.0f}\")\n",
    "print(f\"  Max:    {token_counts.max()}\")\n",
    "print(f\"\\nTruncated entries: {n_truncated}/{len(token_counts)} ({pct_truncated:.1f}%)\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(token_counts, bins=50, color=\"steelblue\", edgecolor=\"white\", alpha=0.8)\n",
    "ax.axvline(\n",
    "    max_seq_length,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Max context ({max_seq_length})\",\n",
    ")\n",
    "ax.set_xlabel(\"Token Count\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(f\"Entry Token Lengths ({pct_truncated:.1f}% truncated)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9937cb",
   "metadata": {},
   "source": [
    "# Section 3: Model & Training (Issue #12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a1702",
   "metadata": {},
   "source": [
    "**Issue #12 — Overfitting Risk:** the model has a very high parameter-to-sample ratio. This section quantifies the risk and monitors the training/validation gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0e812",
   "metadata": {},
   "outputs": [],
   "source": "MODEL_CONFIGS = {\n    \"CORAL\": {\n        \"class\": CriticMLPCORAL,\n        \"loss_fn\": coral_loss_multi,\n        \"is_ordinal\": True,\n    },\n    \"CORN\": {\n        \"class\": CriticMLPCORN,\n        \"loss_fn\": corn_loss_multi,\n        \"is_ordinal\": True,\n    },\n    \"EMD\": {\n        \"class\": CriticMLPEMD,\n        \"loss_fn\": emd_loss_multi,\n        \"is_ordinal\": True,\n    },\n    \"SoftOrdinal\": {\n        \"class\": CriticMLPSoftOrdinal,\n        \"loss_fn\": soft_ordinal_loss_multi,\n        \"is_ordinal\": True,\n    },\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da347995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:30.862065Z",
     "iopub.status.busy": "2026-02-14T09:27:30.861997Z",
     "iopub.status.idle": "2026-02-14T09:27:30.867957Z",
     "shell.execute_reply": "2026-02-14T09:27:30.867640Z"
    }
   },
   "outputs": [],
   "source": "# Filter to active models\nactive_models = {\n    k: dict(v) for k, v in MODEL_CONFIGS.items() if k in CONFIG[\"models_to_train\"]\n}\n\nif not active_models:\n    raise ValueError(\n        f\"No valid models in models_to_train={CONFIG['models_to_train']}. \"\n        f\"Valid names: {list(MODEL_CONFIGS.keys())}\"\n    )\n\nprint(f\"{'Model':<15s} {'Parameters':>12s} {'Output logits':>15s} {'Loss':>20s}\")\nprint(\"-\" * 65)\nn_params = 0  # Track max for overfitting check (Cell 3c)\nfor name, cfg in active_models.items():\n    m = cfg[\"class\"](\n        input_dim=state_encoder.state_dim,\n        hidden_dim=CONFIG[\"hidden_dim\"],\n        dropout=CONFIG[\"dropout\"],\n    )\n    param_count = sum(param.numel() for param in m.parameters())\n    n_params = max(n_params, param_count)\n    n_out = m.fc_out.out_features\n    loss_name = (\n        cfg[\"loss_fn\"].__class__.__name__\n        if isinstance(cfg[\"loss_fn\"], nn.Module)\n        else cfg[\"loss_fn\"].__name__\n    )\n    print(f\"{name:<15s} {param_count:>12,} {n_out:>15d} {loss_name:>20s}\")\nprint(\"-\" * 65)\nprint(f\"\\nMax parameters (for overfitting check): {n_params:,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e109c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:30.868920Z",
     "iopub.status.busy": "2026-02-14T09:27:30.868863Z",
     "iopub.status.idle": "2026-02-14T09:27:32.860533Z",
     "shell.execute_reply": "2026-02-14T09:27:32.860184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3b — Create datasets and dataloaders\n",
    "print(\"Creating datasets (caching embeddings)...\")\n",
    "train_df, val_df, test_df = split_by_persona(\n",
    "    labels_df,\n",
    "    entries_df,\n",
    "    train_ratio=CONFIG[\"train_ratio\"],\n",
    "    val_ratio=CONFIG[\"val_ratio\"],\n",
    "    seed=CONFIG[\"seed\"],\n",
    ")\n",
    "\n",
    "train_dataset = VIFDataset(train_df, state_encoder, cache_embeddings=True)\n",
    "val_dataset = VIFDataset(val_df, state_encoder, cache_embeddings=True)\n",
    "test_dataset = VIFDataset(test_df, state_encoder, cache_embeddings=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "n_train = len(train_dataset)\n",
    "n_val = len(val_dataset)\n",
    "n_test = len(test_dataset)\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(\n",
    "    f\"  Train: {n_train} samples ({train_df.select('persona_id').n_unique()} personas) -> {len(train_loader)} batches\"\n",
    ")\n",
    "print(\n",
    "    f\"  Val:   {n_val} samples ({val_df.select('persona_id').n_unique()} personas) -> {len(val_loader)} batches\"\n",
    ")\n",
    "print(\n",
    "    f\"  Test:  {n_test} samples ({test_df.select('persona_id').n_unique()} personas) -> {len(test_loader)} batches\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389d29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:32.861510Z",
     "iopub.status.busy": "2026-02-14T09:27:32.861453Z",
     "iopub.status.idle": "2026-02-14T09:27:32.863353Z",
     "shell.execute_reply": "2026-02-14T09:27:32.862970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3c — Overfitting risk check\n",
    "param_sample_ratio = n_params / n_train\n",
    "print(f\"Parameters:       {n_params:,}\")\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Param/sample ratio: {param_sample_ratio:.1f}x\")\n",
    "print()\n",
    "if param_sample_ratio > 100:\n",
    "    print(\"SEVERE overfitting risk (>100x). Model will likely memorize training data.\")\n",
    "elif param_sample_ratio > 10:\n",
    "    print(\"HIGH overfitting risk (>10x). Strong regularization essential.\")\n",
    "elif param_sample_ratio > 1:\n",
    "    print(\"MODERATE overfitting risk (>1x). Monitor train/val gap carefully.\")\n",
    "else:\n",
    "    print(\"Low overfitting risk (<1x).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d635b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:32.864272Z",
     "iopub.status.busy": "2026-02-14T09:27:32.864203Z",
     "iopub.status.idle": "2026-02-14T09:27:51.292574Z",
     "shell.execute_reply": "2026-02-14T09:27:51.292256Z"
    }
   },
   "outputs": [],
   "source": "# Cell 3d — Multi-model training loop\n\n\ndef train_model(name, model_cfg, train_loader, val_loader, config, device):\n    \"\"\"Train a single model and return its best state + history.\n\n    Each model uses its own loss for early stopping — losses are on different\n    scales (CORAL ~2.0, EMD ~0.2) so cross-model loss comparison\n    is not meaningful.\n    \"\"\"\n    # Per-model seed for reproducibility\n    torch.manual_seed(config[\"seed\"])\n    np.random.seed(config[\"seed\"])\n\n    model = model_cfg[\"class\"](\n        input_dim=state_encoder.state_dim,\n        hidden_dim=config[\"hidden_dim\"],\n        dropout=config[\"dropout\"],\n    )\n    model.to(device)\n\n    loss_fn = model_cfg[\"loss_fn\"]\n\n    optimizer = AdamW(\n        model.parameters(),\n        lr=config[\"learning_rate\"],\n        weight_decay=config[\"weight_decay\"],\n    )\n    scheduler = ReduceLROnPlateau(\n        optimizer,\n        mode=\"min\",\n        factor=config[\"scheduler_factor\"],\n        patience=config[\"scheduler_patience\"],\n        min_lr=config[\"scheduler_min_lr\"],\n    )\n\n    history = {\"train_loss\": [], \"val_loss\": [], \"lr\": []}\n    best_val_loss = float(\"inf\")\n    best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n    best_epoch = 0\n    patience_counter = 0\n\n    for epoch in range(config[\"epochs\"]):\n        # Train\n        model.train()\n        train_loss = 0.0\n        for batch_x, batch_y in train_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n            optimizer.zero_grad()\n            output = model(batch_x)\n            loss = loss_fn(output, batch_y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n\n        # Validate (using same loss for consistent early stopping)\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch_x, batch_y in val_loader:\n                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n                output = model(batch_x)\n                val_loss += loss_fn(output, batch_y).item()\n        val_loss /= len(val_loader)\n\n        if not np.isfinite(val_loss):\n            print(f\"  Epoch {epoch + 1:3d}: non-finite val_loss, stopping\")\n            break\n\n        scheduler.step(val_loss)\n        current_lr = optimizer.param_groups[0][\"lr\"]\n\n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"lr\"].append(current_lr)\n\n        if val_loss < best_val_loss - config[\"early_stopping_min_delta\"]:\n            best_val_loss = val_loss\n            best_epoch = epoch\n            best_model_state = {\n                k: v.cpu().clone() for k, v in model.state_dict().items()\n            }\n            patience_counter = 0\n            if (epoch + 1) <= 3 or (epoch + 1) % 10 == 0:\n                print(\n                    f\"  Epoch {epoch + 1:3d}: train={train_loss:.4f}, \"\n                    f\"val={val_loss:.4f}, lr={current_lr:.6f} [BEST]\"\n                )\n        else:\n            patience_counter += 1\n            if (epoch + 1) % 10 == 0:\n                print(\n                    f\"  Epoch {epoch + 1:3d}: train={train_loss:.4f}, \"\n                    f\"val={val_loss:.4f}, lr={current_lr:.6f}\"\n                )\n\n        if patience_counter >= config[\"early_stopping_patience\"]:\n            print(f\"  Early stopping at epoch {epoch + 1}\")\n            break\n\n    # Restore best model\n    model.load_state_dict(best_model_state)\n    model.to(device)\n\n    return {\n        \"model\": model,\n        \"history\": history,\n        \"best_epoch\": best_epoch,\n        \"best_val_loss\": best_val_loss,\n    }\n\n\n# Train all active models\ntrained_models = {}\nskipped_models = {}\nfor name, cfg in active_models.items():\n    print(f\"\\n{'=' * 70}\")\n    print(f\"Training {name}\")\n    print(f\"{'=' * 70}\")\n    result = train_model(name, cfg, train_loader, val_loader, CONFIG, device)\n    n_epochs = len(result[\"history\"][\"train_loss\"])\n    if n_epochs == 0:\n        skipped_models[name] = result\n        print(\"  No valid epochs completed; excluding model from evaluation.\")\n        continue\n\n    trained_models[name] = result\n    print(\n        f\"  Best val loss: {result['best_val_loss']:.4f} \"\n        f\"at epoch {result['best_epoch'] + 1}/{n_epochs}\"\n    )\n\nif not trained_models:\n    raise RuntimeError(\"No models completed any valid epochs. Check training logs.\")\n\nprint(f\"\\n{'=' * 70}\")\nprint(\n    f\"Training complete: {len(trained_models)} successful, \"\n    f\"{len(skipped_models)} skipped\"\n)\nprint(f\"{'=' * 70}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ea97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:51.293815Z",
     "iopub.status.busy": "2026-02-14T09:27:51.293723Z",
     "iopub.status.idle": "2026-02-14T09:27:51.454775Z",
     "shell.execute_reply": "2026-02-14T09:27:51.454384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3e — Training curves (one subplot per model)\n",
    "n_models = len(trained_models)\n",
    "if n_models == 0:\n",
    "    print(\"No models trained. Check training loop logs above.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 4), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (name, result) in enumerate(trained_models.items()):\n",
    "        ax = axes[idx]\n",
    "        history = result[\"history\"]\n",
    "        best_ep = result[\"best_epoch\"]\n",
    "        epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "        ax.plot(epochs_range, history[\"train_loss\"], label=\"Train\", alpha=0.8)\n",
    "        ax.plot(epochs_range, history[\"val_loss\"], label=\"Val\", alpha=0.8)\n",
    "        if len(history[\"train_loss\"]) > 0:\n",
    "            ax.axvline(\n",
    "                best_ep + 1,\n",
    "                color=\"green\",\n",
    "                linestyle=\":\",\n",
    "                alpha=0.5,\n",
    "                label=f\"Best ({best_ep + 1})\",\n",
    "            )\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_title(f\"{name}\")\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"Training Curves per Model\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary table\n",
    "    print(\n",
    "        f\"\\n{'Model':<15s} {'Best epoch':>12s} {'Train loss':>12s} {'Val loss':>12s} {'Gap':>10s}\"\n",
    "    )\n",
    "    print(\"-\" * 65)\n",
    "    for name, result in trained_models.items():\n",
    "        h = result[\"history\"]\n",
    "        be = result[\"best_epoch\"]\n",
    "        n_ep = len(h[\"train_loss\"])\n",
    "        if n_ep == 0:\n",
    "            print(f\"{name:<15s} {'N/A':>12s} {'N/A':>12s} {'N/A':>12s} {'N/A':>10s}\")\n",
    "            continue\n",
    "        tl = h[\"train_loss\"][be]\n",
    "        vl = h[\"val_loss\"][be]\n",
    "        gap = vl - tl\n",
    "        print(\n",
    "            f\"{name:<15s} {be + 1:>5d}/{n_ep:<5d} {tl:>12.4f} {vl:>12.4f} {gap:>+10.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe620aa0",
   "metadata": {},
   "source": [
    "# Section 4: Evaluation Dashboard (Issues #10, #11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490772c2",
   "metadata": {},
   "source": "**Issue #10 — Class Imbalance:** Does the model predict only neutrals?\n\nThis section provides confusion matrices and prediction distributions to diagnose class imbalance and hedging behavior."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7111a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:51.456127Z",
     "iopub.status.busy": "2026-02-14T09:27:51.456063Z",
     "iopub.status.idle": "2026-02-14T09:27:52.427724Z",
     "shell.execute_reply": "2026-02-14T09:27:52.427304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4a — Evaluate all models with MC Dropout uncertainty\n",
    "all_results = {}\n",
    "for name, result in trained_models.items():\n",
    "    model = result[\"model\"]\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Evaluating {name} with MC Dropout uncertainty...\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    eval_result = evaluate_with_uncertainty(\n",
    "        model,\n",
    "        test_loader,\n",
    "        n_mc_samples=CONFIG[\"mc_dropout_samples\"],\n",
    "        device=device,\n",
    "        include_ordinal_metrics=True,\n",
    "    )\n",
    "    all_results[name] = eval_result\n",
    "    print(f\"\\n{format_results_table(eval_result)}\")\n",
    "\n",
    "if not all_results:\n",
    "    raise RuntimeError(\"No models were evaluated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21db62fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:52.429112Z",
     "iopub.status.busy": "2026-02-14T09:27:52.429044Z",
     "iopub.status.idle": "2026-02-14T09:27:52.507851Z",
     "shell.execute_reply": "2026-02-14T09:27:52.507448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4b — Model comparison table and grouped bar chart\n",
    "\n",
    "# Build comparison DataFrame\n",
    "rows = []\n",
    "for name, res in all_results.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"MAE\": round(res[\"mae_mean\"], 4),\n",
    "            \"Accuracy\": round(res[\"accuracy_mean\"], 4),\n",
    "            \"QWK\": round(res[\"qwk_mean\"], 4),\n",
    "            \"Spearman\": round(res[\"spearman_mean\"], 4),\n",
    "            \"Calibration\": round(\n",
    "                res[\"calibration\"][\"error_uncertainty_correlation\"], 4\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No evaluated models available for comparison.\")\n",
    "\n",
    "comparison_df = pl.DataFrame(rows)\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Grouped bar chart for 3 key metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "model_names = comparison_df[\"Model\"].to_list()\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.6\n",
    "\n",
    "# MAE (lower is better)\n",
    "mae_vals = comparison_df[\"MAE\"].to_list()\n",
    "best_mae = min(mae_vals)\n",
    "colors = [\"#2ecc71\" if v == best_mae else \"#3498db\" for v in mae_vals]\n",
    "axes[0].bar(x, mae_vals, width, color=colors)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "axes[0].set_ylabel(\"MAE\")\n",
    "axes[0].set_title(\"MAE (lower = better)\")\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Accuracy (higher is better)\n",
    "acc_vals = comparison_df[\"Accuracy\"].to_list()\n",
    "best_acc = max(acc_vals)\n",
    "colors = [\"#2ecc71\" if v == best_acc else \"#3498db\" for v in acc_vals]\n",
    "axes[1].bar(x, acc_vals, width, color=colors)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Accuracy (higher = better)\")\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# QWK (higher is better)\n",
    "qwk_vals = comparison_df[\"QWK\"].to_list()\n",
    "best_qwk = max(qwk_vals)\n",
    "colors = [\"#2ecc71\" if v == best_qwk else \"#3498db\" for v in qwk_vals]\n",
    "axes[2].bar(x, qwk_vals, width, color=colors)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "axes[2].set_ylabel(\"QWK\")\n",
    "axes[2].set_title(\"QWK (higher = better)\")\n",
    "axes[2].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"Model Comparison — Key Metrics\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9321f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:52.508940Z",
     "iopub.status.busy": "2026-02-14T09:27:52.508879Z",
     "iopub.status.idle": "2026-02-14T09:27:52.851906Z",
     "shell.execute_reply": "2026-02-14T09:27:52.851598Z"
    }
   },
   "outputs": [],
   "source": "# Cell 4c — Confusion matrices for best model + per-class recall\n\n# Find best model by QWK\nbest_model_name = max(all_results, key=lambda n: all_results[n][\"qwk_mean\"])\n\n# Plot confusion matrices for best model (2x5 grid)\nres = all_results[best_model_name]\npred_cls = discretize_predictions(res[\"predictions\"])\ntarget_cls = discretize_predictions(res[\"targets\"])\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 8))\naxes_flat = axes.flatten()\nfor i, dim in enumerate(SCHWARTZ_VALUE_ORDER):\n    cm = confusion_matrix(target_cls[:, i], pred_cls[:, i], labels=[-1, 0, 1])\n    disp = ConfusionMatrixDisplay(cm, display_labels=[-1, 0, 1])\n    disp.plot(ax=axes_flat[i], cmap=\"Blues\", colorbar=False)\n    axes_flat[i].set_title(dim, fontsize=10)\n    axes_flat[i].set_xlabel(\"Predicted\" if i >= 5 else \"\")\n    axes_flat[i].set_ylabel(\"True\" if i % 5 == 0 else \"\")\nplt.suptitle(f\"Confusion Matrices — {best_model_name} (Best QWK)\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Per-class recall table across all models\nprint(f\"\\n{'=' * 70}\")\nprint(\"Per-Class Recall Across Models (aggregated over 10 dimensions)\")\nprint(f\"{'=' * 70}\")\nprint(\n    f\"\\n{'Model':<15s} {'Recall -1':>10s} {'Recall 0':>10s} \"\n    f\"{'Recall +1':>10s} {'Mean minority':>15s}\"\n)\nprint(\"-\" * 62)\n\nall_recall_data = {}\nfor model_name, res in all_results.items():\n    pred_cls = discretize_predictions(res[\"predictions\"])\n    target_cls = discretize_predictions(res[\"targets\"])\n\n    recall_minus1 = []\n    recall_zero = []\n    recall_plus1 = []\n\n    for i, dim in enumerate(SCHWARTZ_VALUE_ORDER):\n        cm = confusion_matrix(target_cls[:, i], pred_cls[:, i], labels=[-1, 0, 1])\n        for c, recall_list in enumerate([recall_minus1, recall_zero, recall_plus1]):\n            row_sum = cm[c].sum()\n            recall = cm[c, c] / row_sum if row_sum > 0 else float(\"nan\")\n            recall_list.append(recall)\n\n    mean_r_neg = np.nanmean(recall_minus1)\n    mean_r_zero = np.nanmean(recall_zero)\n    mean_r_pos = np.nanmean(recall_plus1)\n    mean_minority = np.nanmean(recall_minus1 + recall_plus1)\n\n    all_recall_data[model_name] = {\n        \"recall_minus1\": mean_r_neg,\n        \"recall_zero\": mean_r_zero,\n        \"recall_plus1\": mean_r_pos,\n        \"mean_minority\": mean_minority,\n    }\n\n    print(\n        f\"{model_name:<15s} {mean_r_neg:>10.1%} {mean_r_zero:>10.1%} \"\n        f\"{mean_r_pos:>10.1%} {mean_minority:>15.1%}\"\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c1e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:52.853074Z",
     "iopub.status.busy": "2026-02-14T09:27:52.853014Z",
     "iopub.status.idle": "2026-02-14T09:27:52.855971Z",
     "shell.execute_reply": "2026-02-14T09:27:52.855663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4d — Hedging comparison: % predictions in [-0.3, 0.3] per model\n",
    "\n",
    "print(f\"{'=' * (20 + 13 * len(all_results))}\")\n",
    "print(\"Hedging Comparison — % predictions in [-0.3, 0.3]\")\n",
    "print(f\"{'=' * (20 + 13 * len(all_results))}\")\n",
    "\n",
    "print(f\"\\n{'Dimension':<20s}\", end=\"\")\n",
    "for name in all_results:\n",
    "    print(f\" {name:>12s}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (20 + 13 * len(all_results)))\n",
    "\n",
    "all_hedging = {}\n",
    "for name, res in all_results.items():\n",
    "    preds = res[\"predictions\"]\n",
    "    near_zero = ((preds > -0.3) & (preds < 0.3)).mean(axis=0)\n",
    "    all_hedging[name] = near_zero\n",
    "\n",
    "for i, dim in enumerate(SCHWARTZ_VALUE_ORDER):\n",
    "    print(f\"{dim:<20s}\", end=\"\")\n",
    "    for name in all_results:\n",
    "        val = all_hedging[name][i]\n",
    "        flag = \"*\" if val > 0.8 else \" \"\n",
    "        print(f\" {val:>11.1%}{flag}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'MEAN':<20s}\", end=\"\")\n",
    "for name in all_results:\n",
    "    mean_val = all_hedging[name].mean()\n",
    "    print(f\" {mean_val:>11.1%} \", end=\"\")\n",
    "print()\n",
    "print(f\"\\n* = hedging (>80% near zero)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dfd0c",
   "metadata": {},
   "source": [
    "# Section 5: Calibration Analysis (Issue #9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434ec83",
   "metadata": {},
   "source": [
    "**Issue 9 — Uncertainty Calibration:** Does higher MC Dropout uncertainty correspond to higher actual error? If not, uncertainty estimates are unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c2799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:52.856977Z",
     "iopub.status.busy": "2026-02-14T09:27:52.856927Z",
     "iopub.status.idle": "2026-02-14T09:27:52.866475Z",
     "shell.execute_reply": "2026-02-14T09:27:52.866156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5a — Per-dimension error-uncertainty correlation across all models\n",
    "\n",
    "print(f\"{'=' * (20 + 13 * len(all_results))}\")\n",
    "print(\"Calibration Analysis — Error-Uncertainty Spearman Correlation\")\n",
    "print(f\"{'=' * (20 + 13 * len(all_results))}\")\n",
    "\n",
    "print(f\"\\n{'Dimension':<20s}\", end=\"\")\n",
    "for name in all_results:\n",
    "    print(f\" {name:>12s}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (20 + 13 * len(all_results)))\n",
    "\n",
    "all_calibration = {}\n",
    "for name, res in all_results.items():\n",
    "    preds = res[\"predictions\"]\n",
    "    tgts = res[\"targets\"]\n",
    "    uncs = res[\"uncertainties\"]\n",
    "    errs = np.abs(preds - tgts)\n",
    "\n",
    "    cal_per_dim = {}\n",
    "    positive_count = 0\n",
    "\n",
    "    for i, dim in enumerate(SCHWARTZ_VALUE_ORDER):\n",
    "        unc_dim = uncs[:, i]\n",
    "        err_dim = errs[:, i]\n",
    "        if np.std(unc_dim) < 1e-8 or np.std(err_dim) < 1e-8:\n",
    "            corr = float(\"nan\")\n",
    "        else:\n",
    "            corr, _ = stats.spearmanr(unc_dim, err_dim)\n",
    "        cal_per_dim[dim] = corr\n",
    "        if not np.isnan(corr) and corr > 0:\n",
    "            positive_count += 1\n",
    "\n",
    "    all_calibration[name] = {\n",
    "        \"per_dim\": cal_per_dim,\n",
    "        \"global\": res[\"calibration\"][\"error_uncertainty_correlation\"],\n",
    "        \"positive_count\": positive_count,\n",
    "        \"mean_uncertainty\": res[\"calibration\"][\"mean_uncertainty\"],\n",
    "    }\n",
    "\n",
    "for i, dim in enumerate(SCHWARTZ_VALUE_ORDER):\n",
    "    print(f\"{dim:<20s}\", end=\"\")\n",
    "    for name in all_results:\n",
    "        corr = all_calibration[name][\"per_dim\"][dim]\n",
    "        corr_str = f\"{corr:+.3f}\" if not np.isnan(corr) else \"N/A\"\n",
    "        print(f\" {corr_str:>12s}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\n{'Global correlation':<20s}\", end=\"\")\n",
    "for name in all_results:\n",
    "    print(f\" {all_calibration[name]['global']:>+12.3f}\", end=\"\")\n",
    "print()\n",
    "print(f\"{'Positive dims':<20s}\", end=\"\")\n",
    "for name in all_results:\n",
    "    print(f\" {all_calibration[name]['positive_count']:>10d}/10\", end=\"\")\n",
    "print()\n",
    "print(f\"{'Mean uncertainty':<20s}\", end=\"\")\n",
    "for name in all_results:\n",
    "    print(f\" {all_calibration[name]['mean_uncertainty']:>12.4f}\", end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f50845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:52.867343Z",
     "iopub.status.busy": "2026-02-14T09:27:52.867287Z",
     "iopub.status.idle": "2026-02-14T09:27:52.910371Z",
     "shell.execute_reply": "2026-02-14T09:27:52.909984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5b — Calibration scatter for best-calibrated model\n",
    "best_cal_model = max(all_calibration, key=lambda n: all_calibration[n][\"global\"])\n",
    "res = all_results[best_cal_model]\n",
    "\n",
    "unc_flat = res[\"uncertainties\"].flatten()\n",
    "err_flat = np.abs(res[\"predictions\"] - res[\"targets\"]).flatten()\n",
    "global_corr = all_calibration[best_cal_model][\"global\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(unc_flat, err_flat, alpha=0.15, s=8, color=\"steelblue\")\n",
    "\n",
    "finite_mask = np.isfinite(unc_flat) & np.isfinite(err_flat)\n",
    "if finite_mask.sum() > 3:\n",
    "    z = np.polyfit(unc_flat[finite_mask], err_flat[finite_mask], 2)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(unc_flat[finite_mask].min(), unc_flat[finite_mask].max(), 200)\n",
    "    ax.plot(\n",
    "        x_line, p(x_line), \"r-\", linewidth=2, label=f\"Trend (rho={global_corr:.3f})\"\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"MC Dropout Uncertainty (std)\")\n",
    "ax.set_ylabel(\"Absolute Error\")\n",
    "ax.set_title(f\"Uncertainty vs Error — {best_cal_model} (Best Calibrated)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b87d37",
   "metadata": {},
   "source": [
    "# Section 6: Issue Scorecard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791af5d3",
   "metadata": {},
   "source": [
    "Summary of all diagnostics for before/after comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d0e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T09:27:52.911747Z",
     "iopub.status.busy": "2026-02-14T09:27:52.911666Z",
     "iopub.status.idle": "2026-02-14T09:27:52.916484Z",
     "shell.execute_reply": "2026-02-14T09:27:52.916145Z"
    }
   },
   "outputs": [],
   "source": "# Cell 6.1 — Multi-model scorecard\nprint(\"=\" * 90)\nprint(\"DIAGNOSTIC SCORECARD — MULTI-MODEL COMPARISON\")\nprint(\"=\" * 90)\n\n# Issue #9 — Calibration\nprint(f\"\\n{'--- #9 Calibration ':-<90s}\")\nprint(\n    f\"  {'Model':<15s} {'Global corr':>12s} {'Positive dims':>15s} {'Mean unc.':>12s}\"\n)\nprint(f\"  {'-' * 56}\")\nfor name in all_results:\n    cal = all_calibration[name]\n    print(\n        f\"  {name:<15s} {cal['global']:>+12.3f} \"\n        f\"{cal['positive_count']:>13d}/10 {cal['mean_uncertainty']:>12.4f}\"\n    )\nprint(f\"  Guidance: >0.1 useful, <0 dangerous (uncertainty misleads)\")\n\n# Issue #10 — Imbalance\nprint(f\"\\n{'--- #10 Imbalance ':-<90s}\")\nprint(f\"  Overall neutral %: {neutral_pct:.1f}%\")\nprint(\n    f\"  Flagged dims (>10x imbalance): {sum(1 for r in imbalance_ratios if r > 10)}/10\"\n)\nprint(f\"  Unlearnable dims: {unlearnable if unlearnable else 'None'}\")\nprint(\n    f\"\\n  {'Model':<15s} {'Recall -1':>10s} {'Recall +1':>10s} {'Mean minority':>15s}\"\n)\nprint(f\"  {'-' * 52}\")\nfor name in all_results:\n    rd = all_recall_data[name]\n    print(\n        f\"  {name:<15s} {rd['recall_minus1']:>10.1%} \"\n        f\"{rd['recall_plus1']:>10.1%} {rd['mean_minority']:>15.1%}\"\n    )\nprint(f\"  Guidance: Minority recall near 0% = model ignores rare classes\")\n\n# Ordinal quality\nprint(f\"\\n{'--- Ordinal Quality ':-<90s}\")\nprint(f\"  {'Model':<15s} {'QWK':>8s} {'Hedging %':>10s}\")\nprint(f\"  {'-' * 35}\")\nfor name in all_results:\n    qwk = all_results[name][\"qwk_mean\"]\n    hedge = all_hedging[name].mean() * 100\n    print(f\"  {name:<15s} {qwk:>8.3f} {hedge:>9.1f}%\")\nprint(f\"  Guidance: High hedging % = neutral collapse; QWK near 0 = chance\")\n\n# Issue #12 — Overfitting\nprint(f\"\\n{'--- #12 Overfitting ':-<90s}\")\nprint(\n    f\"  {'Model':<15s} {'Best epoch':>12s} {'Best val loss':>15s} {'Gap at best':>12s}\"\n)\nprint(f\"  {'-' * 56}\")\nfor name, result in trained_models.items():\n    h = result[\"history\"]\n    be = result[\"best_epoch\"]\n    n_ep = len(h[\"train_loss\"])\n    bvl = result[\"best_val_loss\"]\n    gap = (\n        h[\"val_loss\"][be] - h[\"train_loss\"][be]\n        if len(h[\"train_loss\"]) > be\n        else float(\"nan\")\n    )\n    print(f\"  {name:<15s} {be + 1:>5d}/{n_ep:<5d} {bvl:>15.4f} {gap:>+12.4f}\")\nprint(f\"  Guidance: Ratio >10x high risk; gap >0.05 suggests overfitting\")\n\n# Issue #14 — Embedding (model-independent)\nprint(f\"\\n{'--- #14 Embedding ':-<90s}\")\nprint(f\"  Encoder: {CONFIG['encoder_model']}\")\nprint(f\"  Truncated entries: {pct_truncated:.1f}%\")\nprint(\n    f\"  Guidance: Gap <0.1 = poor discrimination; >5% truncation = losing information\"\n)\n\n# Winner summary\nprint(f\"\\n{'--- Winner Summary ':-<90s}\")\nbest_qwk_model = max(all_results, key=lambda n: all_results[n][\"qwk_mean\"])\nbest_acc_model = max(all_results, key=lambda n: all_results[n][\"accuracy_mean\"])\nbest_mae_model = min(all_results, key=lambda n: all_results[n][\"mae_mean\"])\nbest_cal_model = max(all_calibration, key=lambda n: all_calibration[n][\"global\"])\nprint(\n    f\"  Best by QWK:         {best_qwk_model:<15s} \"\n    f\"({all_results[best_qwk_model]['qwk_mean']:.3f})\"\n)\nprint(\n    f\"  Best by Accuracy:    {best_acc_model:<15s} \"\n    f\"({all_results[best_acc_model]['accuracy_mean']:.2%})\"\n)\nprint(\n    f\"  Best by MAE:         {best_mae_model:<15s} \"\n    f\"({all_results[best_mae_model]['mae_mean']:.4f})\"\n)\nprint(\n    f\"  Best by Calibration: {best_cal_model:<15s} \"\n    f\"({all_calibration[best_cal_model]['global']:.3f})\"\n)\n\nprint(f\"\\n{'=' * 90}\")"
  },
  {
   "cell_type": "markdown",
   "id": "1tq0vg6zdk2",
   "metadata": {},
   "source": [
    "# Section 7: Experiment Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lpri6vmduba",
   "metadata": {},
   "outputs": [],
   "source": "OBSERVATIONS = \"\"  # Leave empty; experiment-review skill will generate observations\n\nfrom src.vif.experiment_logger import log_experiment_run\n\nlogged_paths = log_experiment_run(\n    config=CONFIG,\n    trained_models=trained_models,\n    all_results=all_results,\n    all_calibration=all_calibration,\n    all_hedging=all_hedging,\n    all_recall_data=all_recall_data,\n    n_train=n_train,\n    n_val=n_val,\n    n_test=n_test,\n    pct_truncated=pct_truncated,\n    state_dim=state_encoder.state_dim,\n    observations=OBSERVATIONS.strip(),\n)\nprint(f\"\\nLogged {len(logged_paths)} experiments:\")\nfor entry in logged_paths:\n    tag = \"UPDATED\" if entry[\"status\"] == \"updated\" else \"CREATED\"\n    print(f\"  [{tag}] {entry['path']}  ({entry['run_id']})\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twinkl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
