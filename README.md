# Twinkl

Twinkl is an "inner compass" that helps users align their daily behavior with their long-term values. Unlike traditional journaling apps that summarize moods and topics, Twinkl maintains a dynamic self-model of the user's declared priorities and surfaces tensions when behavior drifts from intent. It answers the question: *"Am I living in line with what I said I value?"*

> **Project status:** Twinkl is under active development as an academic capstone. Status markers in each section indicate capability maturity:
> ‚úÖ Complete ¬∑ üß™ Experimental ¬∑ ‚ö†Ô∏è Partial ¬∑ üìã Specified ¬∑ ‚ùå Not Started
>
> See [Known Gaps](#known-gaps) for the summary and [Implementation Status](docs/prd.md#implementation-status) for the full breakdown.

## Value Identity Function (VIF) ‚Äî üß™ Experimental

The VIF is Twinkl's core evaluative engine. It compares what users *do* (daily journal entries) against what they *value* (their declared priorities) across multiple dimensions like Health, Relationships, and Growth.

Key properties:
- **Vector-valued**: Tracks multiple life dimensions simultaneously, preserving trade-offs (e.g., "work goals crushed, but sleep suffered")
- **Uncertainty-aware**: Holds back judgment when the situation is complex or data is sparse
- **Trajectory-aware**: Detects patterns over time rather than reacting to single entries

These are target design properties. The Critic model infrastructure is functional (MLP + MC Dropout, SBERT encoder, CLI training scripts) but **QWK metric optimization is in progress** ‚Äî see the [Implementation Status](docs/prd.md#implementation-status) for details.

**Target behavior:** When the VIF detects significant misalignment with high confidence, it triggers the Coach layer to surface evidence-based feedback. Currently the Coach is ‚ö†Ô∏è Partial ‚Äî entry processing is ready but digest generation is not yet implemented. See `docs/vif/` for architecture details.

## Synthetic Data Generation ‚Äî ‚úÖ Complete

Bootstraps training data for value tagging and reward modeling. Generates realistic, longitudinal journal entries from synthetic personas with known Schwartz value profiles.

Key features:
- Personas with 1-2 assigned values expressed through concrete life details (not labels)
- Longitudinal entries that exhibit value drift, conflicts, and ambiguity
- Parallel async pipeline for efficient generation
- Configurable tone, verbosity, and reflection mode per entry
- üß™ Two-way conversational journaling with nudge system (LLM-based classification determines when/how to nudge, LLM generates contextual follow-up questions)

**üß™ Experimental**: Assessing whether nudging helps improve VIF data signal quality. See `docs/pipeline/annotation_guidelines.md` for the study methodology.

See `docs/pipeline/pipeline_specs.md` for implementation details.

### Current Dataset

| Metric | Value |
|--------|-------|
| Personas | 120 |
| Journal entries | 904 |
| Avg entries/persona | 7.5 |
| Entries with nudges | 66.7% |

**Demographics:** 6 cultures, 9 professions, 5 age brackets.

**Schwartz Value Distribution** (personas can have 1-2 values):
| Value | Personas | % |
|-------|----------|---|
| Universalism | 34 | 28% |
| Hedonism | 19 | 16% |
| Conformity | 17 | 14% |
| Stimulation | 17 | 14% |
| Security | 15 | 12% |
| Tradition | 15 | 12% |
| Benevolence | 14 | 12% |
| Self-Direction | 14 | 12% |
| Power | 14 | 12% |
| Achievement | 13 | 11% |

**Nudge types:** Elaboration (43%), Tension Surfacing (41%), Clarification (15%)

See [`docs/pipeline/data_schema.md`](docs/pipeline/data_schema.md) for parquet schemas and query examples.

## Judge Labeling Pipeline ‚Äî ‚úÖ Complete

Scores synthetic journal entries against the 10 Schwartz value dimensions to create training labels for the VIF. The pipeline uses Claude Code subagents for parallel, consistent scoring.

**Workflow:**
```
Registry Check ‚Üí Auto-Wrangle ‚Üí Parallel Labeling (subagents) ‚Üí Validation ‚Üí Consolidation
```

**Usage:** Run `/judge` in Claude Code to execute the full pipeline. The skill:
1. Checks the registry for pending work (`logs/registry/personas.parquet`)
2. Auto-wrangles raw synthetic data if needed (`logs/synthetic_data/` ‚Üí `logs/wrangled/`)
3. Spawns parallel subagents ‚Äî one per persona ‚Äî each scoring all entries
4. Validates JSON output against Pydantic models
5. Consolidates labels to `logs/judge_labels/judge_labels.parquet`

**Scoring:** Each entry receives a 10-dimensional vector with values `{-1, 0, +1}` indicating misalignment, neutrality, or alignment with each Schwartz value. **Rationales** explain each non-zero score. Most entries have 1-3 non-zero scores.

**Data outputs:** See [`docs/pipeline/data_schema.md`](docs/pipeline/data_schema.md) for parquet file schemas, example Polars queries, and analytics guidance.

**Key files:**
- `.claude/commands/judge.md` ‚Äî Skill entry point
- `.claude/skills/judge/orchestration.md` ‚Äî Detailed workflow
- `.claude/skills/judge/annotation_guide.md` ‚Äî Scorability heuristics and calibration examples
- `.claude/skills/judge/rubric.md` ‚Äî Schwartz value reference for scoring

**Primary Generation Method:**
- `docs/pipeline/claude_gen_instructions.md` ‚Äî Instructions for Claude Code to generate synthetic data using parallel subagents

**Experimentation Scripts/Modules** (for prompt iteration and testing):
- `src/synthetic/generation.py` ‚Äî One-way generation primitives (context, date sampling, banned-term guards)
- `src/nudge/decision.py` + `src/nudge/generation.py` ‚Äî Two-way conversational nudging logic
- `scripts/journalling/generation_sanity_check.py` ‚Äî Quick local sanity checks

## Human Annotation Tool ‚Äî ‚úÖ Complete

Validates LLM Judge labels via blind human annotation across 10 Schwartz value dimensions. Annotators provide independent scores without seeing Judge labels first; the system then computes agreement metrics (Cohen's Œ∫, Fleiss' Œ∫).

**Run the tool:**
```sh
shiny run src/annotation_tool/app.py
```

Open `http://127.0.0.1:8000` in your browser.

**Features:**
- Displays persona context (name, age, profession, culture, core values, collapsible bio)
- Shows journal entries with nudge/response threading
- 10-value scoring grid with -1 (misaligned) / 0 (neutral) / +1 (aligned) with CSS tooltips for Schwartz value definitions
- Progress tracking per annotator
- Annotations persisted to `logs/annotations/<annotator>.parquet`
- **Analysis & Metrics panel** ‚Äî Computes inter-annotator agreement (Cohen's Œ∫, Fleiss' Œ∫)
- **Export functionality** ‚Äî CSV, Parquet, and Markdown report formats
- **Comparison view** ‚Äî Inline display of human vs judge labels for review

**Key files:**
- `src/annotation_tool/app.py` ‚Äî Main Shiny application
- `src/annotation_tool/data_loader.py` ‚Äî Loads entries from wrangled files
- `src/annotation_tool/annotation_store.py` ‚Äî Persists annotations with file locking
- `src/annotation_tool/agreement_metrics.py` ‚Äî Kappa calculations and export
- `src/annotation_tool/components/` ‚Äî Modular UI components (scoring grid, comparison view, analysis)
- `src/annotation_tool/state.py` ‚Äî Centralized state management
- `docs/pipeline/annotation_tool_plan.md` ‚Äî Full implementation plan

## Evaluation Pipeline ‚Äî ‚ö†Ô∏è Partial

Sequential validation pipeline for the VIF with four stages:
1. **Judge Validation** ‚Äî Training data quality (Cohen's Œ∫ > 0.60)
2. **Value Modeling** ‚Äî Critic learns value hierarchies correctly
3. **Drift Detection** ‚Äî Triggers fire accurately on misalignment
4. **Explanation Quality** ‚Äî Explanations are grounded and useful

See [`docs/evals/overview.md`](docs/evals/overview.md) for the full pipeline overview and current status.

## Known Gaps

| Capability | Status | Note |
|---|---|---|
| Onboarding (BWS Values Assessment) | üìã Specified | Flow designed; not yet implemented |
| Coach digest generation | ‚ö†Ô∏è Partial | Entry processing ready; weekly digest not built |
| Nudge signal quality validation | üß™ Experimental | Annotation study in progress |
| "Map of Me" visualization | ‚ùå Not Started | Embedding trajectories |
| Journaling anomaly radar | ‚ùå Not Started | Cadence/gap detection |
| Goal-aligned inspiration feed | ‚ùå Not Started | External API integration |

For the full breakdown, see the [Implementation Status](docs/prd.md#implementation-status) table in prd.md.

# Setup

This repo uses `uv` and `pyproject.toml` for dependency management.

1. Install `uv`:
   ```sh
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```
   (Or see https://docs.astral.sh/uv/getting-started/installation/ for other methods)

2. Create the virtual environment:
   ```sh
   uv venv
   ```
3. Activate it (Fish shell):
   ```sh
   source .venv/bin/activate.fish
   ```
4. Create a `.env` file in the project root with your OpenAI API key:
   ```sh
   OPENAI_API_KEY=your-api-key-here
   ```

## Installing dependencies

Dependencies are declared in `pyproject.toml` and pinned in `uv.lock`.

- Install everything from the lockfile:
  ```sh
  uv sync
  ```
- If/when a dev group is added later:
  ```sh
  uv sync --dev
  ```

## Adding a dependency

Use `uv add` to both install into the environment and record it in
`pyproject.toml`:

```sh
uv add <package>
```

Pin an exact version if desired:

```sh
uv add "<package>==<version>"
```

After adding, `uv` updates `uv.lock` automatically.

## Exporting requirements.txt (optional)

Only needed for legacy tooling or platforms that require it:

```sh
uv export -o requirements.txt
```
