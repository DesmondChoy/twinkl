# VIF Experiment: run_009_EMD
# Generated: 2026-02-20T14:55:52
# Git: d0b93da(dirty)
metadata:
  experiment_id: run_009_EMD
  run_id: run_009
  model_name: EMD
  timestamp: '2026-02-20T14:55:52'
  git_commit: d0b93da(dirty)
  config_hash: 1213afd665be
provenance:
  prev_run_id: run_005
  prev_git_commit: 4c48773
  git_log:
  - 'd0b93da chore(judge): wrangle and label 10 Power personas (180/180 complete)'
  - '46004f9 docs: update GEMINI.md architecture snapshot and clarify ambiguities'
  - '0468ddf feat(synth): add Power tension scenarios to tension-selection skill'
  - '1831807 chore(vif): add experiment runs 007-008 and update training notebook'
  - '6118a22 chore(vif): add experiment runs 005-006 and update training notebooks'
  config_delta:
    added: {}
    removed: {}
    changed:
      data.pct_truncated:
        from: 31.441
        to: 31.0959
      model.hidden_dim:
        from: 256
        to: 64
  rationale: Reduced MiniLM hidden_dim from 256 to 64 on the expanded dataset (1020
    train with 10 new Power personas). Tests whether the over-parameterization problem
    seen in run_005 (hd=256, param/sample ~386) can be mitigated by capacity reduction
    while retaining MiniLM's 384d embeddings and window_size=3 state encoder.
config:
  encoder:
    model_name: all-MiniLM-L6-v2
  state_encoder:
    window_size: 3
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 31.0959
    state_dim: 1164
  model:
    hidden_dim: 64
    dropout: 0.2
  training:
    loss_fn: emd
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 1020
  n_val: 230
  n_test: 210
capacity:
  n_parameters: 80926
  param_sample_ratio: 79.3392
training_dynamics:
  best_epoch: 4
  total_epochs: 24
  train_loss_at_best: 0.1489
  val_loss_at_best: 0.1538
  gap_at_best: 0.0048
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2252
  accuracy_mean: 0.7995
  qwk_mean: 0.2593
  spearman_mean: 0.3034
  calibration_global: 0.776
  calibration_positive_dims: 10
  mean_uncertainty: 0.1207
  minority_recall_mean: 0.2232
  recall_minus1: 0.0589
  recall_plus1: 0.3876
  hedging_mean: 0.8324
per_dimension:
  self_direction:
    mae: 0.418
    accuracy: 0.619
    qwk: 0.4061
    spearman: 0.4186
    calibration: 0.471
    hedging: 0.5762
  stimulation:
    mae: 0.1024
    accuracy: 0.9095
    qwk: 0.3879
    spearman: 0.5045
    calibration: 0.9212
    hedging: 0.9048
  hedonism:
    mae: 0.1469
    accuracy: 0.8571
    qwk: 0.2173
    spearman: 0.241
    calibration: 0.7344
    hedging: 0.9429
  achievement:
    mae: 0.1937
    accuracy: 0.819
    qwk: 0.1158
    spearman: 0.2534
    calibration: 0.7144
    hedging: 0.9048
  power:
    mae: 0.0673
    accuracy: 0.9476
    qwk: 0.0154
    spearman: -0.2336
    calibration: 0.8714
    hedging: 0.9762
  security:
    mae: 0.3399
    accuracy: 0.7143
    qwk: 0.2041
    spearman: 0.1625
    calibration: 0.745
    hedging: 0.7952
  conformity:
    mae: 0.3064
    accuracy: 0.7095
    qwk: 0.3844
    spearman: 0.5635
    calibration: 0.549
    hedging: 0.7857
  tradition:
    mae: 0.1419
    accuracy: 0.8714
    qwk: 0.4664
    spearman: 0.5567
    calibration: 0.7259
    hedging: 0.9238
  benevolence:
    mae: 0.287
    accuracy: 0.7429
    qwk: 0.3542
    spearman: 0.3783
    calibration: 0.7446
    hedging: 0.6905
  universalism:
    mae: 0.249
    accuracy: 0.8048
    qwk: 0.0415
    spearman: 0.189
    calibration: 0.8466
    hedging: 0.8238
observations: EMD with MiniLM at hd=64 shows QWK 0.259 (poor). Tradition is a bright
  spot (QWK 0.466, Spearman 0.557). Self_direction retains moderate QWK (0.406).
  Universalism collapsed to 0.042 (from 0.290 in run_005). Power near-zero (0.015,
  Spearman -0.234). At 79:1 param/sample ratio, reducing MiniLM capacity to hd=64
  does not recover the QWK degradation seen in run_005 â€” confirming the bottleneck
  is the encoder/state pipeline, not capacity alone.
