# VIF Experiment: run_003_CORAL
# Generated: 2026-02-18T17:32:28
# Git: a3f493f
metadata:
  experiment_id: run_003_CORAL
  run_id: run_003
  model_name: CORAL
  timestamp: '2026-02-18T17:32:28'
  git_commit: a3f493f
  config_hash: b69cc83c2673
provenance:
  prev_run_id: run_001
  prev_git_commit: e1e08c4(dirty)
  git_log:
  - 'a3f493f test: add test suites for judge, registry, and wrangling modules'
  - 2d3f8fd twinkl-g2p remove label-history EMA from VIF state pipeline
  - 'e5f04e7 test(vif): add StateEncoder and VIFDataset parity test suite (twinkl-dks)'
  - 'c605de1 refactor(vif): drop MSE model, focus exclusively on ordinal models (twinkl-hu9.4)'
  - 'e369ddd fix(vif): fail fast on label-entry join losses in merge_labels_and_entries
    (twinkl-hu9.3)'
  - 'a3f0024 docs: add round 2 experiment comparison images for issue #12'
  - '017e833 docs: refresh stale dataset counts across PRD, README, and eval docs
    (twinkl-hu9.6)'
  - '4a47795 fix(nudge): handle non-string JSON fields safely (twinkl-9hk)'
  - '19b6a85 docs(vif): genericize hardcoded architecture values, point to config/vif.yaml'
  - 'e4530a5 fix(nudge): harden parsing and dedupe notebook helpers (twinkl-44r)'
  - 'f296381 feat(nudge): extract nudge decision logic from notebook into src/nudge/'
  - '17504a3 fix(vif): guard eval metrics against NaN/constant-input warnings'
  - '7c612ac fix(vif): wire train_ratio/val_ratio from config through to split_by_persona'
  - '3153d53 docs: standardize agent instruction files'
  - 'f36c87a feat(vif): add experiment logger and run logs for rounds 1-2'
  - '5da7694 refactor(notebooks): reorganize critic training notebooks into v1/v2
    structure'
  config_delta:
    added: {}
    removed:
      state_encoder.ema_alpha: 0.3
    changed: {}
  rationale: Re-run of MiniLM-384d configuration after removing label-history EMA
    from the state pipeline (commit 2d3f8fd) and dropping MSE loss (commit c605de1).
    Config delta shows ema_alpha removed. Tests whether EMA removal and code fixes
    affect MiniLM performance.
config:
  encoder:
    model_name: all-MiniLM-L6-v2
  state_encoder:
    window_size: 3
  model:
    hidden_dim: 256
    dropout: 0.2
  training:
    loss_fn: coral
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
data:
  n_train: 637
  n_val: 124
  n_test: 143
  train_ratio: 0.7
  val_ratio: 0.15
  split_seed: 2025
  pct_truncated: 33.2965
  state_dim: 1164
capacity:
  n_parameters: 370196
  param_sample_ratio: 581.1554
training_dynamics:
  best_epoch: 3
  total_epochs: 23
  train_loss_at_best: 0.5224
  val_loss_at_best: 0.6281
  gap_at_best: 0.1057
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2408
  accuracy_mean: 0.7762
  qwk_mean: 0.369
  spearman_mean: 0.3939
  calibration_global: 0.6398
  calibration_positive_dims: 10
  mean_uncertainty: 0.0782
  minority_recall_mean: 0.2652
  recall_minus1: 0.0265
  recall_plus1: 0.5039
  hedging_mean: 0.8406
per_dimension:
  self_direction:
    mae: 0.3792
    accuracy: 0.6294
    qwk: 0.278
    spearman: 0.375
    calibration: 0.5163
    hedging: 0.7762
  stimulation:
    mae: 0.1326
    accuracy: 0.8811
    qwk: 0.5179
    spearman: 0.4188
    calibration: 0.7304
    hedging: 0.8671
  hedonism:
    mae: 0.194
    accuracy: 0.8182
    qwk: 0.4485
    spearman: 0.5065
    calibration: 0.4995
    hedging: 0.8811
  achievement:
    mae: 0.3055
    accuracy: 0.7552
    qwk: 0.331
    spearman: 0.1284
    calibration: 0.718
    hedging: 0.7622
  power:
    mae: 0.1403
    accuracy: 0.8741
    qwk: 0.4043
    spearman: 0.3965
    calibration: 0.6822
    hedging: 0.9231
  security:
    mae: 0.3074
    accuracy: 0.6993
    qwk: 0.1887
    spearman: 0.3385
    calibration: 0.4492
    hedging: 0.9301
  conformity:
    mae: 0.2832
    accuracy: 0.7203
    qwk: 0.0312
    spearman: 0.1813
    calibration: 0.4739
    hedging: 0.9301
  tradition:
    mae: 0.1642
    accuracy: 0.8462
    qwk: 0.5013
    spearman: 0.5308
    calibration: 0.7487
    hedging: 0.8741
  benevolence:
    mae: 0.3627
    accuracy: 0.6643
    qwk: 0.3825
    spearman: 0.4743
    calibration: 0.481
    hedging: 0.6643
  universalism:
    mae: 0.139
    accuracy: 0.8741
    qwk: 0.6071
    spearman: 0.5894
    calibration: 0.8314
    hedging: 0.7972
observations: CORAL QWK (0.369) decreased slightly from run_001 (0.398) after EMA
  removal, with state_dim dropping from 1174 to 1164. Hedging increased to 84.1% and
  minority recall dropped to 26.5%. Conformity collapsed (QWK 0.031) compared to
  run_001 (0.177). Tradition improved (QWK 0.501 vs 0.349). Overall a slight
  regression, suggesting EMA features carried some useful signal for CORAL.
