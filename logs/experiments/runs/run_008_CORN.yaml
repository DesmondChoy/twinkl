# VIF Experiment: run_008_CORN
# Generated: 2026-02-20T14:47:14
# Git: d0b93da(dirty)
metadata:
  experiment_id: run_008_CORN
  run_id: run_008
  model_name: CORN
  timestamp: '2026-02-20T14:47:14'
  git_commit: d0b93da(dirty)
  config_hash: 4879bb38a8c8
provenance:
  prev_run_id: run_007
  prev_git_commit: d0b93da(dirty)
  git_log: []
  config_delta:
    added: {}
    removed: {}
    changed:
      model.hidden_dim:
        from: 64
        to: 128
  rationale: Doubled hidden_dim from 64 to 128 on nomic-embed-256d to continue the capacity
    sweep. Tests whether QWK improvements from hd=64 (run_007) continue to scale with
    more parameters, or whether diminishing returns or overfitting emerge at the 53:1
    param/sample ratio.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 128
    dropout: 0.3
  training:
    loss_fn: corn
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 1020
  n_val: 230
  n_test: 210
capacity:
  n_parameters: 53780
  param_sample_ratio: 52.7255
training_dynamics:
  best_epoch: 8
  total_epochs: 28
  train_loss_at_best: 0.2258
  val_loss_at_best: 0.2311
  gap_at_best: 0.0053
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2032
  accuracy_mean: 0.8276
  qwk_mean: 0.3441
  spearman_mean: 0.3588
  calibration_global: 0.7849
  calibration_positive_dims: 10
  mean_uncertainty: 0.1096
  minority_recall_mean: 0.2759
  recall_minus1: 0.0572
  recall_plus1: 0.4945
  hedging_mean: 0.829
per_dimension:
  self_direction:
    mae: 0.3875
    accuracy: 0.6667
    qwk: 0.3516
    spearman: 0.4298
    calibration: 0.5245
    hedging: 0.6952
  stimulation:
    mae: 0.089
    accuracy: 0.919
    qwk: 0.4681
    spearman: 0.4756
    calibration: 0.8315
    hedging: 0.9333
  hedonism:
    mae: 0.1328
    accuracy: 0.881
    qwk: 0.3223
    spearman: 0.2983
    calibration: 0.7513
    hedging: 0.9286
  achievement:
    mae: 0.1849
    accuracy: 0.8524
    qwk: 0.43
    spearman: 0.3238
    calibration: 0.7804
    hedging: 0.8095
  power:
    mae: 0.0655
    accuracy: 0.9476
    qwk: 0.0154
    spearman: -0.2045
    calibration: 0.8757
    hedging: 0.9667
  security:
    mae: 0.3227
    accuracy: 0.7381
    qwk: 0.1236
    spearman: 0.2092
    calibration: 0.6436
    hedging: 0.7905
  conformity:
    mae: 0.274
    accuracy: 0.7333
    qwk: 0.4408
    spearman: 0.5656
    calibration: 0.5647
    hedging: 0.8095
  tradition:
    mae: 0.1408
    accuracy: 0.8667
    qwk: 0.4683
    spearman: 0.5905
    calibration: 0.8296
    hedging: 0.8714
  benevolence:
    mae: 0.2156
    accuracy: 0.8286
    qwk: 0.5646
    spearman: 0.5769
    calibration: 0.8296
    hedging: 0.7286
  universalism:
    mae: 0.2198
    accuracy: 0.8429
    qwk: 0.2558
    spearman: 0.3224
    calibration: 0.9141
    hedging: 0.7571
observations: CORN at hd=128 shows the sharpest capacity-related regression — QWK
  dropped from 0.413 (run_007, best overall) to 0.344. Calibration declined from 0.838
  to 0.785. Benevolence is a bright spot (QWK 0.565, Spearman 0.577 — both best-ever
  for CORN). Tradition strong (0.468). Power near-zero (0.015) with strongly negative
  Spearman (-0.205). Security weak (0.124). CORN appears most capacity-sensitive —
  the hd=64 sweet spot is critical for this loss function.
