# VIF Experiment: run_007_CORAL
# Generated: 2026-02-20T14:40:56
# Git: d0b93da(dirty)
metadata:
  experiment_id: run_007_CORAL
  run_id: run_007
  model_name: CORAL
  timestamp: '2026-02-20T14:40:56'
  git_commit: d0b93da(dirty)
  config_hash: 369eb2a70a22
provenance:
  prev_run_id: run_006
  prev_git_commit: 4c48773(dirty)
  git_log:
  - 'd0b93da chore(judge): wrangle and label 10 Power personas (180/180 complete)'
  - '46004f9 docs: update GEMINI.md architecture snapshot and clarify ambiguities'
  - '0468ddf feat(synth): add Power tension scenarios to tension-selection skill'
  - '1831807 chore(vif): add experiment runs 007-008 and update training notebook'
  - '6118a22 chore(vif): add experiment runs 005-006 and update training notebooks'
  config_delta:
    added: {}
    removed: {}
    changed:
      model.hidden_dim:
        from: 32
        to: 64
  rationale: Doubled hidden_dim from 32 to 64 on nomic-embed-256d to test whether additional
    capacity improves QWK and minority recall. Dataset expanded to 1020 train with 10
    new Power tension personas (180/180 complete). Tests the capacity sweet spot between
    the compact hd=32 and the over-parameterized MiniLM hd=256.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 64
    dropout: 0.3
  training:
    loss_fn: coral
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 1020
  n_val: 230
  n_test: 210
capacity:
  n_parameters: 22804
  param_sample_ratio: 22.3569
training_dynamics:
  best_epoch: 22
  total_epochs: 42
  train_loss_at_best: 0.4148
  val_loss_at_best: 0.4734
  gap_at_best: 0.0586
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2076
  accuracy_mean: 0.8186
  qwk_mean: 0.3672
  spearman_mean: 0.398
  calibration_global: 0.8296
  calibration_positive_dims: 10
  mean_uncertainty: 0.1475
  minority_recall_mean: 0.2473
  recall_minus1: 0.0694
  recall_plus1: 0.4252
  hedging_mean: 0.8424
per_dimension:
  self_direction:
    mae: 0.3871
    accuracy: 0.6524
    qwk: 0.439
    spearman: 0.4501
    calibration: 0.5549
    hedging: 0.6429
  stimulation:
    mae: 0.0942
    accuracy: 0.919
    qwk: 0.4436
    spearman: 0.3204
    calibration: 0.8768
    hedging: 0.9286
  hedonism:
    mae: 0.1437
    accuracy: 0.8571
    qwk: 0.2441
    spearman: 0.3464
    calibration: 0.7823
    hedging: 0.9333
  achievement:
    mae: 0.1779
    accuracy: 0.8571
    qwk: 0.3909
    spearman: 0.3456
    calibration: 0.7677
    hedging: 0.8571
  power:
    mae: 0.057
    accuracy: 0.9476
    qwk: 0.2731
    spearman: 0.2731
    calibration: 0.9391
    hedging: 0.9714
  security:
    mae: 0.3226
    accuracy: 0.719
    qwk: 0.1559
    spearman: 0.2905
    calibration: 0.7293
    hedging: 0.8048
  conformity:
    mae: 0.2787
    accuracy: 0.7476
    qwk: 0.476
    spearman: 0.5357
    calibration: 0.749
    hedging: 0.7476
  tradition:
    mae: 0.1697
    accuracy: 0.8524
    qwk: 0.4035
    spearman: 0.4691
    calibration: 0.8722
    hedging: 0.881
  benevolence:
    mae: 0.2329
    accuracy: 0.8238
    qwk: 0.532
    spearman: 0.5177
    calibration: 0.7792
    hedging: 0.8
  universalism:
    mae: 0.2123
    accuracy: 0.8095
    qwk: 0.3137
    spearman: 0.4312
    calibration: 0.9059
    hedging: 0.8571
observations: CORAL at hd=64 achieves QWK 0.367 â€” the best CORAL result on nomic and
  comparable to MiniLM's best (0.398, run_001). Calibration 0.830 (good) is the best
  CORAL has achieved across all runs. Benevolence excels (QWK 0.532) and conformity
  shows dramatic improvement (0.476 vs near-zero in earlier runs). Security remains weak
  (0.156). Power near-zero (0.273 with 97.1% hedging). MAE 0.208 is the best for any
  CORAL run on the expanded dataset.
