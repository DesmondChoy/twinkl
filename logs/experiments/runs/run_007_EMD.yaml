# VIF Experiment: run_007_EMD
# Generated: 2026-02-20T10:42:25
# Git: 4c48773(dirty)
metadata:
  experiment_id: run_007_EMD
  run_id: run_007
  model_name: EMD
  timestamp: '2026-02-20T10:42:25'
  git_commit: 4c48773(dirty)
  config_hash: d8fff8658e1c
provenance:
  prev_run_id: run_006
  prev_git_commit: 4c48773(dirty)
  git_log: []
  config_delta:
    added: {}
    removed: {}
    changed:
      model.hidden_dim:
        from: 32
        to: 64
  rationale: Capacity exploration on nomic-embed-256d encoder, increasing hidden_dim
    from 32 to 64 while keeping all other config identical to run_006. Tests whether
    doubling MLP capacity improves QWK and minority recall on the expanded dataset.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 64
    dropout: 0.3
  training:
    loss_fn: emd
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 958
  n_val: 214
  n_test: 202
capacity:
  n_parameters: 23454
  param_sample_ratio: 24.4823
training_dynamics:
  best_epoch: 6
  total_epochs: 26
  train_loss_at_best: 0.154
  val_loss_at_best: 0.1723
  gap_at_best: 0.0183
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2023
  accuracy_mean: 0.8203
  qwk_mean: 0.3639
  spearman_mean: 0.3797
  calibration_global: 0.7496
  calibration_positive_dims: 10
  mean_uncertainty: 0.117
  minority_recall_mean: 0.2171
  recall_minus1: 0.013
  recall_plus1: 0.4212
  hedging_mean: 0.85
per_dimension:
  self_direction:
    mae: 0.3107
    accuracy: 0.7178
    qwk: 0.3205
    spearman: 0.3088
    calibration: 0.5292
    hedging: 0.7921
  stimulation:
    mae: 0.1372
    accuracy: 0.8713
    qwk: 0.3516
    spearman: 0.555
    calibration: 0.8257
    hedging: 0.9208
  hedonism:
    mae: 0.1184
    accuracy: 0.9109
    qwk: 0.505
    spearman: 0.2986
    calibration: 0.9063
    hedging: 0.8911
  achievement:
    mae: 0.2485
    accuracy: 0.797
    qwk: 0.5491
    spearman: 0.5745
    calibration: 0.7686
    hedging: 0.6089
  power:
    mae: 0.1466
    accuracy: 0.8713
    qwk: 0.0896
    spearman: 0.2303
    calibration: 0.8031
    hedging: 0.9604
  security:
    mae: 0.2093
    accuracy: 0.802
    qwk: 0.3329
    spearman: 0.2778
    calibration: 0.6488
    hedging: 0.9307
  conformity:
    mae: 0.2434
    accuracy: 0.7723
    qwk: 0.204
    spearman: 0.2739
    calibration: 0.6193
    hedging: 0.9406
  tradition:
    mae: 0.17
    accuracy: 0.8416
    qwk: 0.4088
    spearman: 0.3193
    calibration: 0.6666
    hedging: 0.9109
  benevolence:
    mae: 0.3055
    accuracy: 0.7327
    qwk: 0.3616
    spearman: 0.3902
    calibration: 0.597
    hedging: 0.7079
  universalism:
    mae: 0.1331
    accuracy: 0.8861
    qwk: 0.516
    spearman: 0.5689
    calibration: 0.9072
    hedging: 0.8366
observations: EMD achieves the best MAE of any run (0.202) and best accuracy (0.820)
  with hd=64. QWK improved to 0.364 (from 0.324 in run_006 hd=32). Achievement was
  the strongest dimension (QWK 0.549, Spearman 0.575). Universalism recovered to
  0.516 (from 0.449 in run_006). Power remains near-zero (0.090). Training gap is
  minimal (0.018) with best_epoch=6, suggesting efficient learning. This is the best
  overall EMD configuration on the expanded dataset.
