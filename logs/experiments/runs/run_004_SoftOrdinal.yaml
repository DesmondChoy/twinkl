# VIF Experiment: run_004_SoftOrdinal
# Generated: 2026-02-18T17:35:21
# Git: a3f493f(dirty)
metadata:
  experiment_id: run_004_SoftOrdinal
  run_id: run_004
  model_name: SoftOrdinal
  timestamp: '2026-02-18T17:35:21'
  git_commit: a3f493f(dirty)
  config_hash: cb1ef6abeca2
provenance:
  prev_run_id: run_002
  prev_git_commit: e1e08c4(dirty)
  git_log:
  - 'a3f493f test: add test suites for judge, registry, and wrangling modules'
  - 2d3f8fd twinkl-g2p remove label-history EMA from VIF state pipeline
  - 'e5f04e7 test(vif): add StateEncoder and VIFDataset parity test suite (twinkl-dks)'
  - 'c605de1 refactor(vif): drop MSE model, focus exclusively on ordinal models (twinkl-hu9.4)'
  - 'e369ddd fix(vif): fail fast on label-entry join losses in merge_labels_and_entries
    (twinkl-hu9.3)'
  - 'a3f0024 docs: add round 2 experiment comparison images for issue #12'
  - '017e833 docs: refresh stale dataset counts across PRD, README, and eval docs
    (twinkl-hu9.6)'
  - '4a47795 fix(nudge): handle non-string JSON fields safely (twinkl-9hk)'
  - '19b6a85 docs(vif): genericize hardcoded architecture values, point to config/vif.yaml'
  - 'e4530a5 fix(nudge): harden parsing and dedupe notebook helpers (twinkl-44r)'
  - 'f296381 feat(nudge): extract nudge decision logic from notebook into src/nudge/'
  - '17504a3 fix(vif): guard eval metrics against NaN/constant-input warnings'
  - '7c612ac fix(vif): wire train_ratio/val_ratio from config through to split_by_persona'
  - '3153d53 docs: standardize agent instruction files'
  - 'f36c87a feat(vif): add experiment logger and run logs for rounds 1-2'
  - '5da7694 refactor(notebooks): reorganize critic training notebooks into v1/v2
    structure'
  config_delta:
    added: {}
    removed:
      state_encoder.ema_alpha: 0.3
    changed: {}
  rationale: Re-run of nomic-embed-256d configuration after removing label-history
    EMA from the state pipeline (commit 2d3f8fd) and dropping MSE loss (commit c605de1).
    Config delta shows ema_alpha removed. Tests whether EMA removal and code fixes
    affect nomic/small-model performance.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  model:
    hidden_dim: 32
    dropout: 0.3
  training:
    loss_fn: soft_ordinal
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
data:
  n_train: 637
  n_val: 124
  n_test: 143
  train_ratio: 0.7
  val_ratio: 0.15
  split_seed: 2025
  pct_truncated: 0.0
  state_dim: 266
capacity:
  n_parameters: 10718
  param_sample_ratio: 16.8257
training_dynamics:
  best_epoch: 23
  total_epochs: 43
  train_loss_at_best: 2.0881
  val_loss_at_best: 2.4362
  gap_at_best: 0.3481
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2691
  accuracy_mean: 0.779
  qwk_mean: 0.3849
  spearman_mean: 0.3513
  calibration_global: 0.7865
  calibration_positive_dims: 10
  mean_uncertainty: 0.1796
  minority_recall_mean: 0.3322
  recall_minus1: 0.0929
  recall_plus1: 0.5715
  hedging_mean: 0.7552
per_dimension:
  self_direction:
    mae: 0.4071
    accuracy: 0.6573
    qwk: 0.3691
    spearman: 0.4073
    calibration: 0.6115
    hedging: 0.5944
  stimulation:
    mae: 0.1449
    accuracy: 0.8601
    qwk: 0.4568
    spearman: 0.4361
    calibration: 0.866
    hedging: 0.8531
  hedonism:
    mae: 0.22
    accuracy: 0.8112
    qwk: 0.4187
    spearman: 0.373
    calibration: 0.6666
    hedging: 0.8811
  achievement:
    mae: 0.3667
    accuracy: 0.6993
    qwk: 0.2595
    spearman: 0.1743
    calibration: 0.7509
    hedging: 0.6713
  power:
    mae: 0.145
    accuracy: 0.8951
    qwk: 0.3481
    spearman: 0.1684
    calibration: 0.9243
    hedging: 0.8881
  security:
    mae: 0.3292
    accuracy: 0.7133
    qwk: 0.3125
    spearman: 0.2849
    calibration: 0.5248
    hedging: 0.7832
  conformity:
    mae: 0.2906
    accuracy: 0.7762
    qwk: 0.3689
    spearman: 0.2379
    calibration: 0.7706
    hedging: 0.7902
  tradition:
    mae: 0.2039
    accuracy: 0.8392
    qwk: 0.4887
    spearman: 0.4755
    calibration: 0.8148
    hedging: 0.8112
  benevolence:
    mae: 0.3975
    accuracy: 0.6783
    qwk: 0.4176
    spearman: 0.4701
    calibration: 0.7322
    hedging: 0.5175
  universalism:
    mae: 0.1857
    accuracy: 0.8601
    qwk: 0.4092
    spearman: 0.4858
    calibration: 0.9209
    hedging: 0.7622
observations: SoftOrdinal QWK (0.385) is nearly identical to run_002 (0.385), showing
  complete robustness to EMA removal. Achieves the highest calibration of any run
  (0.786) and lowest hedging among nomic ordinal runs (0.755). Power calibration
  (0.924) is exceptional. Tradition (QWK 0.489) and benevolence (QWK 0.418) are
  strong. Achievement Spearman (0.174) remains anomalously low. SoftOrdinal on nomic
  is the best-calibrated configuration across all experiments.
