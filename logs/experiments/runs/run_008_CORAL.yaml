# VIF Experiment: run_008_CORAL
# Generated: 2026-02-20T10:47:40
# Git: 6118a22(dirty)
metadata:
  experiment_id: run_008_CORAL
  run_id: run_008
  model_name: CORAL
  timestamp: '2026-02-20T10:47:40'
  git_commit: 6118a22(dirty)
  config_hash: 67897a9de61a
provenance:
  prev_run_id: run_007
  prev_git_commit: 4c48773(dirty)
  git_log:
  - '6118a22 chore(vif): add experiment runs 005-006 and update training notebooks'
  config_delta:
    added: {}
    removed: {}
    changed:
      model.hidden_dim:
        from: 64
        to: 128
  rationale: Continued capacity exploration on nomic-embed-256d encoder, increasing
    hidden_dim from 64 to 128. Tests whether further capacity gains improve metrics
    or trigger overfitting at param/sample ratio ~56.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 128
    dropout: 0.3
  training:
    loss_fn: coral
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 958
  n_val: 214
  n_test: 202
capacity:
  n_parameters: 53780
  param_sample_ratio: 56.1378
training_dynamics:
  best_epoch: 6
  total_epochs: 26
  train_loss_at_best: 0.4866
  val_loss_at_best: 0.5854
  gap_at_best: 0.0988
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2045
  accuracy_mean: 0.8129
  qwk_mean: 0.3565
  spearman_mean: 0.4027
  calibration_global: 0.762
  calibration_positive_dims: 10
  mean_uncertainty: 0.1064
  minority_recall_mean: 0.2353
  recall_minus1: 0.0265
  recall_plus1: 0.4441
  hedging_mean: 0.8322
per_dimension:
  self_direction:
    mae: 0.3356
    accuracy: 0.7079
    qwk: 0.2867
    spearman: 0.3466
    calibration: 0.5922
    hedging: 0.7178
  stimulation:
    mae: 0.1254
    accuracy: 0.8762
    qwk: 0.4314
    spearman: 0.5473
    calibration: 0.782
    hedging: 0.8614
  hedonism:
    mae: 0.1118
    accuracy: 0.9109
    qwk: 0.5551
    spearman: 0.421
    calibration: 0.852
    hedging: 0.8713
  achievement:
    mae: 0.2415
    accuracy: 0.7772
    qwk: 0.4916
    spearman: 0.5543
    calibration: 0.7464
    hedging: 0.6782
  power:
    mae: 0.1465
    accuracy: 0.8713
    qwk: 0.147
    spearman: 0.1336
    calibration: 0.6926
    hedging: 0.9505
  security:
    mae: 0.2024
    accuracy: 0.802
    qwk: 0.3936
    spearman: 0.3064
    calibration: 0.7697
    hedging: 0.8861
  conformity:
    mae: 0.2438
    accuracy: 0.7723
    qwk: 0.2769
    spearman: 0.3112
    calibration: 0.6886
    hedging: 0.8911
  tradition:
    mae: 0.179
    accuracy: 0.8218
    qwk: 0.2242
    spearman: 0.4485
    calibration: 0.6316
    hedging: 0.9406
  benevolence:
    mae: 0.3101
    accuracy: 0.7277
    qwk: 0.3487
    spearman: 0.4061
    calibration: 0.6679
    hedging: 0.6683
  universalism:
    mae: 0.1484
    accuracy: 0.8614
    qwk: 0.4097
    spearman: 0.5523
    calibration: 0.832
    hedging: 0.8564
observations: CORAL QWK improved slightly to 0.357 (from 0.349 at hd=64), with
  the best CORAL MAE overall (0.205). Achievement continued strong (QWK 0.492).
  Hedonism reached 0.555, the best for any CORAL run. Calibration held at 0.762.
  Best epoch dropped to 6 (from 10 at hd=64), indicating faster convergence. Power
  remained weak (0.147). CORAL shows monotonic improvement with capacity on nomic
  but with diminishing returns between hd=64 and hd=128.
