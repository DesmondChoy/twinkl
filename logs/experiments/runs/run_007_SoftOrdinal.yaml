# VIF Experiment: run_007_SoftOrdinal
# Generated: 2026-02-20T10:42:25
# Git: 4c48773(dirty)
metadata:
  experiment_id: run_007_SoftOrdinal
  run_id: run_007
  model_name: SoftOrdinal
  timestamp: '2026-02-20T10:42:25'
  git_commit: 4c48773(dirty)
  config_hash: f53684eed126
provenance:
  prev_run_id: run_006
  prev_git_commit: 4c48773(dirty)
  git_log: []
  config_delta:
    added: {}
    removed: {}
    changed:
      model.hidden_dim:
        from: 32
        to: 64
  rationale: Capacity exploration on nomic-embed-256d encoder, increasing hidden_dim
    from 32 to 64 while keeping all other config identical to run_006. Tests whether
    doubling MLP capacity improves QWK and minority recall on the expanded dataset.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 64
    dropout: 0.3
  training:
    loss_fn: soft_ordinal
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 958
  n_val: 214
  n_test: 202
capacity:
  n_parameters: 23454
  param_sample_ratio: 24.4823
training_dynamics:
  best_epoch: 5
  total_epochs: 25
  train_loss_at_best: 2.1348
  val_loss_at_best: 2.2682
  gap_at_best: 0.1334
  final_lr: 0.0005
evaluation:
  mae_mean: 0.211
  accuracy_mean: 0.8168
  qwk_mean: 0.3285
  spearman_mean: 0.3847
  calibration_global: 0.7609
  calibration_positive_dims: 10
  mean_uncertainty: 0.1354
  minority_recall_mean: 0.218
  recall_minus1: 0.0308
  recall_plus1: 0.4051
  hedging_mean: 0.8342
per_dimension:
  self_direction:
    mae: 0.3191
    accuracy: 0.7426
    qwk: 0.4308
    spearman: 0.3655
    calibration: 0.5358
    hedging: 0.7376
  stimulation:
    mae: 0.1345
    accuracy: 0.8663
    qwk: 0.2793
    spearman: 0.5151
    calibration: 0.8153
    hedging: 0.9109
  hedonism:
    mae: 0.1139
    accuracy: 0.901
    qwk: 0.4428
    spearman: 0.4531
    calibration: 0.8179
    hedging: 0.8911
  achievement:
    mae: 0.2692
    accuracy: 0.7921
    qwk: 0.5484
    spearman: 0.5703
    calibration: 0.773
    hedging: 0.5297
  power:
    mae: 0.1503
    accuracy: 0.8713
    qwk: -0.0172
    spearman: 0.0865
    calibration: 0.8289
    hedging: 0.9505
  security:
    mae: 0.2158
    accuracy: 0.797
    qwk: 0.3285
    spearman: 0.3743
    calibration: 0.6783
    hedging: 0.9109
  conformity:
    mae: 0.2526
    accuracy: 0.7921
    qwk: 0.3777
    spearman: 0.303
    calibration: 0.709
    hedging: 0.8614
  tradition:
    mae: 0.1932
    accuracy: 0.802
    qwk: 0.0424
    spearman: 0.2363
    calibration: 0.6409
    hedging: 0.9653
  benevolence:
    mae: 0.3205
    accuracy: 0.7228
    qwk: 0.2798
    spearman: 0.2946
    calibration: 0.5744
    hedging: 0.7525
  universalism:
    mae: 0.1412
    accuracy: 0.8812
    qwk: 0.5721
    spearman: 0.648
    calibration: 0.8522
    hedging: 0.8317
observations: SoftOrdinal QWK slightly decreased from 0.358 (run_006 hd=32) to
  0.329 with hd=64 â€” unlike CORAL/EMD, SoftOrdinal does not benefit from increased
  capacity here. Achievement was exceptional (QWK 0.548, best for any SoftOrdinal
  run). Self_direction improved to 0.431 (best across all runs for that dimension).
  Tradition collapsed to 0.042 and power turned negative (-0.017). Universalism
  remained strong at 0.572. The training gap (0.133) is moderate.
