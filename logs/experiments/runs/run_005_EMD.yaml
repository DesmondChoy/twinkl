# VIF Experiment: run_005_EMD
# Generated: 2026-02-20T09:25:59
# Git: 4c48773(dirty)
metadata:
  experiment_id: run_005_EMD
  run_id: run_005
  model_name: EMD
  timestamp: '2026-02-20T09:25:59'
  git_commit: 4c48773(dirty)
  config_hash: e45f0a21e12e
provenance:
  prev_run_id: run_003
  prev_git_commit: a3f493f
  git_log:
  - '4c48773 docs: remove redundant introductory sentences from AGENTS.md and GEMINI.md'
  - '31918e6 fix(vif): include dataset splits in experiment logger deduplication hash'
  - 'e5cb197 fix(vif): include dataset splits in experiment logger deduplication hash'
  - 'b84d1ca fix(judge): backfill rationales for 61 personas and fix under-labeled
    entry'
  - 'c0435e1 chore(data): add batch 2 judge labels for 60 new personas'
  - 'cd59d19 chore(data): add batch 2 synthetic and wrangled persona data (60 personas)'
  - '986e408 feat(judge): add consolidation module, harden wrangling parser, and update
    judge pipeline docs'
  - 'a036004 chore(data): remove batch 1A pre-tension Universalism personas (10)'
  - de4a5e1 Refactor notebook references to scripts and harden synthetic helpers (twinkl-ayp)
  - '62cbe11 docs(gen): fix config variables — remove dead START_DATE, wire MIN_DAYS_BETWEEN_ENTRIES'
  - '9e3e22a feat(vif): add runs 003/004 experiment logs with provenance backfill'
  config_delta:
    added:
      data.train_ratio: 0.7
      data.val_ratio: 0.15
      data.split_seed: 2025
      data.pct_truncated: 31.441
      data.state_dim: 1164
      uncertainty.mc_dropout_samples: 50
    removed: {}
    changed: {}
  rationale: Re-run of MiniLM-384d configuration with expanded dataset (958 train
    vs 637). Added 60 new personas from batch 2, removed 10 pre-tension Universalism
    personas, and added MC dropout uncertainty (50 samples). Tests whether 50% more
    data improves MiniLM performance.
config:
  encoder:
    model_name: all-MiniLM-L6-v2
  state_encoder:
    window_size: 3
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 31.441
    state_dim: 1164
  model:
    hidden_dim: 256
    dropout: 0.2
  training:
    loss_fn: emd
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 958
  n_val: 214
  n_test: 202
capacity:
  n_parameters: 372766
  param_sample_ratio: 389.1086
training_dynamics:
  best_epoch: 4
  total_epochs: 24
  train_loss_at_best: 0.0948
  val_loss_at_best: 0.18
  gap_at_best: 0.0851
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2257
  accuracy_mean: 0.7906
  qwk_mean: 0.2716
  spearman_mean: 0.3118
  calibration_global: 0.6514
  calibration_positive_dims: 10
  mean_uncertainty: 0.0844
  minority_recall_mean: 0.2169
  recall_minus1: 0.067
  recall_plus1: 0.3668
  hedging_mean: 0.8411
per_dimension:
  self_direction:
    mae: 0.3681
    accuracy: 0.6535
    qwk: 0.1223
    spearman: 0.2227
    calibration: 0.4535
    hedging: 0.7178
  stimulation:
    mae: 0.1423
    accuracy: 0.8515
    qwk: 0.1903
    spearman: 0.512
    calibration: 0.5058
    hedging: 0.9703
  hedonism:
    mae: 0.1189
    accuracy: 0.8861
    qwk: 0.5183
    spearman: 0.4562
    calibration: 0.7719
    hedging: 0.9059
  achievement:
    mae: 0.2551
    accuracy: 0.7624
    qwk: 0.4535
    spearman: 0.516
    calibration: 0.7098
    hedging: 0.5941
  power:
    mae: 0.1723
    accuracy: 0.8762
    qwk: 0.276
    spearman: 0.168
    calibration: 0.7489
    hedging: 0.8911
  security:
    mae: 0.2436
    accuracy: 0.7673
    qwk: 0.3159
    spearman: 0.1386
    calibration: 0.6801
    hedging: 0.8762
  conformity:
    mae: 0.2543
    accuracy: 0.7475
    qwk: 0.0352
    spearman: 0.3439
    calibration: 0.4729
    hedging: 0.9653
  tradition:
    mae: 0.1941
    accuracy: 0.8119
    qwk: 0.2075
    spearman: 0.2311
    calibration: 0.5453
    hedging: 0.9406
  benevolence:
    mae: 0.3203
    accuracy: 0.7129
    qwk: 0.3069
    spearman: 0.3264
    calibration: 0.5003
    hedging: 0.703
  universalism:
    mae: 0.1884
    accuracy: 0.8366
    qwk: 0.2902
    spearman: 0.2024
    calibration: 0.824
    hedging: 0.8465
observations: EMD shows the sharpest QWK regression — from 0.410 (run_003, best
  overall) to 0.272 (poor). MAE improved to 0.226, but hedging rose to 84.1% and
  minority recall fell to 21.7%. Self_direction QWK collapsed to 0.122 (from 0.326),
  conformity to 0.035 (from 0.314). Universalism dropped from 0.732 to 0.290.
  Achievement improved (0.454 vs 0.283), suggesting the new data helps some dimensions
  while degrading others. The dataset composition change appears to have introduced
  class imbalance that overwhelms EMD's distributional advantages.
