# VIF Experiment: run_005_CORAL
# Generated: 2026-02-20T09:25:59
# Git: 4c48773
metadata:
  experiment_id: run_005_CORAL
  run_id: run_005
  model_name: CORAL
  timestamp: '2026-02-20T09:25:59'
  git_commit: 4c48773
  config_hash: f29ddfd7fb9d
provenance:
  prev_run_id: run_003
  prev_git_commit: a3f493f
  git_log:
  - '4c48773 docs: remove redundant introductory sentences from AGENTS.md and GEMINI.md'
  - '31918e6 fix(vif): include dataset splits in experiment logger deduplication hash'
  - 'e5cb197 fix(vif): include dataset splits in experiment logger deduplication hash'
  - 'b84d1ca fix(judge): backfill rationales for 61 personas and fix under-labeled
    entry'
  - 'c0435e1 chore(data): add batch 2 judge labels for 60 new personas'
  - 'cd59d19 chore(data): add batch 2 synthetic and wrangled persona data (60 personas)'
  - '986e408 feat(judge): add consolidation module, harden wrangling parser, and update
    judge pipeline docs'
  - 'a036004 chore(data): remove batch 1A pre-tension Universalism personas (10)'
  - de4a5e1 Refactor notebook references to scripts and harden synthetic helpers (twinkl-ayp)
  - '62cbe11 docs(gen): fix config variables — remove dead START_DATE, wire MIN_DAYS_BETWEEN_ENTRIES'
  - '9e3e22a feat(vif): add runs 003/004 experiment logs with provenance backfill'
  config_delta:
    added:
      data.train_ratio: 0.7
      data.val_ratio: 0.15
      data.split_seed: 2025
      data.pct_truncated: 31.441
      data.state_dim: 1164
      uncertainty.mc_dropout_samples: 50
    removed: {}
    changed: {}
  rationale: Re-run of MiniLM-384d configuration with expanded dataset (958 train
    vs 637). Added 60 new personas from batch 2, removed 10 pre-tension Universalism
    personas, and added MC dropout uncertainty (50 samples). Tests whether 50% more
    data improves MiniLM performance.
config:
  encoder:
    model_name: all-MiniLM-L6-v2
  state_encoder:
    window_size: 3
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 31.441
    state_dim: 1164
  model:
    hidden_dim: 256
    dropout: 0.2
  training:
    loss_fn: coral
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 958
  n_val: 214
  n_test: 202
capacity:
  n_parameters: 370196
  param_sample_ratio: 386.4259
training_dynamics:
  best_epoch: 2
  total_epochs: 22
  train_loss_at_best: 0.5542
  val_loss_at_best: 0.6024
  gap_at_best: 0.0483
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2155
  accuracy_mean: 0.7985
  qwk_mean: 0.2817
  spearman_mean: 0.4115
  calibration_global: 0.6286
  calibration_positive_dims: 10
  mean_uncertainty: 0.0686
  minority_recall_mean: 0.1857
  recall_minus1: 0.0087
  recall_plus1: 0.3627
  hedging_mean: 0.8584
per_dimension:
  self_direction:
    mae: 0.3544
    accuracy: 0.698
    qwk: 0.2524
    spearman: 0.3271
    calibration: 0.5271
    hedging: 0.5644
  stimulation:
    mae: 0.1405
    accuracy: 0.8614
    qwk: 0.2741
    spearman: 0.5546
    calibration: 0.5502
    hedging: 0.9653
  hedonism:
    mae: 0.1051
    accuracy: 0.9109
    qwk: 0.5697
    spearman: 0.5877
    calibration: 0.6979
    hedging: 0.8663
  achievement:
    mae: 0.275
    accuracy: 0.7525
    qwk: 0.3988
    spearman: 0.486
    calibration: 0.6822
    hedging: 0.604
  power:
    mae: 0.1525
    accuracy: 0.8564
    qwk: 0.0433
    spearman: 0.1886
    calibration: 0.7522
    hedging: 0.9653
  security:
    mae: 0.2163
    accuracy: 0.7871
    qwk: 0.2448
    spearman: 0.3873
    calibration: 0.5752
    hedging: 0.9257
  conformity:
    mae: 0.2331
    accuracy: 0.7772
    qwk: 0.1964
    spearman: 0.4409
    calibration: 0.5184
    hedging: 0.9505
  tradition:
    mae: 0.2016
    accuracy: 0.797
    qwk: .nan
    spearman: 0.2116
    calibration: 0.211
    hedging: 1.0
  benevolence:
    mae: 0.3151
    accuracy: 0.698
    qwk: 0.2217
    spearman: 0.3546
    calibration: 0.4994
    hedging: 0.8713
  universalism:
    mae: 0.1615
    accuracy: 0.8465
    qwk: 0.3342
    spearman: 0.5769
    calibration: 0.6879
    hedging: 0.8713
observations: CORAL with the expanded dataset shows improved MAE (0.216 vs 0.241
  run_003) but degraded QWK (0.282 vs 0.369) and dramatically increased hedging (85.8%
  vs 84.1%). Minority recall collapsed to 18.6% (from 26.5%). Tradition QWK is NaN
  — degenerate predictions with 100% hedging on that dimension. Power QWK near-zero
  (0.043). The larger dataset pushes the model further into majority-class hedging.
  Universalism dropped from 0.607 to 0.334, likely due to removal of pre-tension
  Universalism personas.
