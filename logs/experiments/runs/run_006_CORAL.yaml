# VIF Experiment: run_006_CORAL
# Generated: 2026-02-20T09:27:37
# Git: 4c48773(dirty)
metadata:
  experiment_id: run_006_CORAL
  run_id: run_006
  model_name: CORAL
  timestamp: '2026-02-20T09:27:37'
  git_commit: 4c48773(dirty)
  config_hash: a6762d146a65
provenance:
  prev_run_id: run_004
  prev_git_commit: a3f493f(dirty)
  git_log:
  - '4c48773 docs: remove redundant introductory sentences from AGENTS.md and GEMINI.md'
  - '31918e6 fix(vif): include dataset splits in experiment logger deduplication hash'
  - 'e5cb197 fix(vif): include dataset splits in experiment logger deduplication hash'
  - 'b84d1ca fix(judge): backfill rationales for 61 personas and fix under-labeled
    entry'
  - 'c0435e1 chore(data): add batch 2 judge labels for 60 new personas'
  - 'cd59d19 chore(data): add batch 2 synthetic and wrangled persona data (60 personas)'
  - '986e408 feat(judge): add consolidation module, harden wrangling parser, and update
    judge pipeline docs'
  - 'a036004 chore(data): remove batch 1A pre-tension Universalism personas (10)'
  - de4a5e1 Refactor notebook references to scripts and harden synthetic helpers (twinkl-ayp)
  - '62cbe11 docs(gen): fix config variables — remove dead START_DATE, wire MIN_DAYS_BETWEEN_ENTRIES'
  - '9e3e22a feat(vif): add runs 003/004 experiment logs with provenance backfill'
  config_delta:
    added:
      data.train_ratio: 0.7
      data.val_ratio: 0.15
      data.split_seed: 2025
      data.pct_truncated: 0.0
      data.state_dim: 266
      uncertainty.mc_dropout_samples: 50
    removed: {}
    changed: {}
  rationale: Re-run of nomic-embed-256d configuration with expanded dataset (958
    train vs 637). Added 60 new personas from batch 2, removed 10 pre-tension
    Universalism personas, and added MC dropout uncertainty (50 samples). Tests
    whether 50% more data improves nomic/small-model performance.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 32
    dropout: 0.3
  training:
    loss_fn: coral
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 958
  n_val: 214
  n_test: 202
capacity:
  n_parameters: 10388
  param_sample_ratio: 10.8434
training_dynamics:
  best_epoch: 26
  total_epochs: 46
  train_loss_at_best: 0.5046
  val_loss_at_best: 0.6182
  gap_at_best: 0.1135
  final_lr: 0.0005
evaluation:
  mae_mean: 0.233
  accuracy_mean: 0.7931
  qwk_mean: 0.278
  spearman_mean: 0.3538
  calibration_global: 0.7685
  calibration_positive_dims: 10
  mean_uncertainty: 0.1612
  minority_recall_mean: 0.1658
  recall_minus1: 0.0174
  recall_plus1: 0.3142
  hedging_mean: 0.8485
per_dimension:
  self_direction:
    mae: 0.3827
    accuracy: 0.6683
    qwk: 0.2577
    spearman: 0.2437
    calibration: 0.5318
    hedging: 0.6832
  stimulation:
    mae: 0.1367
    accuracy: 0.8713
    qwk: 0.3396
    spearman: 0.4969
    calibration: 0.848
    hedging: 0.9059
  hedonism:
    mae: 0.1426
    accuracy: 0.8713
    qwk: 0.3689
    spearman: 0.3097
    calibration: 0.8558
    hedging: 0.9158
  achievement:
    mae: 0.2849
    accuracy: 0.7624
    qwk: 0.442
    spearman: 0.4991
    calibration: 0.7277
    hedging: 0.6287
  power:
    mae: 0.1613
    accuracy: 0.8663
    qwk: 0.098
    spearman: 0.2049
    calibration: 0.8557
    hedging: 0.9604
  security:
    mae: 0.2361
    accuracy: 0.7822
    qwk: 0.2154
    spearman: 0.3159
    calibration: 0.6195
    hedging: 0.9307
  conformity:
    mae: 0.2894
    accuracy: 0.7673
    qwk: 0.2033
    spearman: 0.2294
    calibration: 0.7338
    hedging: 0.8663
  tradition:
    mae: 0.1929
    accuracy: 0.8218
    qwk: 0.2242
    spearman: 0.2341
    calibration: 0.7135
    hedging: 0.9356
  benevolence:
    mae: 0.3187
    accuracy: 0.703
    qwk: 0.2935
    spearman: 0.4248
    calibration: 0.5062
    hedging: 0.797
  universalism:
    mae: 0.185
    accuracy: 0.8168
    qwk: 0.3375
    spearman: 0.5793
    calibration: 0.9095
    hedging: 0.8614
observations: CORAL with nomic on the expanded dataset shows QWK 0.278 (down from
  0.331 in run_004) but MAE improved to 0.233. Hedging rose to 84.9% with minority
  recall at 16.6%. Calibration remained strong at 0.769 — only a slight dip from
  0.755. Achievement was a standout (QWK 0.442, best for CORAL across all runs).
  Power QWK near-zero (0.098). The nomic model shows the same dataset scaling
  regression as MiniLM but with better calibration resilience.
