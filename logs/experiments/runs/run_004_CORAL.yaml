# VIF Experiment: run_004_CORAL
# Generated: 2026-02-18T17:35:20
# Git: a3f493f(dirty)
metadata:
  experiment_id: run_004_CORAL
  run_id: run_004
  model_name: CORAL
  timestamp: '2026-02-18T17:35:20'
  git_commit: a3f493f(dirty)
  config_hash: 0284d6f2a351
provenance:
  prev_run_id: run_002
  prev_git_commit: e1e08c4(dirty)
  git_log:
  - 'a3f493f test: add test suites for judge, registry, and wrangling modules'
  - 2d3f8fd twinkl-g2p remove label-history EMA from VIF state pipeline
  - 'e5f04e7 test(vif): add StateEncoder and VIFDataset parity test suite (twinkl-dks)'
  - 'c605de1 refactor(vif): drop MSE model, focus exclusively on ordinal models (twinkl-hu9.4)'
  - 'e369ddd fix(vif): fail fast on label-entry join losses in merge_labels_and_entries
    (twinkl-hu9.3)'
  - 'a3f0024 docs: add round 2 experiment comparison images for issue #12'
  - '017e833 docs: refresh stale dataset counts across PRD, README, and eval docs
    (twinkl-hu9.6)'
  - '4a47795 fix(nudge): handle non-string JSON fields safely (twinkl-9hk)'
  - '19b6a85 docs(vif): genericize hardcoded architecture values, point to config/vif.yaml'
  - 'e4530a5 fix(nudge): harden parsing and dedupe notebook helpers (twinkl-44r)'
  - 'f296381 feat(nudge): extract nudge decision logic from notebook into src/nudge/'
  - '17504a3 fix(vif): guard eval metrics against NaN/constant-input warnings'
  - '7c612ac fix(vif): wire train_ratio/val_ratio from config through to split_by_persona'
  - '3153d53 docs: standardize agent instruction files'
  - 'f36c87a feat(vif): add experiment logger and run logs for rounds 1-2'
  - '5da7694 refactor(notebooks): reorganize critic training notebooks into v1/v2
    structure'
  config_delta:
    added: {}
    removed:
      state_encoder.ema_alpha: 0.3
    changed: {}
  rationale: Re-run of nomic-embed-256d configuration after removing label-history
    EMA from the state pipeline (commit 2d3f8fd) and dropping MSE loss (commit c605de1).
    Config delta shows ema_alpha removed. Tests whether EMA removal and code fixes
    affect nomic/small-model performance.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  model:
    hidden_dim: 32
    dropout: 0.3
  training:
    loss_fn: coral
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
data:
  n_train: 637
  n_val: 124
  n_test: 143
  train_ratio: 0.7
  val_ratio: 0.15
  split_seed: 2025
  pct_truncated: 0.0
  state_dim: 266
capacity:
  n_parameters: 10388
  param_sample_ratio: 16.3077
training_dynamics:
  best_epoch: 27
  total_epochs: 47
  train_loss_at_best: 0.5473
  val_loss_at_best: 0.6348
  gap_at_best: 0.0875
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2732
  accuracy_mean: 0.7678
  qwk_mean: 0.3306
  spearman_mean: 0.3457
  calibration_global: 0.7553
  calibration_positive_dims: 10
  mean_uncertainty: 0.1875
  minority_recall_mean: 0.2694
  recall_minus1: 0.0577
  recall_plus1: 0.4811
  hedging_mean: 0.7762
per_dimension:
  self_direction:
    mae: 0.4215
    accuracy: 0.6503
    qwk: 0.3362
    spearman: 0.3313
    calibration: 0.5645
    hedging: 0.5664
  stimulation:
    mae: 0.1498
    accuracy: 0.8811
    qwk: 0.5179
    spearman: 0.3912
    calibration: 0.876
    hedging: 0.8252
  hedonism:
    mae: 0.253
    accuracy: 0.7762
    qwk: 0.2188
    spearman: 0.2609
    calibration: 0.693
    hedging: 0.9021
  achievement:
    mae: 0.3103
    accuracy: 0.7483
    qwk: 0.2867
    spearman: 0.2415
    calibration: 0.6511
    hedging: 0.7692
  power:
    mae: 0.1611
    accuracy: 0.8601
    qwk: 0.3071
    spearman: 0.2641
    calibration: 0.836
    hedging: 0.8811
  security:
    mae: 0.3631
    accuracy: 0.6573
    qwk: 0.1538
    spearman: 0.2843
    calibration: 0.5424
    hedging: 0.7972
  conformity:
    mae: 0.2915
    accuracy: 0.7552
    qwk: 0.1805
    spearman: 0.2529
    calibration: 0.6967
    hedging: 0.7972
  tradition:
    mae: 0.2148
    accuracy: 0.8182
    qwk: 0.4139
    spearman: 0.4763
    calibration: 0.8288
    hedging: 0.8322
  benevolence:
    mae: 0.3678
    accuracy: 0.6713
    qwk: 0.4197
    spearman: 0.4836
    calibration: 0.6544
    hedging: 0.6294
  universalism:
    mae: 0.1993
    accuracy: 0.8601
    qwk: 0.4717
    spearman: 0.4708
    calibration: 0.9269
    hedging: 0.7622
observations: CORAL with nomic shows comparable QWK (0.331) to run_002 (0.335) after
  EMA removal â€” the change is within noise. Calibration improved slightly (0.755 vs
  0.734). Hedging decreased from 81.9% to 77.6%, showing more decisive predictions.
  State_dim dropped from 276 to 266. Security remains weak (QWK 0.154). Benevolence
  maintained decent performance (QWK 0.420). CORAL appears robust to EMA removal
  on the nomic encoder.
