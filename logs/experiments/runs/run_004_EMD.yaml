# VIF Experiment: run_004_EMD
# Generated: 2026-02-18T17:35:20
# Git: a3f493f(dirty)
metadata:
  experiment_id: run_004_EMD
  run_id: run_004
  model_name: EMD
  timestamp: '2026-02-18T17:35:20'
  git_commit: a3f493f(dirty)
  config_hash: e5d05e4dc5b7
provenance:
  prev_run_id: run_002
  prev_git_commit: e1e08c4(dirty)
  git_log:
  - 'a3f493f test: add test suites for judge, registry, and wrangling modules'
  - 2d3f8fd twinkl-g2p remove label-history EMA from VIF state pipeline
  - 'e5f04e7 test(vif): add StateEncoder and VIFDataset parity test suite (twinkl-dks)'
  - 'c605de1 refactor(vif): drop MSE model, focus exclusively on ordinal models (twinkl-hu9.4)'
  - 'e369ddd fix(vif): fail fast on label-entry join losses in merge_labels_and_entries
    (twinkl-hu9.3)'
  - 'a3f0024 docs: add round 2 experiment comparison images for issue #12'
  - '017e833 docs: refresh stale dataset counts across PRD, README, and eval docs
    (twinkl-hu9.6)'
  - '4a47795 fix(nudge): handle non-string JSON fields safely (twinkl-9hk)'
  - '19b6a85 docs(vif): genericize hardcoded architecture values, point to config/vif.yaml'
  - 'e4530a5 fix(nudge): harden parsing and dedupe notebook helpers (twinkl-44r)'
  - 'f296381 feat(nudge): extract nudge decision logic from notebook into src/nudge/'
  - '17504a3 fix(vif): guard eval metrics against NaN/constant-input warnings'
  - '7c612ac fix(vif): wire train_ratio/val_ratio from config through to split_by_persona'
  - '3153d53 docs: standardize agent instruction files'
  - 'f36c87a feat(vif): add experiment logger and run logs for rounds 1-2'
  - '5da7694 refactor(notebooks): reorganize critic training notebooks into v1/v2
    structure'
  config_delta:
    added: {}
    removed:
      state_encoder.ema_alpha: 0.3
    changed: {}
  rationale: Re-run of nomic-embed-256d configuration after removing label-history
    EMA from the state pipeline (commit 2d3f8fd) and dropping MSE loss (commit c605de1).
    Config delta shows ema_alpha removed. Tests whether EMA removal and code fixes
    affect nomic/small-model performance.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  model:
    hidden_dim: 32
    dropout: 0.3
  training:
    loss_fn: emd
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
data:
  n_train: 637
  n_val: 124
  n_test: 143
  train_ratio: 0.7
  val_ratio: 0.15
  split_seed: 2025
  pct_truncated: 0.0
  state_dim: 266
capacity:
  n_parameters: 10718
  param_sample_ratio: 16.8257
training_dynamics:
  best_epoch: 23
  total_epochs: 43
  train_loss_at_best: 0.1509
  val_loss_at_best: 0.1869
  gap_at_best: 0.036
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2662
  accuracy_mean: 0.7797
  qwk_mean: 0.3912
  spearman_mean: 0.361
  calibration_global: 0.7658
  calibration_positive_dims: 10
  mean_uncertainty: 0.1816
  minority_recall_mean: 0.3426
  recall_minus1: 0.1204
  recall_plus1: 0.5648
  hedging_mean: 0.7692
per_dimension:
  self_direction:
    mae: 0.4059
    accuracy: 0.6643
    qwk: 0.3966
    spearman: 0.426
    calibration: 0.5493
    hedging: 0.5524
  stimulation:
    mae: 0.151
    accuracy: 0.8671
    qwk: 0.4326
    spearman: 0.3712
    calibration: 0.8658
    hedging: 0.8392
  hedonism:
    mae: 0.2151
    accuracy: 0.8112
    qwk: 0.3959
    spearman: 0.3709
    calibration: 0.6934
    hedging: 0.8951
  achievement:
    mae: 0.3189
    accuracy: 0.7483
    qwk: 0.3662
    spearman: 0.2016
    calibration: 0.7489
    hedging: 0.7832
  power:
    mae: 0.138
    accuracy: 0.8951
    qwk: 0.4058
    spearman: 0.2949
    calibration: 0.7703
    hedging: 0.9021
  security:
    mae: 0.3464
    accuracy: 0.6853
    qwk: 0.2388
    spearman: 0.2684
    calibration: 0.5658
    hedging: 0.8042
  conformity:
    mae: 0.2895
    accuracy: 0.7622
    qwk: 0.3299
    spearman: 0.3602
    calibration: 0.7661
    hedging: 0.7622
  tradition:
    mae: 0.2056
    accuracy: 0.8322
    qwk: 0.4767
    spearman: 0.4283
    calibration: 0.8079
    hedging: 0.8182
  benevolence:
    mae: 0.4103
    accuracy: 0.6713
    qwk: 0.4006
    spearman: 0.4377
    calibration: 0.6119
    hedging: 0.5594
  universalism:
    mae: 0.1813
    accuracy: 0.8601
    qwk: 0.4685
    spearman: 0.4508
    calibration: 0.9324
    hedging: 0.7762
observations: EMD improved QWK from 0.365 (run_002) to 0.391 (run_004) after EMA
  removal â€” one of the few losses to improve. Calibration also improved (0.766 vs
  0.772). Achievement QWK jumped to 0.366 (from 0.236), the best achievement score
  for any nomic run. Self_direction improved to QWK 0.397 (from 0.284). EMD is the
  most robust loss on the nomic encoder, confirming the pattern seen in the MiniLM
  runs. Power QWK (0.406) is the highest for any nomic run on that dimension.
