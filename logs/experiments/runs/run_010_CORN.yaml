# VIF Experiment: run_010_CORN
# Generated: 2026-02-22T23:03:30
# Git: f532024(dirty)
metadata:
  experiment_id: run_010_CORN
  run_id: run_010
  model_name: CORN
  timestamp: '2026-02-22T23:03:30'
  git_commit: f532024(dirty)
  config_hash: a37bde71dd03
provenance:
  prev_run_id: run_008
  prev_git_commit: d0b93da(dirty)
  git_log:
  - 'f532024 fix(vif): preserve manual sections in experiment index.md'
  - '9d0fe47 Merge pull request #19 from DesmondChoy/km-annotations-1'
  - '7faf46a feat: finish  km annotations 11-19'
  - 'dce8204 docs: clarify /quality alias and remove unused command (twinkl-4s3)'
  - '7538d36 twinkl-ig4: add CORAL importance weights and train-split wiring'
  - '13f2cbf docs(evals): establish QWK and minority recall as primary entry-level
    metrics'
  - '099abf3 docs(experiments): retire MiniLM encoder from future experiment scope'
  - 'cabfba6 docs(experiments): add Findings section with Universalism QWK analysis'
  - '433b16e fix(wrangling): update registry for skipped files on re-runs (twinkl-qx1)'
  - '6a1178a fix(vif): restore dropout state after MC sampling in predict_with_uncertainty'
  - '78db05b refactor(vif): remove unused threshold param from compute_accuracy_per_dimension
    (twinkl-vlk)'
  - '026812a docs: qualify README status claims and add legend (twinkl-ivd)'
  - 'c1dd1f9 docs: add status boundary markers to README, PRD, and worked example
    (twinkl-ivd)'
  - '6432e4c docs: defer PRD onboarding details to onboarding spec (twinkl-8yg)'
  - '79c7121 docs: clarify experiment-review skill ambiguities (twinkl-9a1)'
  - '3fed082 chore(vif): note config values are preliminary and under ablation'
  - '1cb4e65 docs: fix Streamlitâ†’Shiny framework mismatch in architecture snapshots
    (twinkl-hu9.8)'
  - 'e2ec30e docs(vif): fix Critic scoring-scope contradiction in worked example'
  - '4718875 docs(evals): sync status to current run reality (twinkl-hu9.7)'
  - 'a4fbf07 docs: align data_schema.md with actual parquet columns'
  - '8a02296 feat: added some annotations for km'
  - '1e958de chore(vif): add experiment run 009 and update experiment review skill'
  config_delta:
    added: {}
    removed: {}
    changed:
      model.hidden_dim:
        from: 128
        to: 64
  rationale: >-
    Follow-up to run_008 to evaluate model.hidden_dim 128 to 64 while keeping the nomic encoder and
    split seed fixed on the 1020-sample dataset. The intent is to isolate whether this ablation
    improves QWK/minority recall without sacrificing calibration.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 64
    dropout: 0.3
  training:
    loss_fn: corn
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 1020
  n_val: 230
  n_test: 210
capacity:
  n_parameters: 22804
  param_sample_ratio: 22.3569
training_dynamics:
  best_epoch: 16
  total_epochs: 36
  train_loss_at_best: 0.2249
  val_loss_at_best: 0.2318
  gap_at_best: 0.0069
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2056
  accuracy_mean: 0.8205
  qwk_mean: 0.434
  spearman_mean: 0.4068
  calibration_global: 0.8345
  calibration_positive_dims: 10
  mean_uncertainty: 0.1512
  minority_recall_mean: 0.2848
  recall_minus1: 0.0892
  recall_plus1: 0.4804
  hedging_mean: 0.82
per_dimension:
  self_direction:
    mae: 0.4212
    accuracy: 0.6143
    qwk: 0.3913
    spearman: 0.4123
    calibration: 0.5378
    hedging: 0.6048
  stimulation:
    mae: 0.0904
    accuracy: 0.919
    qwk: 0.5159
    spearman: 0.4506
    calibration: 0.8798
    hedging: 0.9286
  hedonism:
    mae: 0.1345
    accuracy: 0.8905
    qwk: 0.4631
    spearman: 0.2764
    calibration: 0.8368
    hedging: 0.9048
  achievement:
    mae: 0.1609
    accuracy: 0.8619
    qwk: 0.432
    spearman: 0.3161
    calibration: 0.7642
    hedging: 0.8714
  power:
    mae: 0.0538
    accuracy: 0.9524
    qwk: 0.3776
    spearman: 0.3362
    calibration: 0.918
    hedging: 0.9667
  security:
    mae: 0.3091
    accuracy: 0.7238
    qwk: 0.1853
    spearman: 0.2716
    calibration: 0.6999
    hedging: 0.8571
  conformity:
    mae: 0.2632
    accuracy: 0.7619
    qwk: 0.5459
    spearman: 0.5426
    calibration: 0.7154
    hedging: 0.7143
  tradition:
    mae: 0.1449
    accuracy: 0.8762
    qwk: 0.5307
    spearman: 0.4818
    calibration: 0.8893
    hedging: 0.8667
  benevolence:
    mae: 0.2705
    accuracy: 0.7857
    qwk: 0.4546
    spearman: 0.4753
    calibration: 0.8141
    hedging: 0.6762
  universalism:
    mae: 0.2078
    accuracy: 0.819
    qwk: 0.444
    spearman: 0.5056
    calibration: 0.8991
    hedging: 0.8095
observations: >-
  run_010 CORN records QWK 0.434 (moderate), calibration 0.835 (good), MAE 0.206, accuracy 0.821,
  minority recall 0.285, and hedging 82.0% (excessive). Compared with run_008 CORN, QWK is +0.090,
  calibration +0.050, minority recall +0.009, and hedging -0.9%. Largest QWK gains are power 0.378
  (+0.362), universalism 0.444 (+0.188); largest regressions are benevolence 0.455 (-0.110). The
  hardest dimension in this run is security at QWK 0.185.
