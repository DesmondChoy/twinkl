# VIF Experiment: run_008_EMD
# Generated: 2026-02-20T14:47:14
# Git: d0b93da(dirty)
metadata:
  experiment_id: run_008_EMD
  run_id: run_008
  model_name: EMD
  timestamp: '2026-02-20T14:47:14'
  git_commit: d0b93da(dirty)
  config_hash: bae14f9dd760
provenance:
  prev_run_id: run_007
  prev_git_commit: d0b93da(dirty)
  git_log: []
  config_delta:
    added: {}
    removed: {}
    changed:
      model.hidden_dim:
        from: 64
        to: 128
  rationale: Doubled hidden_dim from 64 to 128 on nomic-embed-256d to continue the capacity
    sweep. Tests whether QWK improvements from hd=64 (run_007) continue to scale with
    more parameters, or whether diminishing returns or overfitting emerge at the 54:1
    param/sample ratio.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 128
    dropout: 0.3
  training:
    loss_fn: emd
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 1020
  n_val: 230
  n_test: 210
capacity:
  n_parameters: 55070
  param_sample_ratio: 53.9902
training_dynamics:
  best_epoch: 4
  total_epochs: 24
  train_loss_at_best: 0.1544
  val_loss_at_best: 0.1351
  gap_at_best: -0.0193
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2
  accuracy_mean: 0.8205
  qwk_mean: 0.3647
  spearman_mean: 0.3896
  calibration_global: 0.8022
  calibration_positive_dims: 10
  mean_uncertainty: 0.1109
  minority_recall_mean: 0.2999
  recall_minus1: 0.081
  recall_plus1: 0.5189
  hedging_mean: 0.8129
per_dimension:
  self_direction:
    mae: 0.3949
    accuracy: 0.6429
    qwk: 0.4208
    spearman: 0.487
    calibration: 0.5242
    hedging: 0.5619
  stimulation:
    mae: 0.068
    accuracy: 0.9429
    qwk: 0.6671
    spearman: 0.6765
    calibration: 0.8743
    hedging: 0.8667
  hedonism:
    mae: 0.1393
    accuracy: 0.8857
    qwk: 0.3967
    spearman: 0.3131
    calibration: 0.8405
    hedging: 0.8619
  achievement:
    mae: 0.1764
    accuracy: 0.8476
    qwk: 0.4054
    spearman: 0.3263
    calibration: 0.7955
    hedging: 0.819
  power:
    mae: 0.0741
    accuracy: 0.9476
    qwk: -0.1856
    spearman: -0.2037
    calibration: 0.8302
    hedging: 0.9667
  security:
    mae: 0.3359
    accuracy: 0.7
    qwk: 0.1484
    spearman: 0.0472
    calibration: 0.6539
    hedging: 0.8571
  conformity:
    mae: 0.2502
    accuracy: 0.7571
    qwk: 0.5143
    spearman: 0.6062
    calibration: 0.6713
    hedging: 0.7143
  tradition:
    mae: 0.1386
    accuracy: 0.8619
    qwk: 0.4269
    spearman: 0.5438
    calibration: 0.7484
    hedging: 0.9095
  benevolence:
    mae: 0.2184
    accuracy: 0.8095
    qwk: 0.5501
    spearman: 0.6029
    calibration: 0.8674
    hedging: 0.7286
  universalism:
    mae: 0.2041
    accuracy: 0.8095
    qwk: 0.3027
    spearman: 0.4969
    calibration: 0.9197
    hedging: 0.8429
observations: EMD at hd=128 achieves the lowest MAE of any run (0.200) with comparable
  QWK to hd=64 (0.365 vs 0.358). Stimulation reaches QWK 0.667 — the highest single-
  dimension QWK in runs 007-008 (Spearman 0.677, good). Benevolence strong (0.550).
  Conformity 0.514. Power is strongly negative (QWK -0.186, Spearman -0.204).
  Calibration declined from 0.849 (hd=64) to 0.802 (hd=128). EMD is the most capacity-
  robust loss — QWK comparable across hd=64 and hd=128 while other losses regressed.
  Best_epoch=4 with negative gap (-0.019) confirms healthy generalization.
