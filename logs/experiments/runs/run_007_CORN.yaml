# VIF Experiment: run_007_CORN
# Generated: 2026-02-20T10:42:25
# Git: 4c48773(dirty)
metadata:
  experiment_id: run_007_CORN
  run_id: run_007
  model_name: CORN
  timestamp: '2026-02-20T10:42:25'
  git_commit: 4c48773(dirty)
  config_hash: a37bde71dd03
provenance:
  prev_run_id: run_006
  prev_git_commit: 4c48773(dirty)
  git_log: []
  config_delta:
    added: {}
    removed: {}
    changed:
      model.hidden_dim:
        from: 32
        to: 64
  rationale: Capacity exploration on nomic-embed-256d encoder, increasing hidden_dim
    from 32 to 64 while keeping all other config identical to run_006. Tests whether
    doubling MLP capacity improves QWK and minority recall on the expanded dataset.
config:
  encoder:
    model_name: nomic-ai/nomic-embed-text-v1.5
    truncate_dim: 256
    text_prefix: 'classification: '
    trust_remote_code: true
  state_encoder:
    window_size: 1
  data:
    train_ratio: 0.7
    val_ratio: 0.15
    split_seed: 2025
    pct_truncated: 0.0
    state_dim: 266
  model:
    hidden_dim: 64
    dropout: 0.3
  training:
    loss_fn: corn
    learning_rate: 0.001
    weight_decay: 0.01
    batch_size: 16
    epochs: 100
    early_stopping_patience: 20
    scheduler_factor: 0.5
    scheduler_patience: 10
    seed: 2025
  uncertainty:
    mc_dropout_samples: 50
data:
  n_train: 958
  n_val: 214
  n_test: 202
capacity:
  n_parameters: 22804
  param_sample_ratio: 23.8038
training_dynamics:
  best_epoch: 15
  total_epochs: 35
  train_loss_at_best: 0.2272
  val_loss_at_best: 0.2914
  gap_at_best: 0.0643
  final_lr: 0.0005
evaluation:
  mae_mean: 0.2241
  accuracy_mean: 0.7975
  qwk_mean: 0.2901
  spearman_mean: 0.3493
  calibration_global: 0.7766
  calibration_positive_dims: 10
  mean_uncertainty: 0.1467
  minority_recall_mean: 0.1986
  recall_minus1: 0.0558
  recall_plus1: 0.3414
  hedging_mean: 0.8465
per_dimension:
  self_direction:
    mae: 0.3492
    accuracy: 0.6881
    qwk: 0.2449
    spearman: 0.3249
    calibration: 0.6441
    hedging: 0.7327
  stimulation:
    mae: 0.1367
    accuracy: 0.8762
    qwk: 0.4307
    spearman: 0.5003
    calibration: 0.846
    hedging: 0.896
  hedonism:
    mae: 0.1182
    accuracy: 0.896
    qwk: 0.505
    spearman: 0.4591
    calibration: 0.8401
    hedging: 0.896
  achievement:
    mae: 0.2512
    accuracy: 0.7871
    qwk: 0.5208
    spearman: 0.5578
    calibration: 0.7345
    hedging: 0.6436
  power:
    mae: 0.1618
    accuracy: 0.8515
    qwk: -0.0653
    spearman: 0.1138
    calibration: 0.8164
    hedging: 0.9604
  security:
    mae: 0.2342
    accuracy: 0.7921
    qwk: 0.3293
    spearman: 0.3376
    calibration: 0.7276
    hedging: 0.8564
  conformity:
    mae: 0.2688
    accuracy: 0.7723
    qwk: 0.2843
    spearman: 0.2489
    calibration: 0.7087
    hedging: 0.8515
  tradition:
    mae: 0.1875
    accuracy: 0.8069
    qwk: 0.1154
    spearman: 0.3676
    calibration: 0.6761
    hedging: 0.9604
  benevolence:
    mae: 0.334
    accuracy: 0.6832
    qwk: 0.2007
    spearman: 0.298
    calibration: 0.5777
    hedging: 0.802
  universalism:
    mae: 0.1999
    accuracy: 0.8218
    qwk: 0.335
    spearman: 0.2854
    calibration: 0.8831
    hedging: 0.8663
observations: CORN QWK marginally improved from 0.280 (run_006 hd=32) to 0.290
  with hd=64. Achievement was the standout (QWK 0.521, best for any CORN run).
  Power QWK turned negative (-0.065), confirming CORN's systematic failure on sparse
  dimensions. Tradition collapsed to 0.115 (from 0.282). Hedonism maintained 0.505,
  resolving the NaN issue seen in earlier small-model CORN runs.
